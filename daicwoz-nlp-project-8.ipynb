{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9156874,"sourceType":"datasetVersion","datasetId":5519598}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:07:48.896144Z","iopub.execute_input":"2025-11-09T06:07:48.896361Z","iopub.status.idle":"2025-11-09T06:07:48.917746Z","shell.execute_reply.started":"2025-11-09T06:07:48.896343Z","shell.execute_reply":"2025-11-09T06:07:48.917077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Separating and Saving Transcripts as CSVs**","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\n\n# Create output directory\noutput_dir = '/kaggle/working/transcripts/'\nos.makedirs(output_dir, exist_ok=True)\n\n# Function to find and copy transcripts\ndef separate_transcripts(input_dir='/kaggle/input/daicwoz/daicwoz/daicwoz'):\n    transcript_files = []\n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith('_TRANSCRIPT.csv'):\n                src_path = os.path.join(root, file)\n                participant_id = file.split('_')[0]  # e.g., '300' from '300_TRANSCRIPT.csv'\n                dest_path = os.path.join(output_dir, f'{participant_id}.csv')\n                \n                # Read tab-separated and save as comma-separated\n                df = pd.read_csv(src_path, sep='\\t')\n                df.to_csv(dest_path, index=False)\n                \n                transcript_files.append(dest_path)\n    return transcript_files\n\n# Run separation\ntranscripts = separate_transcripts()\nprint(f'Separated and saved {len(transcripts)} transcript CSVs.')\n\n# Visualize: Plot number of transcripts (simple bar for count)\nimport matplotlib.pyplot as plt\nplt.bar(['Transcripts'], [len(transcripts)])\nplt.title('Number of Separated Transcripts')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:01:11.217842Z","iopub.execute_input":"2025-11-09T06:01:11.218448Z","iopub.status.idle":"2025-11-09T06:01:14.324334Z","shell.execute_reply.started":"2025-11-09T06:01:11.218421Z","shell.execute_reply":"2025-11-09T06:01:14.323720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def measure_csv_space(dir_path=output_dir):\n    total_size = 0\n    for file in os.listdir(dir_path):\n        if file.endswith('.csv'):\n            total_size += os.path.getsize(os.path.join(dir_path, file))\n    total_mb = total_size / (1024 * 1024)\n    print(f'Total space for CSVs: {total_mb:.2f} MB')\n    return total_mb\n\n# Run measurement\nspace = measure_csv_space()\n\n# Visualize: Simple pie chart (space used vs. arbitrary free space for demo)\nplt.pie([space, 100 - space], labels=['Used (MB)', 'Free'], autopct='%1.1f%%')\nplt.title('CSV Space Usage (Demo Scale)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:01:33.741227Z","iopub.execute_input":"2025-11-09T06:01:33.741523Z","iopub.status.idle":"2025-11-09T06:01:33.838200Z","shell.execute_reply.started":"2025-11-09T06:01:33.741501Z","shell.execute_reply":"2025-11-09T06:01:33.837402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Preprocessing Text Data**","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nimport re\nfrom wordcloud import WordCloud\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Load labels (combine train/dev/test for full processing; adjust paths)\nlabel_files = [\n    '/kaggle/input/daicwoz/daicwoz/daicwoz/train_split_Depression_AVEC2017.csv',\n    '/kaggle/input/daicwoz/daicwoz/daicwoz/dev_split_Depression_AVEC2017.csv',\n    '/kaggle/input/daicwoz/daicwoz/daicwoz/test_split_Depression_AVEC2017.csv'\n]\nlabels = pd.concat([pd.read_csv(f) for f in label_files if os.path.exists(f)], ignore_index=True)\nlabels = labels[['Participant_ID', 'PHQ8_Binary']]  # Focus on ID and binary label\n\n# Preprocessing function\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation\n    tokens = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize\n    return ' '.join(tokens)\n\n# Process all transcripts\nprocessed_data = []\nfor csv in os.listdir(output_dir):\n    if csv.endswith('.csv'):\n        participant_id = int(csv.split('.')[0])\n        df = pd.read_csv(os.path.join(output_dir, csv))\n        participant_text = ' '.join(df[df['speaker'] == 'Participant']['value'].fillna('').astype(str))\n        \n        # Before preprocessing visualization: Word cloud\n        if participant_text:\n            wc_before = WordCloud(width=800, height=400).generate(participant_text)\n            plt.figure(figsize=(10, 5))\n            plt.imshow(wc_before, interpolation='bilinear')\n            plt.title(f'Word Cloud Before Preprocessing (ID: {participant_id})')\n            plt.axis('off')\n            plt.show()\n        \n        cleaned_text = preprocess_text(participant_text)\n        \n        # After preprocessing visualization: Word cloud\n        if cleaned_text:\n            wc_after = WordCloud(width=800, height=400).generate(cleaned_text)\n            plt.figure(figsize=(10, 5))\n            plt.imshow(wc_after, interpolation='bilinear')\n            plt.title(f'Word Cloud After Preprocessing (ID: {participant_id})')\n            plt.axis('off')\n            plt.show()\n        \n        # Histogram of word lengths before/after\n        words_before = word_tokenize(participant_text)\n        lengths_before = [len(word) for word in words_before]\n        plt.hist(lengths_before, bins=20, alpha=0.5, label='Before')\n        \n        words_after = word_tokenize(cleaned_text)\n        lengths_after = [len(word) for word in words_after]\n        plt.hist(lengths_after, bins=20, alpha=0.5, label='After')\n        plt.title(f'Word Length Distribution (ID: {participant_id})')\n        plt.legend()\n        plt.show()\n        \n        label = labels[labels['Participant_ID'] == participant_id]['PHQ8_Binary'].values[0] if participant_id in labels['Participant_ID'].values else None\n        processed_data.append({'id': participant_id, 'text': cleaned_text, 'label': label})\n\n# Save processed data\nprocessed_df = pd.DataFrame(processed_data)\nprocessed_df.to_csv('/kaggle/working/processed_data.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:09:14.580325Z","iopub.execute_input":"2025-11-09T06:09:14.581125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Feature Extraction**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom textblob import TextBlob\nimport numpy as np\n\n# Load processed data\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label'])  # Drop if no label\n\n# TF-IDF features\nvectorizer = TfidfVectorizer(max_features=500)  # Limit for simplicity\ntfidf_features = vectorizer.fit_transform(df['text']).toarray()\n\n# Additional features: sentiment, word count, avg response time\nfeatures = []\nfor idx, row in df.iterrows():\n    participant_id = row['id']\n    transcript_path = os.path.join(output_dir, f'{participant_id}.csv')\n    trans_df = pd.read_csv(transcript_path)\n    part_df = trans_df[trans_df['speaker'] == 'Participant']\n    \n    # Avg response time\n    response_times = (part_df['stop_time'] - part_df['start_time']).values\n    avg_time = np.mean(response_times) if len(response_times) > 0 else 0\n    \n    # Sentiment\n    blob = TextBlob(row['text'])\n    polarity = blob.sentiment.polarity\n    subjectivity = blob.sentiment.subjectivity\n    \n    # Word count\n    word_count = len(word_tokenize(row['text']))\n    \n    features.append([polarity, subjectivity, avg_time, word_count])\n\n# Combine TF-IDF with custom features\ncustom_features = np.array(features)\nall_features = np.hstack((tfidf_features, custom_features))\n\n# Visualize: Feature correlation heatmap (sample)\nimport seaborn as sns\nfeat_df = pd.DataFrame(custom_features, columns=['Polarity', 'Subjectivity', 'Avg Time', 'Word Count'])\nsns.heatmap(feat_df.corr(), annot=True)\nplt.title('Custom Feature Correlations')\nplt.show()\n\n# Save features\nnp.save('/kaggle/working/features-2.npy', all_features)\ndf[['id', 'label']].to_csv('/kaggle/working/labels-2.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:15:50.714794Z","iopub.execute_input":"2025-11-09T06:15:50.715098Z","iopub.status.idle":"2025-11-09T06:15:52.385880Z","shell.execute_reply.started":"2025-11-09T06:15:50.715076Z","shell.execute_reply":"2025-11-09T06:15:52.385234Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# **MODEL RUN**","metadata":{}},{"cell_type":"code","source":"# GPU Mode: Eradicate all RAPIDS/NVIDIA traces enforcing old ABI\n!pip uninstall -y pyarrow cudf-cu12 dask-cudf-cu12 libcugraph-cu12 pylibcugraph-cu12 pylibraft-cu12 rmm-cu12 nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-nvjitlink-cu12 bigframes pandas-gbq mkl-umath mkl-random mkl-fft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:57:05.834000Z","iopub.execute_input":"2025-11-10T04:57:05.834218Z","iopub.status.idle":"2025-11-10T04:57:31.575521Z","shell.execute_reply.started":"2025-11-10T04:57:05.834198Z","shell.execute_reply":"2025-11-10T04:57:31.574889Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Found existing installation: pyarrow 22.0.0\nUninstalling pyarrow-22.0.0:\n  Successfully uninstalled pyarrow-22.0.0\nFound existing installation: cudf-cu12 25.2.2\nUninstalling cudf-cu12-25.2.2:\n  Successfully uninstalled cudf-cu12-25.2.2\nFound existing installation: dask-cudf-cu12 25.2.2\nUninstalling dask-cudf-cu12-25.2.2:\n  Successfully uninstalled dask-cudf-cu12-25.2.2\nFound existing installation: libcugraph-cu12 25.6.0\nUninstalling libcugraph-cu12-25.6.0:\n  Successfully uninstalled libcugraph-cu12-25.6.0\nFound existing installation: pylibcugraph-cu12 25.6.0\nUninstalling pylibcugraph-cu12-25.6.0:\n  Successfully uninstalled pylibcugraph-cu12-25.6.0\nFound existing installation: pylibraft-cu12 25.2.0\nUninstalling pylibraft-cu12-25.2.0:\n  Successfully uninstalled pylibraft-cu12-25.2.0\nFound existing installation: rmm-cu12 25.2.0\nUninstalling rmm-cu12-25.2.0:\n  Successfully uninstalled rmm-cu12-25.2.0\nFound existing installation: nvidia-cublas-cu12 12.5.3.2\nUninstalling nvidia-cublas-cu12-12.5.3.2:\n  Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\nFound existing installation: nvidia-cuda-cupti-cu12 12.5.82\nUninstalling nvidia-cuda-cupti-cu12-12.5.82:\n  Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\nFound existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\nUninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n  Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\nFound existing installation: nvidia-cuda-runtime-cu12 12.5.82\nUninstalling nvidia-cuda-runtime-cu12-12.5.82:\n  Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\nFound existing installation: nvidia-cudnn-cu12 9.3.0.75\nUninstalling nvidia-cudnn-cu12-9.3.0.75:\n  Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\nFound existing installation: nvidia-cufft-cu12 11.2.3.61\nUninstalling nvidia-cufft-cu12-11.2.3.61:\n  Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\nFound existing installation: nvidia-curand-cu12 10.3.6.82\nUninstalling nvidia-curand-cu12-10.3.6.82:\n  Successfully uninstalled nvidia-curand-cu12-10.3.6.82\nFound existing installation: nvidia-cusolver-cu12 11.6.3.83\nUninstalling nvidia-cusolver-cu12-11.6.3.83:\n  Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nFound existing installation: nvidia-cusparse-cu12 12.5.1.3\nUninstalling nvidia-cusparse-cu12-12.5.1.3:\n  Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\nFound existing installation: nvidia-nvjitlink-cu12 12.5.82\nUninstalling nvidia-nvjitlink-cu12-12.5.82:\n  Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\nFound existing installation: bigframes 2.12.0\nUninstalling bigframes-2.12.0:\n  Successfully uninstalled bigframes-2.12.0\nFound existing installation: pandas-gbq 0.29.2\nUninstalling pandas-gbq-0.29.2:\n  Successfully uninstalled pandas-gbq-0.29.2\nFound existing installation: mkl-umath 0.1.1\nUninstalling mkl-umath-0.1.1:\n  Successfully uninstalled mkl-umath-0.1.1\nFound existing installation: mkl-random 1.2.4\nUninstalling mkl-random-1.2.4:\n  Successfully uninstalled mkl-random-1.2.4\nFound existing installation: mkl-fft 1.3.8\nUninstalling mkl-fft-1.3.8:\n  Successfully uninstalled mkl-fft-1.3.8\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install pyarrow==21.0.0\n!pip install xxhash huggingface-hub\n!pip install --upgrade datasets\n# Workflow essentials\n!pip install transformers evaluate accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:57:42.213964Z","iopub.execute_input":"2025-11-10T04:57:42.214205Z","iopub.status.idle":"2025-11-10T04:59:10.253787Z","shell.execute_reply.started":"2025-11-10T04:57:42.214175Z","shell.execute_reply":"2025-11-10T04:59:10.253155Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting pyarrow==21.0.0\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcudf-cu12 25.2.2 requires rmm-cu12==25.2.*, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pyarrow-21.0.0\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.10.5)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (21.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nCollecting mkl_fft (from numpy>=1.17->datasets)\n  Downloading mkl_fft-2.1.1-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\nCollecting mkl_random (from numpy>=1.17->datasets)\n  Downloading mkl_random-1.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nCollecting mkl_umath (from numpy>=1.17->datasets)\n  Downloading mkl_umath-0.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nCollecting numpy>=1.17 (from datasets)\n  Downloading numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nDownloading numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml-cu12 25.2.1 requires cudf-cu12==25.2.*, which is not installed.\ncuml-cu12 25.2.1 requires dask-cudf-cu12==25.2.*, which is not installed.\ncuml-cu12 25.2.1 requires nvidia-cublas-cu12, which is not installed.\ncuml-cu12 25.2.1 requires nvidia-cufft-cu12, which is not installed.\ncuml-cu12 25.2.1 requires nvidia-curand-cu12, which is not installed.\ncuml-cu12 25.2.1 requires nvidia-cusolver-cu12, which is not installed.\ncuml-cu12 25.2.1 requires nvidia-cusparse-cu12, which is not installed.\ncuml-cu12 25.2.1 requires pylibraft-cu12==25.2.*, which is not installed.\ncuml-cu12 25.2.1 requires rmm-cu12==25.2.*, which is not installed.\ncuvs-cu12 25.2.1 requires pylibraft-cu12==25.2.*, which is not installed.\nucxx-cu12 0.42.0 requires rmm-cu12==25.2.*, which is not installed.\nnx-cugraph-cu12 25.6.0 requires pylibcugraph-cu12==25.6.*, which is not installed.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (21.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mmm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml-cu12 25.2.1 requires cudf-cu12==25.2.*, which is not installed.\ncuml-cu12 25.2.1 requires dask-cudf-cu12==25.2.*, which is not installed.\ncuml-cu12 25.2.1 requires pylibraft-cu12==25.2.*, which is not installed.\ncuml-cu12 25.2.1 requires rmm-cu12==25.2.*, which is not installed.\ncuvs-cu12 25.2.1 requires pylibraft-cu12==25.2.*, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install --upgrade datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:24:28.994395Z","iopub.execute_input":"2025-11-09T06:24:28.994705Z","iopub.status.idle":"2025-11-09T06:24:32.501189Z","shell.execute_reply.started":"2025-11-09T06:24:28.994685Z","shell.execute_reply":"2025-11-09T06:24:32.500193Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.4)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (21.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from datasets import Dataset\nprint(\"Datasets operational:\", Dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:59:18.886054Z","iopub.execute_input":"2025-11-10T04:59:18.886350Z","iopub.status.idle":"2025-11-10T04:59:18.891438Z","shell.execute_reply.started":"2025-11-10T04:59:18.886320Z","shell.execute_reply":"2025-11-10T04:59:18.890654Z"}},"outputs":[{"name":"stdout","text":"Datasets operational: <class 'datasets.arrow_dataset.Dataset'>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **BERT and DisTilBERT**","metadata":{}},{"cell_type":"code","source":"# Workflow essentials\n!pip install transformers evaluate accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:59:21.576651Z","iopub.execute_input":"2025-11-10T04:59:21.576889Z","iopub.status.idle":"2025-11-10T04:59:25.183707Z","shell.execute_reply.started":"2025-11-10T04:59:21.576835Z","shell.execute_reply":"2025-11-10T04:59:25.183091Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (21.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Split into train/test (80/20, stratified for imbalance)\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef train_model(model_name, tokenizer_class, model_class):\n    # Tokenize function\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n   \n    # Load tokenizer and model\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n   \n    # Tokenize datasets\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n   \n    # Compute class weights for imbalance\n    from sklearn.utils.class_weight import compute_class_weight\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n   \n    # Custom loss function with weights\n    def weighted_loss(logits, labels):\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        return loss_fn(logits, labels)\n   \n    # Compute metrics (accuracy, F1)\n    accuracy = load('accuracy')\n    f1 = load('f1')\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return {\n            'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n            'f1': f1.compute(predictions=predictions, references=labels)['f1']\n        }\n   \n    # Training args (optimized for space and accuracy)\n    training_args = TrainingArguments(\n        output_dir='/kaggle/working/results',\n        num_train_epochs=10,  # Reduced to avoid overfitting\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=200,\n        weight_decay=0.01,\n        learning_rate=2e-5,\n        lr_scheduler_type='cosine',\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=2,  # Keep only 2 best checkpoints\n        logging_strategy='no',  # Disable logging to save space\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n   \n    # Trainer\n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics\n    )\n   \n    # Train\n    trainer.train()\n   \n    # Predict on test for probs and preds\n    predictions = trainer.predict(tokenized_test)\n    probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)[:, 1].numpy()\n    preds = np.argmax(predictions.predictions, axis=-1)\n    true_labels = predictions.label_ids\n   \n    # History for curves\n    history = trainer.state.log_history\n   \n    # Cleanup checkpoints to save space\n    checkpoint_dir = '/kaggle/working/results'\n    if os.path.exists(checkpoint_dir):\n        for folder in os.listdir(checkpoint_dir):\n            folder_path = os.path.join(checkpoint_dir, folder)\n            if os.path.isdir(folder_path) and folder != 'runs':\n                shutil.rmtree(folder_path)\n   \n    return history, probs, preds, true_labels, trainer\n\n# Train BERT\nprint(\"Training BERT...\")\nbert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n    'bert-base-uncased', BertTokenizer, BertForSequenceClassification\n)\n\n# Train DistilBERT\nprint(\"Training DistilBERT...\")\ndistil_history, distil_probs, distil_preds, distil_true, distil_trainer = train_model(\n    'distilbert-base-uncased', DistilBertTokenizer, DistilBertForSequenceClassification\n)\n\n# Extract training/validation curves (accuracy and loss)\ndef extract_curves(history):\n    train_loss = [log['loss'] for log in history if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n    val_loss = [log['eval_loss'] for log in history if 'eval_loss' in log]\n    val_acc = [log['eval_accuracy'] for log in history if 'eval_accuracy' in log]\n    epochs = range(1, len(val_loss) + 1)\n    return epochs, train_loss, val_loss, val_acc\n\nbert_epochs, bert_train_loss, bert_val_loss, bert_val_acc = extract_curves(bert_history)\ndistil_epochs, distil_train_loss, distil_val_loss, distil_val_acc = extract_curves(distil_history)\n\n# Plot training-validation curves (save with lower DPI)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].plot(bert_epochs, bert_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(bert_epochs, bert_val_loss, label='BERT Val Loss')\naxs[0].plot(distil_epochs, distil_train_loss, label='DistilBERT Train Loss', linestyle='--')\naxs[0].plot(distil_epochs, distil_val_loss, label='DistilBERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\naxs[1].plot(bert_epochs, bert_val_acc, label='BERT Val Acc')\naxs[1].plot(distil_epochs, distil_val_acc, label='DistilBERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves.png', dpi=100)  # Lower DPI\nplt.close()\n\n# ROC curves\nfpr_bert, tpr_bert, _ = roc_curve(bert_true, bert_probs)\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nfpr_distil, tpr_distil, _ = roc_curve(distil_true, distil_probs)\nroc_auc_distil = auc(fpr_distil, tpr_distil)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot(fpr_distil, tpr_distil, label=f'DistilBERT (AUC = {roc_auc_distil:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves.png', dpi=100)\nplt.close()\n\n# Classification reports as images\ndef report_to_image(report_str, title, save_path):\n    fig, ax = plt.subplots(figsize=(8, 4))\n    ax.text(0.5, 0.5, report_str, ha='center', va='center', fontsize=10, family='monospace')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(save_path, dpi=100)\n    plt.close()\n\nbert_report = classification_report(bert_true, bert_preds)\nreport_to_image(bert_report, 'BERT Classification Report', '/kaggle/working/bert_classification_report.png')\ndistil_report = classification_report(distil_true, distil_preds)\nreport_to_image(distil_report, 'DistilBERT Classification Report', '/kaggle/working/distilbert_classification_report.png')\n\n# Confusion matrices\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(bert_true, bert_preds)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\ncm_distil = confusion_matrix(distil_true, distil_preds)\nsns.heatmap(cm_distil, annot=True, fmt='d', cmap='Blues', ax=axs[1])\naxs[1].set_title('DistilBERT Confusion Matrix')\naxs[1].set_xlabel('Predicted')\naxs[1].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices.png', dpi=100)\nplt.close()\n\n# Save models\nbert_trainer.save_model('/kaggle/working/bert_model')\ndistil_trainer.save_model('/kaggle/working/distilbert_model')\n\n# Cleanup results directory to save space\nif os.path.exists('/kaggle/working/results'):\n    shutil.rmtree('/kaggle/working/results')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:37:38.169034Z","iopub.execute_input":"2025-11-09T06:37:38.169887Z","iopub.status.idle":"2025-11-09T06:40:09.476345Z","shell.execute_reply.started":"2025-11-09T06:37:38.169854Z","shell.execute_reply":"2025-11-09T06:40:09.475015Z"}},"outputs":[{"name":"stdout","text":"Training BERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af1b761ad9644b5497b73180d5c64f7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0996283ffe3340b38862f2e1962691d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806303a970a945ce9eb420461606fb0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbde64f05b884459aa51b2335aac1b78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e200f5c1bdce4319bb645ef40fc86614"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551ac7577aad452f811e51bc125ee397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"524a9c1357ce4e6ba31c9e686e7699d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aee79b5e0874baeb7d7e41d4b8303fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbd61db1b0aa4d79adb708f358bbc8b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 01:23, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.686992</td>\n      <td>0.482759</td>\n      <td>0.516129</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.686827</td>\n      <td>0.551724</td>\n      <td>0.551724</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.686664</td>\n      <td>0.551724</td>\n      <td>0.518519</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.686542</td>\n      <td>0.620690</td>\n      <td>0.521739</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.686560</td>\n      <td>0.655172</td>\n      <td>0.545455</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.687034</td>\n      <td>0.551724</td>\n      <td>0.518519</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.687122</td>\n      <td>0.517241</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.686989</td>\n      <td>0.551724</td>\n      <td>0.518519</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.687052</td>\n      <td>0.448276</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>0.686998</td>\n      <td>0.482759</td>\n      <td>0.516129</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Training DistilBERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"000b5749d58246909e12706c11065cc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b3a9184aa7049828f7d399a33207330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f70795de8ca42a8a407d41ec938f83a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"125756ad33ea4dcc8b41a04eb751cc79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfe952aef7d640f9bfc023650838e072"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b2ffd521ba54e7488b1b28f1f68c4a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"501bbc2a88384b6bb68d951b4860388b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 00:46, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.704373</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.703577</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.702255</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.700529</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.698876</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.696522</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.694662</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.693778</td>\n      <td>0.689655</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.695885</td>\n      <td>0.724138</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>0.700637</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/332577881.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# Plot training-validation curves (save with lower DPI)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BERT Train Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BERT Val Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistil_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistil_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DistilBERT Train Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (0,)"],"ename":"ValueError","evalue":"x and y must have same first dimension, but have shapes (10,) and (0,)","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"!pip install nlpaug","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:42:08.455695Z","iopub.execute_input":"2025-11-09T06:42:08.456040Z","iopub.status.idle":"2025-11-09T06:42:12.546281Z","shell.execute_reply.started":"2025-11-09T06:42:08.456007Z","shell.execute_reply":"2025-11-09T06:42:12.545505Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.3)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.5)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.20.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.10.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Split into train/test (80/20, stratified for imbalance)\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n# Data augmentation for training set to improve accuracy\naug = naw.SynonymAug(aug_src='wordnet', aug_max=3)\ntrain_df['text'] = train_df['text'].apply(lambda x: aug.augment(x) if np.random.rand() > 0.5 else x)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef train_model(model_name, tokenizer_class, model_class):\n    # Tokenize function\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n   \n    # Load tokenizer and model\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n   \n    # Gradual unfreezing: Freeze base layers\n    for param in model.base_model.parameters():\n        param.requires_grad = False\n   \n    # Tokenize datasets\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n   \n    # Compute class weights for imbalance\n    from sklearn.utils.class_weight import compute_class_weight\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n   \n    # Custom loss function with weights\n    def weighted_loss(logits, labels):\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        return loss_fn(logits, labels)\n   \n    # Compute metrics (accuracy, F1)\n    accuracy = load('accuracy')\n    f1 = load('f1')\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return {\n            'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n            'f1': f1.compute(predictions=predictions, references=labels)['f1']\n        }\n   \n    # Phase 1: Train classifier head only\n    training_args_phase1 = TrainingArguments(\n        output_dir='/kaggle/working/results_phase1',\n        num_train_epochs=2,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=100,\n        weight_decay=0.01,\n        learning_rate=1e-4,  # Higher for head\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer_phase1 = Trainer(\n        model=model,\n        args=training_args_phase1,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics\n    )\n   \n    trainer_phase1.train()\n   \n    # Unfreeze base layers for phase 2\n    for param in model.base_model.parameters():\n        param.requires_grad = True\n   \n    # Phase 2: Full fine-tuning\n    training_args = TrainingArguments(\n        output_dir='/kaggle/working/results',\n        num_train_epochs=10,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=200,\n        weight_decay=0.01,\n        learning_rate=2e-5,\n        lr_scheduler_type='cosine',\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=2,\n        logging_strategy='epoch',  # Enable to log train loss\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n   \n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics\n    )\n   \n    trainer.train(resume_from_checkpoint=training_args_phase1.output_dir)\n   \n    # Predict on test for probs and preds\n    predictions = trainer.predict(tokenized_test)\n    probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)[:, 1].numpy()\n    preds = np.argmax(predictions.predictions, axis=-1)\n    true_labels = predictions.label_ids\n   \n    # History for curves\n    history = trainer.state.log_history\n   \n    # Cleanup checkpoints\n    for dir_name in ['/kaggle/working/results', '/kaggle/working/results_phase1']:\n        if os.path.exists(dir_name):\n            shutil.rmtree(dir_name)\n   \n    return history, probs, preds, true_labels, trainer\n\n# Train BERT\nprint(\"Training BERT...\")\nbert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n    'bert-base-uncased', BertTokenizer, BertForSequenceClassification\n)\n\n# Train DistilBERT\nprint(\"Training DistilBERT...\")\ndistil_history, distil_probs, distil_preds, distil_true, distil_trainer = train_model(\n    'distilbert-base-uncased', DistilBertTokenizer, DistilBertForSequenceClassification\n)\n\n# Extract training/validation curves (accuracy and loss)\ndef extract_curves(history):\n    train_loss = [log['loss'] for log in history if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n    val_loss = [log['eval_loss'] for log in history if 'eval_loss' in log]\n    val_acc = [log['eval_accuracy'] for log in history if 'eval_accuracy' in log]\n    epochs = range(1, len(val_loss) + 1)\n    if len(train_loss) != len(epochs):\n        train_loss = train_loss[:len(epochs)]  # Truncate if mismatch\n    return epochs, train_loss, val_loss, val_acc\n\nbert_epochs, bert_train_loss, bert_val_loss, bert_val_acc = extract_curves(bert_history)\ndistil_epochs, distil_train_loss, distil_val_loss, distil_val_acc = extract_curves(distil_history)\n\n# Plot training-validation curves (save with lower DPI)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].plot(bert_epochs, bert_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(bert_epochs, bert_val_loss, label='BERT Val Loss')\naxs[0].plot(distil_epochs, distil_train_loss, label='DistilBERT Train Loss', linestyle='--')\naxs[0].plot(distil_epochs, distil_val_loss, label='DistilBERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\naxs[1].plot(bert_epochs, bert_val_acc, label='BERT Val Acc')\naxs[1].plot(distil_epochs, distil_val_acc, label='DistilBERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves.png', dpi=100)\nplt.close()\n\n# ROC curves\nfpr_bert, tpr_bert, _ = roc_curve(bert_true, bert_probs)\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nfpr_distil, tpr_distil, _ = roc_curve(distil_true, distil_probs)\nroc_auc_distil = auc(fpr_distil, tpr_distil)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot(fpr_distil, tpr_distil, label=f'DistilBERT (AUC = {roc_auc_distil:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves.png', dpi=100)\nplt.close()\n\n# Classification reports as images\ndef report_to_image(report_str, title, save_path):\n    fig, ax = plt.subplots(figsize=(8, 4))\n    ax.text(0.5, 0.5, report_str, ha='center', va='center', fontsize=10, family='monospace')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(save_path, dpi=100)\n    plt.close()\n\nbert_report = classification_report(bert_true, bert_preds)\nreport_to_image(bert_report, 'BERT Classification Report', '/kaggle/working/bert_classification_report.png')\ndistil_report = classification_report(distil_true, distil_preds)\nreport_to_image(distil_report, 'DistilBERT Classification Report', '/kaggle/working/distilbert_classification_report.png')\n\n# Confusion matrices\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(bert_true, bert_preds)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\ncm_distil = confusion_matrix(distil_true, distil_preds)\nsns.heatmap(cm_distil, annot=True, fmt='d', cmap='Blues', ax=axs[1])\naxs[1].set_title('DistilBERT Confusion Matrix')\naxs[1].set_xlabel('Predicted')\naxs[1].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices.png', dpi=100)\nplt.close()\n\n# Save models\nbert_trainer.save_model('/kaggle/working/bert_model')\ndistil_trainer.save_model('/kaggle/working/distilbert_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T06:50:22.182424Z","iopub.execute_input":"2025-11-09T06:50:22.182807Z","iopub.status.idle":"2025-11-09T06:50:22.291657Z","shell.execute_reply.started":"2025-11-09T06:50:22.182780Z","shell.execute_reply":"2025-11-09T06:50:22.290428Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nlpaug/model/word_dict/wordnet.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(cls, tokens)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang, loc)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang, loc)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2056632872.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Data augmentation for training set to improve accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0maug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSynonymAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_src\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Convert to Hugging Face Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/2056632872.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Data augmentation for training set to improve accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0maug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSynonymAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_src\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Convert to Hugging Face Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nlpaug/base_augmenter.py\u001b[0m in \u001b[0;36maugment\u001b[0;34m(self, data, n, num_thread)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# Single Thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_thread\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0maugmented_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m# Multi Thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nlpaug/base_augmenter.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# Single Thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_thread\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0maugmented_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m# Multi Thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nlpaug/augmenter/word/synonym.py\u001b[0m in \u001b[0;36msubstitute\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0moriginal_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_original_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0maug_idxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_aug_idxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nlpaug/model/word_dict/wordnet.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(cls, tokens)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang, loc)\u001b[0m\n\u001b[1;32m    178\u001b[0m         )\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparam_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang, loc)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mload_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nltk\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Split into train/test (80/20, stratified for imbalance)\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n# Data augmentation for training set to improve accuracy\naug = naw.SynonymAug(aug_src='wordnet', aug_max=3)\ntrain_df['text'] = train_df['text'].apply(lambda x: aug.augment(x)[0] if np.random.rand() > 0.5 else x)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef train_model(model_name, tokenizer_class, model_class):\n    # Tokenize function\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n   \n    # Load tokenizer and model\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n   \n    # Gradual unfreezing: Freeze base layers\n    for param in model.base_model.parameters():\n        param.requires_grad = False\n   \n    # Tokenize datasets\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n   \n    # Compute class weights for imbalance\n    from sklearn.utils.class_weight import compute_class_weight\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n   \n    # Custom loss function with weights\n    def weighted_loss(logits, labels):\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        return loss_fn(logits, labels)\n   \n    # Compute metrics (accuracy, F1)\n    accuracy = load('accuracy')\n    f1 = load('f1')\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return {\n            'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n            'f1': f1.compute(predictions=predictions, references=labels)['f1']\n        }\n   \n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n   \n    # Phase 1: Train classifier head only with weighted loss\n    training_args_phase1 = TrainingArguments(\n        output_dir='/kaggle/working/results_phase1',\n        num_train_epochs=2,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=100,\n        weight_decay=0.01,\n        learning_rate=1e-4,  # Higher for head\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer_phase1 = WeightedTrainer(\n        model=model,\n        args=training_args_phase1,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics\n    )\n   \n    trainer_phase1.train()\n   \n    # Unfreeze base layers for phase 2\n    for param in model.base_model.parameters():\n        param.requires_grad = True\n   \n    # Phase 2: Full fine-tuning\n    training_args = TrainingArguments(\n        output_dir='/kaggle/working/results',\n        num_train_epochs=10,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=200,\n        weight_decay=0.01,\n        learning_rate=2e-5,\n        lr_scheduler_type='cosine',\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=2,\n        logging_strategy='epoch',  # Enable to log train loss\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics\n    )\n   \n    trainer.train(resume_from_checkpoint=training_args_phase1.output_dir)\n   \n    # Predict on test for probs and preds\n    predictions = trainer.predict(tokenized_test)\n    probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)[:, 1].numpy()\n    preds = np.argmax(predictions.predictions, axis=-1)\n    true_labels = predictions.label_ids\n   \n    # History for curves\n    history = trainer.state.log_history\n   \n    # Cleanup checkpoints\n    for dir_name in ['/kaggle/working/results', '/kaggle/working/results_phase1']:\n        if os.path.exists(dir_name):\n            shutil.rmtree(dir_name)\n   \n    return history, probs, preds, true_labels, trainer\n\n# Train BERT\nprint(\"Training BERT...\")\nbert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n    'bert-base-uncased', BertTokenizer, BertForSequenceClassification\n)\n\n# Train DistilBERT\nprint(\"Training DistilBERT...\")\ndistil_history, distil_probs, distil_preds, distil_true, distil_trainer = train_model(\n    'distilbert-base-uncased', DistilBertTokenizer, DistilBertForSequenceClassification\n)\n\n# Extract training/validation curves (accuracy and loss)\ndef extract_curves(history):\n    train_loss = [log['loss'] for log in history if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n    val_loss = [log['eval_loss'] for log in history if 'eval_loss' in log]\n    val_acc = [log['eval_accuracy'] for log in history if 'eval_accuracy' in log]\n    epochs = range(1, len(val_loss) + 1)\n    if len(train_loss) != len(epochs):\n        train_loss = train_loss[:len(epochs)]  # Truncate if mismatch\n    return epochs, train_loss, val_loss, val_acc\n\nbert_epochs, bert_train_loss, bert_val_loss, bert_val_acc = extract_curves(bert_history)\ndistil_epochs, distil_train_loss, distil_val_loss, distil_val_acc = extract_curves(distil_history)\n\n# Plot training-validation curves (save with lower DPI)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].plot(bert_epochs, bert_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(bert_epochs, bert_val_loss, label='BERT Val Loss')\naxs[0].plot(distil_epochs, distil_train_loss, label='DistilBERT Train Loss', linestyle='--')\naxs[0].plot(distil_epochs, distil_val_loss, label='DistilBERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\naxs[1].plot(bert_epochs, bert_val_acc, label='BERT Val Acc')\naxs[1].plot(distil_epochs, distil_val_acc, label='DistilBERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves.png', dpi=100)\nplt.close()\n\n# ROC curves\nfpr_bert, tpr_bert, _ = roc_curve(bert_true, bert_probs)\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nfpr_distil, tpr_distil, _ = roc_curve(distil_true, distil_probs)\nroc_auc_distil = auc(fpr_distil, tpr_distil)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot(fpr_distil, tpr_distil, label=f'DistilBERT (AUC = {roc_auc_distil:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves.png', dpi=100)\nplt.close()\n\n# Classification reports as images\ndef report_to_image(report_str, title, save_path):\n    fig, ax = plt.subplots(figsize=(8, 4))\n    ax.text(0.5, 0.5, report_str, ha='center', va='center', fontsize=10, family='monospace')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(save_path, dpi=100)\n    plt.close()\n\nbert_report = classification_report(bert_true, bert_preds)\nreport_to_image(bert_report, 'BERT Classification Report', '/kaggle/working/bert_classification_report.png')\ndistil_report = classification_report(distil_true, distil_preds)\nreport_to_image(distil_report, 'DistilBERT Classification Report', '/kaggle/working/distilbert_classification_report.png')\n\n# Confusion matrices\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(bert_true, bert_preds)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\ncm_distil = confusion_matrix(distil_true, distil_preds)\nsns.heatmap(cm_distil, annot=True, fmt='d', cmap='Blues', ax=axs[1])\naxs[1].set_title('DistilBERT Confusion Matrix')\naxs[1].set_xlabel('Predicted')\naxs[1].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices.png', dpi=100)\nplt.close()\n\n# Save models\nbert_trainer.save_model('/kaggle/working/bert_model')\ndistil_trainer.save_model('/kaggle/working/distilbert_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:15:42.595124Z","iopub.execute_input":"2025-11-09T07:15:42.595933Z","iopub.status.idle":"2025-11-09T07:15:58.536112Z","shell.execute_reply.started":"2025-11-09T07:15:42.595909Z","shell.execute_reply":"2025-11-09T07:15:58.535170Z"}},"outputs":[{"name":"stdout","text":"Training BERT...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d808b54d68bd4bb383978e14b06edb08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d448eae0ad384b109ff3c34420f84f56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8/8 00:07, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.745100</td>\n      <td>0.709289</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.723000</td>\n      <td>0.708293</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2608470027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;31m# Train BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training BERT...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m bert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m )\n","\u001b[0;32m/tmp/ipykernel_48/2608470027.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, tokenizer_class, model_class)\u001b[0m\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args_phase1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Predict on test for probs and preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_deepspeed_enabled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2178\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2179\u001b[0m             \u001b[0;31m# In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2180\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINER_STATE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(self, resume_from_checkpoint, model)\u001b[0m\n\u001b[1;32m   2805\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0madapter_subdirs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m         ):\n\u001b[0;32m-> 2807\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find a valid checkpoint at {resume_from_checkpoint}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model from {resume_from_checkpoint}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Can't find a valid checkpoint at /kaggle/working/results_phase1"],"ename":"ValueError","evalue":"Can't find a valid checkpoint at /kaggle/working/results_phase1","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nltk\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Split into train/test (80/20, stratified for imbalance)\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n# Data augmentation for training set to improve accuracy\naug = naw.SynonymAug(aug_src='wordnet', aug_max=3)\ntrain_df['text'] = train_df['text'].apply(lambda x: aug.augment(x)[0] if np.random.rand() > 0.5 else x)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef train_model(model_name, tokenizer_class, model_class):\n    # Tokenize function\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n   \n    # Load tokenizer and model\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n   \n    # Gradual unfreezing: Freeze base layers\n    for param in model.base_model.parameters():\n        param.requires_grad = False\n   \n    # Tokenize datasets\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n   \n    # Compute class weights for imbalance\n    from sklearn.utils.class_weight import compute_class_weight\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n   \n    # Custom loss function with weights\n    def weighted_loss(logits, labels):\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        return loss_fn(logits, labels)\n   \n    # Compute metrics (accuracy, F1)\n    accuracy = load('accuracy')\n    f1 = load('f1')\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return {\n            'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n            'f1': f1.compute(predictions=predictions, references=labels)['f1']\n        }\n   \n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n   \n    # Phase 1: Train classifier head only with weighted loss\n    training_args_phase1 = TrainingArguments(\n        output_dir='/kaggle/working/results_phase1.2',\n        num_train_epochs=10,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=100,\n        weight_decay=0.01,\n        learning_rate=1e-4,  # Higher for head\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer_phase1 = WeightedTrainer(\n        model=model,\n        args=training_args_phase1,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics\n    )\n   \n    trainer_phase1.train()\n   \n    # Cleanup phase 1 checkpoints early\n    shutil.rmtree('/kaggle/working/results_phase1.2')\n   \n    # Unfreeze base layers for phase 2\n    for param in model.base_model.parameters():\n        param.requires_grad = True\n   \n    # Phase 2: Full fine-tuning\n    training_args = TrainingArguments(\n        output_dir='/kaggle/working/results2',\n        num_train_epochs=30,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=200,\n        weight_decay=0.01,\n        learning_rate=2e-5,\n        lr_scheduler_type='cosine',\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=2,\n        logging_strategy='epoch',  # Enable to log train loss\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics\n    )\n   \n    trainer.train()  # No resume_from_checkpoint needed\n   \n    # Predict on test for probs and preds\n    predictions = trainer.predict(tokenized_test)\n    probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)[:, 1].numpy()\n    preds = np.argmax(predictions.predictions, axis=-1)\n    true_labels = predictions.label_ids\n   \n    # History for curves\n    history = trainer.state.log_history\n   \n    # Cleanup checkpoints\n    shutil.rmtree('/kaggle/working/results2')\n   \n    return history, probs, preds, true_labels, trainer\n\n# Train BERT\nprint(\"Training BERT...\")\nbert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n    'bert-base-uncased', BertTokenizer, BertForSequenceClassification\n)\n\n# Train DistilBERT\nprint(\"Training DistilBERT...\")\ndistil_history, distil_probs, distil_preds, distil_true, distil_trainer = train_model(\n    'distilbert-base-uncased', DistilBertTokenizer, DistilBertForSequenceClassification\n)\n\n# Extract training/validation curves (accuracy and loss)\ndef extract_curves(history):\n    train_loss = [log['loss'] for log in history if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n    val_loss = [log['eval_loss'] for log in history if 'eval_loss' in log]\n    val_acc = [log['eval_accuracy'] for log in history if 'eval_accuracy' in log]\n    epochs = range(1, len(val_loss) + 1)\n    if len(train_loss) != len(epochs):\n        train_loss = train_loss[:len(epochs)]  # Truncate if mismatch\n    return epochs, train_loss, val_loss, val_acc\n\nbert_epochs, bert_train_loss, bert_val_loss, bert_val_acc = extract_curves(bert_history)\ndistil_epochs, distil_train_loss, distil_val_loss, distil_val_acc = extract_curves(distil_history)\n\n# Plot training-validation curves (save with lower DPI)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].plot(bert_epochs, bert_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(bert_epochs, bert_val_loss, label='BERT Val Loss')\naxs[0].plot(distil_epochs, distil_train_loss, label='DistilBERT Train Loss', linestyle='--')\naxs[0].plot(distil_epochs, distil_val_loss, label='DistilBERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\naxs[1].plot(bert_epochs, bert_val_acc, label='BERT Val Acc')\naxs[1].plot(distil_epochs, distil_val_acc, label='DistilBERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves-2.png', dpi=100)\nplt.close()\n\n# ROC curves\nfpr_bert, tpr_bert, _ = roc_curve(bert_true, bert_probs)\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nfpr_distil, tpr_distil, _ = roc_curve(distil_true, distil_probs)\nroc_auc_distil = auc(fpr_distil, tpr_distil)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot(fpr_distil, tpr_distil, label=f'DistilBERT (AUC = {roc_auc_distil:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves-2.png', dpi=100)\nplt.close()\n\n# Classification reports as images\ndef report_to_image(report_str, title, save_path):\n    fig, ax = plt.subplots(figsize=(8, 4))\n    ax.text(0.5, 0.5, report_str, ha='center', va='center', fontsize=10, family='monospace')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(save_path, dpi=100)\n    plt.close()\n\nbert_report = classification_report(bert_true, bert_preds)\nreport_to_image(bert_report, 'BERT Classification Report', '/kaggle/working/bert_classification_report-2.png')\ndistil_report = classification_report(distil_true, distil_preds)\nreport_to_image(distil_report, 'DistilBERT Classification Report', '/kaggle/working/distilbert_classification_report-2.png')\n\n# Confusion matrices\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(bert_true, bert_preds)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\ncm_distil = confusion_matrix(distil_true, distil_preds)\nsns.heatmap(cm_distil, annot=True, fmt='d', cmap='Blues', ax=axs[1])\naxs[1].set_title('DistilBERT Confusion Matrix')\naxs[1].set_xlabel('Predicted')\naxs[1].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-2.png', dpi=100)\nplt.close()\n# Save models\nbert_trainer.save_model('/kaggle/working/bert_model-2')\ndistil_trainer.save_model('/kaggle/working/distilbert_model-2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:40:28.865184Z","iopub.execute_input":"2025-11-09T07:40:28.865920Z","iopub.status.idle":"2025-11-09T07:48:46.082525Z","shell.execute_reply.started":"2025-11-09T07:40:28.865898Z","shell.execute_reply":"2025-11-09T07:48:46.081887Z"}},"outputs":[{"name":"stdout","text":"Training BERT...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99800cace874aa2a527cf38c2d61c47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e5efad68fa948c8929824031e44e5a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 00:37, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.742400</td>\n      <td>0.709297</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.724400</td>\n      <td>0.708307</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.751800</td>\n      <td>0.706917</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.724000</td>\n      <td>0.705002</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.711000</td>\n      <td>0.702722</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.699200</td>\n      <td>0.700170</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.707500</td>\n      <td>0.697671</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.708500</td>\n      <td>0.695605</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.722000</td>\n      <td>0.693612</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.705800</td>\n      <td>0.691932</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 04:32, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.741500</td>\n      <td>0.706862</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.721500</td>\n      <td>0.699861</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.739500</td>\n      <td>0.693671</td>\n      <td>0.310345</td>\n      <td>0.473684</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.704900</td>\n      <td>0.690774</td>\n      <td>0.344828</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.693900</td>\n      <td>0.693039</td>\n      <td>0.551724</td>\n      <td>0.133333</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.684800</td>\n      <td>0.699124</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.694300</td>\n      <td>0.703549</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.678900</td>\n      <td>0.705089</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.689000</td>\n      <td>0.700601</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.679400</td>\n      <td>0.696900</td>\n      <td>0.655172</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.682100</td>\n      <td>0.700060</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.662900</td>\n      <td>0.705190</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.680900</td>\n      <td>0.719572</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.642800</td>\n      <td>0.711938</td>\n      <td>0.517241</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.637200</td>\n      <td>0.730619</td>\n      <td>0.689655</td>\n      <td>0.181818</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.595300</td>\n      <td>0.738687</td>\n      <td>0.586207</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.602000</td>\n      <td>0.764929</td>\n      <td>0.586207</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.547600</td>\n      <td>0.804579</td>\n      <td>0.689655</td>\n      <td>0.181818</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.528900</td>\n      <td>0.800754</td>\n      <td>0.482759</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.487700</td>\n      <td>0.865036</td>\n      <td>0.724138</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.424000</td>\n      <td>0.876425</td>\n      <td>0.586207</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.430800</td>\n      <td>0.875040</td>\n      <td>0.689655</td>\n      <td>0.181818</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.332600</td>\n      <td>0.955501</td>\n      <td>0.724138</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.335400</td>\n      <td>0.892465</td>\n      <td>0.551724</td>\n      <td>0.235294</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.344100</td>\n      <td>1.048310</td>\n      <td>0.724138</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.263100</td>\n      <td>1.011287</td>\n      <td>0.724138</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.368800</td>\n      <td>0.927013</td>\n      <td>0.655172</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.214700</td>\n      <td>1.140174</td>\n      <td>0.724138</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.187700</td>\n      <td>1.138857</td>\n      <td>0.724138</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.170800</td>\n      <td>1.034846</td>\n      <td>0.620690</td>\n      <td>0.266667</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Training DistilBERT...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88bd62642a364c43960de23d21b130dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e9f8d80f2b4114aa656a9288d5b46a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 00:20, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.692700</td>\n      <td>0.697810</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.689600</td>\n      <td>0.697614</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.695700</td>\n      <td>0.697086</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.691100</td>\n      <td>0.696545</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.691100</td>\n      <td>0.695977</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.692300</td>\n      <td>0.695089</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.694000</td>\n      <td>0.694613</td>\n      <td>0.655172</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.694500</td>\n      <td>0.694171</td>\n      <td>0.413793</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.691200</td>\n      <td>0.693582</td>\n      <td>0.482759</td>\n      <td>0.516129</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.688500</td>\n      <td>0.693357</td>\n      <td>0.448276</td>\n      <td>0.500000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 02:27, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.689400</td>\n      <td>0.693576</td>\n      <td>0.482759</td>\n      <td>0.516129</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.684400</td>\n      <td>0.693750</td>\n      <td>0.413793</td>\n      <td>0.413793</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.689300</td>\n      <td>0.694210</td>\n      <td>0.379310</td>\n      <td>0.181818</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.684600</td>\n      <td>0.694669</td>\n      <td>0.517241</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.682500</td>\n      <td>0.695459</td>\n      <td>0.620690</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.686200</td>\n      <td>0.695045</td>\n      <td>0.551724</td>\n      <td>0.133333</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.685800</td>\n      <td>0.694456</td>\n      <td>0.551724</td>\n      <td>0.133333</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.690100</td>\n      <td>0.694106</td>\n      <td>0.344828</td>\n      <td>0.173913</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.687100</td>\n      <td>0.693630</td>\n      <td>0.413793</td>\n      <td>0.451613</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.685500</td>\n      <td>0.693354</td>\n      <td>0.448276</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.685000</td>\n      <td>0.698318</td>\n      <td>0.655172</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.686300</td>\n      <td>0.706847</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.707000</td>\n      <td>0.708135</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.695200</td>\n      <td>0.697222</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.681900</td>\n      <td>0.691610</td>\n      <td>0.655172</td>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.682700</td>\n      <td>0.690433</td>\n      <td>0.448276</td>\n      <td>0.529412</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.682700</td>\n      <td>0.690163</td>\n      <td>0.344828</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.683600</td>\n      <td>0.690636</td>\n      <td>0.344828</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.675200</td>\n      <td>0.693324</td>\n      <td>0.482759</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.666000</td>\n      <td>0.696707</td>\n      <td>0.655172</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.668800</td>\n      <td>0.699556</td>\n      <td>0.655172</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.650700</td>\n      <td>0.697199</td>\n      <td>0.517241</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.638500</td>\n      <td>0.698653</td>\n      <td>0.413793</td>\n      <td>0.260870</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.625700</td>\n      <td>0.719939</td>\n      <td>0.655172</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.600400</td>\n      <td>0.719515</td>\n      <td>0.689655</td>\n      <td>0.307692</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.551300</td>\n      <td>0.710026</td>\n      <td>0.586207</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.494300</td>\n      <td>0.746335</td>\n      <td>0.689655</td>\n      <td>0.307692</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.460300</td>\n      <td>0.749721</td>\n      <td>0.551724</td>\n      <td>0.380952</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.376500</td>\n      <td>0.837450</td>\n      <td>0.655172</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.336100</td>\n      <td>0.760609</td>\n      <td>0.620690</td>\n      <td>0.421053</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nltk\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Split into train/test (80/20, stratified for imbalance)\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n# Data augmentation for training set to improve accuracy (increased probability)\naug = naw.SynonymAug(aug_src='wordnet', aug_max=3)\ntrain_df['text'] = train_df['text'].apply(lambda x: aug.augment(x)[0] if np.random.rand() > 0.25 else x)  # 75% chance\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef train_model(model_name, tokenizer_class, model_class):\n    # Tokenize function\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n   \n    # Load tokenizer and model\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n   \n    # Gradual unfreezing: Freeze base layers\n    for param in model.base_model.parameters():\n        param.requires_grad = False\n   \n    # Tokenize datasets\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n   \n    # Compute class weights for imbalance\n    from sklearn.utils.class_weight import compute_class_weight\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n   \n    # Custom loss function with weights\n    def weighted_loss(logits, labels):\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        return loss_fn(logits, labels)\n   \n    # Compute metrics (accuracy, F1)\n    accuracy = load('accuracy')\n    f1 = load('f1')\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return {\n            'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n            'f1': f1.compute(predictions=predictions, references=labels)['f1']\n        }\n   \n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n   \n    # Phase 1: Train classifier head only with weighted loss\n    training_args_phase1 = TrainingArguments(\n        output_dir='/kaggle/working/results_phase1.3',\n        num_train_epochs=3,  # Reduced for efficiency\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=100,\n        weight_decay=0.01,\n        learning_rate=1e-4,  # Higher for head\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer_phase1 = WeightedTrainer(\n        model=model,\n        args=training_args_phase1,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Add early stopping\n    )\n   \n    trainer_phase1.train()\n   \n    # Cleanup phase 1 checkpoints early\n    shutil.rmtree('/kaggle/working/results_phase1.3')\n   \n    # Unfreeze base layers for phase 2\n    for param in model.base_model.parameters():\n        param.requires_grad = True\n   \n    # Phase 2: Full fine-tuning\n    training_args = TrainingArguments(\n        output_dir='/kaggle/working/results-3',\n        num_train_epochs=20,  # Max, but early stopping will halt if needed\n        per_device_train_batch_size=8,  # Reduced for finer gradients, better accuracy\n        per_device_eval_batch_size=16,\n        warmup_steps=200,\n        weight_decay=0.01,\n        learning_rate=3e-5,  # Slightly higher for faster convergence\n        lr_scheduler_type='cosine',\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=2,\n        logging_strategy='epoch',  # Enable to log train loss\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Add early stopping\n    )\n   \n    trainer.train()  # No resume_from_checkpoint needed\n   \n    # Predict on test for probs and preds\n    predictions = trainer.predict(tokenized_test)\n    probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)[:, 1].numpy()\n    preds = np.argmax(predictions.predictions, axis=-1)\n    true_labels = predictions.label_ids\n   \n    # History for curves\n    history = trainer.state.log_history\n   \n    # Cleanup checkpoints\n    shutil.rmtree('/kaggle/working/results-3')\n   \n    return history, probs, preds, true_labels, trainer\n\n# Train BERT\nprint(\"Training BERT...\")\nbert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n    'bert-base-uncased', BertTokenizer, BertForSequenceClassification\n)\n\n# Train DistilBERT\nprint(\"Training DistilBERT...\")\ndistil_history, distil_probs, distil_preds, distil_true, distil_trainer = train_model(\n    'distilbert-base-uncased', DistilBertTokenizer, DistilBertForSequenceClassification\n)\n\n# Extract training/validation curves (accuracy and loss)\ndef extract_curves(history):\n    train_loss = [log['loss'] for log in history if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n    val_loss = [log['eval_loss'] for log in history if 'eval_loss' in log]\n    val_acc = [log['eval_accuracy'] for log in history if 'eval_accuracy' in log]\n    epochs = range(1, len(val_loss) + 1)\n    if len(train_loss) != len(epochs):\n        train_loss = train_loss[:len(epochs)]  # Truncate if mismatch\n    return epochs, train_loss, val_loss, val_acc\n\nbert_epochs, bert_train_loss, bert_val_loss, bert_val_acc = extract_curves(bert_history)\ndistil_epochs, distil_train_loss, distil_val_loss, distil_val_acc = extract_curves(distil_history)\n\n# Plot training-validation curves (save with lower DPI)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].plot(bert_epochs, bert_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(bert_epochs, bert_val_loss, label='BERT Val Loss')\naxs[0].plot(distil_epochs, distil_train_loss, label='DistilBERT Train Loss', linestyle='--')\naxs[0].plot(distil_epochs, distil_val_loss, label='DistilBERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\naxs[1].plot(bert_epochs, bert_val_acc, label='BERT Val Acc')\naxs[1].plot(distil_epochs, distil_val_acc, label='DistilBERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves-3.png', dpi=100)\nplt.close()\n\n# ROC curves\nfpr_bert, tpr_bert, _ = roc_curve(bert_true, bert_probs)\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nfpr_distil, tpr_distil, _ = roc_curve(distil_true, distil_probs)\nroc_auc_distil = auc(fpr_distil, tpr_distil)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot(fpr_distil, tpr_distil, label=f'DistilBERT (AUC = {roc_auc_distil:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves.png', dpi=100)\nplt.close()\n\n# Classification reports as images\ndef report_to_image(report_str, title, save_path):\n    fig, ax = plt.subplots(figsize=(8, 4))\n    ax.text(0.5, 0.5, report_str, ha='center', va='center', fontsize=10, family='monospace')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(save_path, dpi=100)\n    plt.close()\n\nbert_report = classification_report(bert_true, bert_preds)\nreport_to_image(bert_report, 'BERT Classification Report', '/kaggle/working/bert_classification_report-3.png')\ndistil_report = classification_report(distil_true, distil_preds)\nreport_to_image(distil_report, 'DistilBERT Classification Report', '/kaggle/working/distilbert_classification_report-3.png')\n\n# Confusion matrices\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(bert_true, bert_preds)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\ncm_distil = confusion_matrix(distil_true, distil_preds)\nsns.heatmap(cm_distil, annot=True, fmt='d', cmap='Blues', ax=axs[1])\naxs[1].set_title('DistilBERT Confusion Matrix')\naxs[1].set_xlabel('Predicted')\naxs[1].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-3.png', dpi=100)\nplt.close()\n\n# Save models\nbert_trainer.save_model('/kaggle/working/bert_model-3')\ndistil_trainer.save_model('/kaggle/working/distilbert_model-3')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:00:23.992127Z","iopub.execute_input":"2025-11-09T08:00:23.992712Z","iopub.status.idle":"2025-11-09T08:02:06.529751Z","shell.execute_reply.started":"2025-11-09T08:00:23.992675Z","shell.execute_reply":"2025-11-09T08:02:06.529101Z"}},"outputs":[{"name":"stdout","text":"Training BERT...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be17a3ce60fe47a190d7fe51c3c8f468"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6726f8ff6dbc4b22956f1d1f2356d046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 00:10, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.697300</td>\n      <td>0.679195</td>\n      <td>0.379310</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.694400</td>\n      <td>0.679133</td>\n      <td>0.379310</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.702000</td>\n      <td>0.679085</td>\n      <td>0.379310</td>\n      <td>0.500000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='35' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 35/140 00:46 < 02:27, 0.71 it/s, Epoch 5/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.693900</td>\n      <td>0.679388</td>\n      <td>0.344828</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.709900</td>\n      <td>0.677268</td>\n      <td>0.413793</td>\n      <td>0.514286</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.708600</td>\n      <td>0.679757</td>\n      <td>0.724138</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.704500</td>\n      <td>0.688759</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.692200</td>\n      <td>0.682695</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Training DistilBERT...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe47bf2eec454617a42a4c8f09dc6ec7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744e0103da8a4d8eb55262fd7fd4745b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 00:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.692600</td>\n      <td>0.697826</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.698400</td>\n      <td>0.697474</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.694300</td>\n      <td>0.696801</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='28' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 28/140 00:19 < 01:25, 1.31 it/s, Epoch 4/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.695500</td>\n      <td>0.697739</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.690800</td>\n      <td>0.699045</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.694700</td>\n      <td>0.701243</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.697800</td>\n      <td>0.698587</td>\n      <td>0.689655</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\nimport nltk\nfrom imbalanced_learn.over_sampling import SMOTE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Split into train/test (80/20, stratified for imbalance)\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n# Multi-round augmentation for training set (synonym + insertion)\naug_syn = naw.SynonymAug(aug_src='wordnet', aug_max=5)\naug_ins = nac.RandomCharAug(action=\"insert\", aug_char_max=3)\ntrain_df['text'] = train_df['text'].apply(lambda x: aug_syn.augment(aug_ins.augment(x)[0])[0] if np.random.rand() > 0.25 else x)  # 75% chance, 2 rounds\n\n# Oversampling with SMOTE (TF-IDF vectorization for numerical input)\nvectorizer = TfidfVectorizer(max_features=5000)\nX_train_vec = vectorizer.fit_transform(train_df['text'])\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train_vec, train_df['label'])\n\n# Reconstruct augmented/oversampled DataFrame (inverse transform for text)\ntrain_df_res = pd.DataFrame({'text': vectorizer.inverse_transform(X_train_res), 'label': y_train_res})\ntrain_df_res['text'] = train_df_res['text'].apply(lambda x: ' '.join(x))\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df_res[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef train_model(model_name, tokenizer_class, model_class):\n    # Tokenize function\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n   \n    # Load tokenizer and model (use Auto for flexibility)\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n   \n    # Gradual unfreezing: Freeze base layers\n    for param in model.base_model.parameters():\n        param.requires_grad = False\n   \n    # Tokenize datasets\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n   \n    # Compute class weights for imbalance (recompute on resampled)\n    from sklearn.utils.class_weight import compute_class_weight\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df_res['label']), y=train_df_res['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n   \n    # Custom loss function with weights\n    def weighted_loss(logits, labels):\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        return loss_fn(logits, labels)\n   \n    # Compute metrics (accuracy, F1)\n    accuracy = load('accuracy')\n    f1 = load('f1')\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return {\n            'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n            'f1': f1.compute(predictions=predictions, references=labels)['f1']\n        }\n   \n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n   \n    # Phase 1: Train classifier head only with weighted loss\n    training_args_phase1 = TrainingArguments(\n        output_dir='/kaggle/working/results_phase1.4',\n        num_train_epochs=5,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=100,\n        weight_decay=0.01,\n        learning_rate=1e-4,\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer_phase1 = WeightedTrainer(\n        model=model,\n        args=training_args_phase1,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]  # Increased patience\n    )\n   \n    trainer_phase1.train()\n   \n    # Cleanup phase 1 checkpoints early\n    shutil.rmtree('/kaggle/working/results_phase1.4')\n   \n    # Unfreeze base layers for phase 2\n    for param in model.base_model.parameters():\n        param.requires_grad = True\n   \n    # Phase 2: Full fine-tuning\n    training_args = TrainingArguments(\n        output_dir='/kaggle/working/results-4',\n        num_train_epochs=30,  # Max for deeper training\n        per_device_train_batch_size=16,  # Balanced for GPU\n        per_device_eval_batch_size=16,\n        warmup_steps=200,\n        weight_decay=0.005,  # Lower to reduce regularization\n        learning_rate=1e-5,  # Lower for stable high accuracy\n        lr_scheduler_type='cosine',\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=2,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]  # Increased for more training\n    )\n   \n    trainer.train()  # No resume_from_checkpoint needed\n   \n    # Predict on test for probs and preds\n    predictions = trainer.predict(tokenized_test)\n    probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)[:, 1].numpy()\n    preds = np.argmax(predictions.predictions, axis=-1)\n    true_labels = predictions.label_ids\n   \n    # History for curves\n    history = trainer.state.log_history\n   \n    # Cleanup checkpoints\n    shutil.rmtree('/kaggle/working/results-4')\n   \n    return history, probs, preds, true_labels, trainer\n\n# Train BERT (switch to 'roberta-base' for potential +2-5% accuracy)\nprint(\"Training BERT (RoBERTa variant)...\")\nbert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n    'roberta-base', AutoTokenizer, AutoModelForSequenceClassification\n)\n\n# Train DistilBERT\nprint(\"Training DistilBERT...\")\ndistil_history, distil_probs, distil_preds, distil_true, distil_trainer = train_model(\n    'distilbert-base-uncased', DistilBertTokenizer, DistilBertForSequenceClassification\n)\n\n# Extract training/validation curves (accuracy and loss)\ndef extract_curves(history):\n    train_loss = [log['loss'] for log in history if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n    val_loss = [log['eval_loss'] for log in history if 'eval_loss' in log]\n    val_acc = [log['eval_accuracy'] for log in history if 'eval_accuracy' in log]\n    epochs = range(1, len(val_loss) + 1)\n    if len(train_loss) != len(epochs):\n        train_loss = train_loss[:len(epochs)]  # Truncate if mismatch\n    return epochs, train_loss, val_loss, val_acc\n\nbert_epochs, bert_train_loss, bert_val_loss, bert_val_acc = extract_curves(bert_history)\ndistil_epochs, distil_train_loss, distil_val_loss, distil_val_acc = extract_curves(distil_history)\n\n# Plot training-validation curves (save with lower DPI)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].plot(bert_epochs, bert_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(bert_epochs, bert_val_loss, label='BERT Val Loss')\naxs[0].plot(distil_epochs, distil_train_loss, label='DistilBERT Train Loss', linestyle='--')\naxs[0].plot(distil_epochs, distil_val_loss, label='DistilBERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\naxs[1].plot(bert_epochs, bert_val_acc, label='BERT Val Acc')\naxs[1].plot(distil_epochs, distil_val_acc, label='DistilBERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves-4.png', dpi=100)\nplt.close()\n\n# ROC curves\nfpr_bert, tpr_bert, _ = roc_curve(bert_true, bert_probs)\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nfpr_distil, tpr_distil, _ = roc_curve(distil_true, distil_probs)\nroc_auc_distil = auc(fpr_distil, tpr_distil)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot(fpr_distil, tpr_distil, label=f'DistilBERT (AUC = {roc_auc_distil:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves-4.png', dpi=100)\nplt.close()\n\n# Classification reports as images\ndef report_to_image(report_str, title, save_path):\n    fig, ax = plt.subplots(figsize=(8, 4))\n    ax.text(0.5, 0.5, report_str, ha='center', va='center', fontsize=10, family='monospace')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(save_path, dpi=100)\n    plt.close()\n\nbert_report = classification_report(bert_true, bert_preds)\nreport_to_image(bert_report, 'BERT Classification Report', '/kaggle/working/bert_classification_report-4.png')\ndistil_report = classification_report(distil_true, distil_preds)\nreport_to_image(distil_report, 'DistilBERT Classification Report', '/kaggle/working/distilbert_classification_report-4.png')\n\n# Confusion matrices\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(bert_true, bert_preds)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\ncm_distil = confusion_matrix(distil_true, distil_preds)\nsns.heatmap(cm_distil, annot=True, fmt='d', cmap='Blues', ax=axs[1])\naxs[1].set_title('DistilBERT Confusion Matrix')\naxs[1].set_xlabel('Predicted')\naxs[1].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-4.png', dpi=100)\nplt.close()\n\n# Save models\nbert_trainer.save_model('/kaggle/working/bert_model-4')\ndistil_trainer.save_model('/kaggle/working/distilbert_model-4')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:20:40.841200Z","iopub.execute_input":"2025-11-09T08:20:40.841897Z","iopub.status.idle":"2025-11-09T08:20:40.887849Z","shell.execute_reply.started":"2025-11-09T08:20:40.841874Z","shell.execute_reply":"2025-11-09T08:20:40.886896Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/435686598.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnlpaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimbalanced_learn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imbalanced_learn'"],"ename":"ModuleNotFoundError","evalue":"No module named 'imbalanced_learn'","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\nimport nltk\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Split into train/test (80/20, stratified for imbalance)\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n# Multi-round augmentation for training set (synonym + insertion + deletion for more diversity)\naug_syn = naw.SynonymAug(aug_src='wordnet', aug_max=5)\naug_ins = nac.RandomCharAug(action=\"insert\", aug_char_max=3)\naug_del = nac.RandomCharAug(action=\"delete\", aug_char_max=2)\ntrain_df['text'] = train_df['text'].apply(lambda x: aug_syn.augment(aug_ins.augment(aug_del.augment(x)[0])[0])[0] if np.random.rand() > 0.2 else x)  # 80% chance, 3 rounds\n\n# Manual oversampling of minority class (duplicate samples to balance)\nmajority_class = train_df['label'].value_counts().idxmax()\nminority_class = 1 - majority_class\nmajority_count = len(train_df[train_df['label'] == majority_class])\nminority_df = train_df[train_df['label'] == minority_class]\noversample_count = majority_count - len(minority_df)\nif oversample_count > 0:\n    minority_oversampled = minority_df.sample(oversample_count, replace=True, random_state=42)\n    train_df = pd.concat([train_df, minority_oversampled]).reset_index(drop=True)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef train_model(model_name, tokenizer_class, model_class):\n    # Tokenize function\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n   \n    # Load tokenizer and model (use Auto for flexibility)\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n   \n    # Gradual unfreezing: Freeze base layers\n    for param in model.base_model.parameters():\n        param.requires_grad = False\n   \n    # Tokenize datasets\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n   \n    # Compute class weights for imbalance (on balanced train_df)\n    from sklearn.utils.class_weight import compute_class_weight\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n   \n    # Custom loss function with weights\n    def weighted_loss(logits, labels):\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        return loss_fn(logits, labels)\n   \n    # Compute metrics (accuracy, F1)\n    accuracy = load('accuracy')\n    f1 = load('f1')\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return {\n            'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n            'f1': f1.compute(predictions=predictions, references=labels)['f1']\n        }\n   \n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n   \n    # Phase 1: Train classifier head only with weighted loss\n    training_args_phase1 = TrainingArguments(\n        output_dir='/kaggle/working/results_phase1.5',\n        num_train_epochs=10,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=100,\n        weight_decay=0.01,\n        learning_rate=1e-4,\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer_phase1 = WeightedTrainer(\n        model=model,\n        args=training_args_phase1,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics,\n        # callbacks=[EarlyStoppingCallback(early_stopping_patience=20)]  # Increased patience\n    )\n   \n    trainer_phase1.train()\n   \n    # Cleanup phase 1 checkpoints early\n    shutil.rmtree('/kaggle/working/results_phase1.5')\n   \n    # Unfreeze base layers for phase 2\n    for param in model.base_model.parameters():\n        param.requires_grad = True\n   \n    # Phase 2: Full fine-tuning\n    training_args = TrainingArguments(\n        output_dir='/kaggle/working/results-5',\n        num_train_epochs=30,  # Max for deeper training\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=200,\n        weight_decay=0.01,\n        learning_rate=3e-5,  # Balanced for convergence\n        lr_scheduler_type='cosine',\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=2,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n   \n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        compute_metrics=compute_metrics,\n        # callbacks=[EarlyStoppingCallback(early_stopping_patience=20)]  # Increased for more training\n    )\n   \n    trainer.train()  # No resume_from_checkpoint needed\n   \n    # Predict on test for probs and preds\n    predictions = trainer.predict(tokenized_test)\n    probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)[:, 1].numpy()\n    preds = np.argmax(predictions.predictions, axis=-1)\n    true_labels = predictions.label_ids\n   \n    # History for curves\n    history = trainer.state.log_history\n   \n    # Cleanup checkpoints\n    shutil.rmtree('/kaggle/working/results-5')\n   \n    return history, probs, preds, true_labels, trainer\n\n# Train BERT (switch to 'google/electra-base-discriminator' for +3-5% potential)\nprint(\"Training BERT (ELECTRA variant)...\")\nbert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n    'google/electra-base-discriminator', AutoTokenizer, AutoModelForSequenceClassification\n)\n\n# Train DistilBERT (switch to 'distilroberta-base' for better performance)\nprint(\"Training DistilBERT (DistilRoBERTa variant)...\")\ndistil_history, distil_probs, distil_preds, distil_true, distil_trainer = train_model(\n    'distilroberta-base', AutoTokenizer, AutoModelForSequenceClassification\n)\n\n# Extract training/validation curves (accuracy and loss)\ndef extract_curves(history):\n    train_loss = [log['loss'] for log in history if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n    val_loss = [log['eval_loss'] for log in history if 'eval_loss' in log]\n    val_acc = [log['eval_accuracy'] for log in history if 'eval_accuracy' in log]\n    epochs = range(1, len(val_loss) + 1)\n    if len(train_loss) != len(epochs):\n        train_loss = train_loss[:len(epochs)]  # Truncate if mismatch\n    return epochs, train_loss, val_loss, val_acc\n\nbert_epochs, bert_train_loss, bert_val_loss, bert_val_acc = extract_curves(bert_history)\ndistil_epochs, distil_train_loss, distil_val_loss, distil_val_acc = extract_curves(distil_history)\n\n# Plot training-validation curves (save with lower DPI)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].plot(bert_epochs, bert_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(bert_epochs, bert_val_loss, label='BERT Val Loss')\naxs[0].plot(distil_epochs, distil_train_loss, label='DistilBERT Train Loss', linestyle='--')\naxs[0].plot(distil_epochs, distil_val_loss, label='DistilBERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\naxs[1].plot(bert_epochs, bert_val_acc, label='BERT Val Acc')\naxs[1].plot(distil_epochs, distil_val_acc, label='DistilBERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves-5.png', dpi=100)\nplt.close()\n\n# ROC curves\nfpr_bert, tpr_bert, _ = roc_curve(bert_true, bert_probs)\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nfpr_distil, tpr_distil, _ = roc_curve(distil_true, distil_probs)\nroc_auc_distil = auc(fpr_distil, tpr_distil)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot(fpr_distil, tpr_distil, label=f'DistilBERT (AUC = {roc_auc_distil:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves-5.png', dpi=100)\nplt.close()\n\n# Classification reports as images\ndef report_to_image(report_str, title, save_path):\n    fig, ax = plt.subplots(figsize=(8, 4))\n    ax.text(0.5, 0.5, report_str, ha='center', va='center', fontsize=10, family='monospace')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(save_path, dpi=100)\n    plt.close()\n\nbert_report = classification_report(bert_true, bert_preds)\nreport_to_image(bert_report, 'BERT Classification Report', '/kaggle/working/bert_classification_report-5.png')\ndistil_report = classification_report(distil_true, distil_preds)\nreport_to_image(distil_report, 'DistilBERT Classification Report', '/kaggle/working/distilbert_classification_report-5.png')\n\n# Confusion matrices\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(bert_true, bert_preds)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\ncm_distil = confusion_matrix(distil_true, distil_preds)\nsns.heatmap(cm_distil, annot=True, fmt='d', cmap='Blues', ax=axs[1])\naxs[1].set_title('DistilBERT Confusion Matrix')\naxs[1].set_xlabel('Predicted')\naxs[1].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-5.png', dpi=100)\nplt.close()\n\n# Save models\nbert_trainer.save_model('/kaggle/working/bert_model-5')\ndistil_trainer.save_model('/kaggle/working/distilbert_model-5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:31:24.374631Z","iopub.execute_input":"2025-11-09T08:31:24.375256Z","iopub.status.idle":"2025-11-09T08:35:35.048905Z","shell.execute_reply.started":"2025-11-09T08:31:24.375234Z","shell.execute_reply":"2025-11-09T08:35:35.047895Z"}},"outputs":[{"name":"stdout","text":"Training BERT (ELECTRA variant)...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/158 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b6b12312cf4a4ab57cd9be58bd2ae4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e92f578c61ed41f48702cc7ec0bb974b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:45, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.696500</td>\n      <td>0.698530</td>\n      <td>0.379310</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.689000</td>\n      <td>0.699323</td>\n      <td>0.379310</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.689700</td>\n      <td>0.701834</td>\n      <td>0.344828</td>\n      <td>0.424242</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.692200</td>\n      <td>0.702223</td>\n      <td>0.379310</td>\n      <td>0.437500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.693100</td>\n      <td>0.698715</td>\n      <td>0.344828</td>\n      <td>0.240000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.690900</td>\n      <td>0.698548</td>\n      <td>0.344828</td>\n      <td>0.173913</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.692300</td>\n      <td>0.693423</td>\n      <td>0.551724</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.691000</td>\n      <td>0.689913</td>\n      <td>0.620690</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.680600</td>\n      <td>0.701306</td>\n      <td>0.344828</td>\n      <td>0.095238</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.682800</td>\n      <td>0.706814</td>\n      <td>0.379310</td>\n      <td>0.250000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='91' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 91/150 03:11 < 02:06, 0.47 it/s, Epoch 18/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.693700</td>\n      <td>0.702352</td>\n      <td>0.344828</td>\n      <td>0.387097</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.685100</td>\n      <td>0.701415</td>\n      <td>0.379310</td>\n      <td>0.357143</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.684700</td>\n      <td>0.701023</td>\n      <td>0.344828</td>\n      <td>0.296296</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.688900</td>\n      <td>0.697856</td>\n      <td>0.379310</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.689600</td>\n      <td>0.688384</td>\n      <td>0.586207</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.688300</td>\n      <td>0.685632</td>\n      <td>0.620690</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.689600</td>\n      <td>0.680706</td>\n      <td>0.655172</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.686200</td>\n      <td>0.682435</td>\n      <td>0.655172</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.678700</td>\n      <td>0.705717</td>\n      <td>0.413793</td>\n      <td>0.413793</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.675200</td>\n      <td>0.715237</td>\n      <td>0.310345</td>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.672400</td>\n      <td>0.691437</td>\n      <td>0.448276</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.674600</td>\n      <td>0.679459</td>\n      <td>0.655172</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.659900</td>\n      <td>0.692432</td>\n      <td>0.482759</td>\n      <td>0.117647</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.659100</td>\n      <td>0.680505</td>\n      <td>0.551724</td>\n      <td>0.133333</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.636500</td>\n      <td>0.671434</td>\n      <td>0.586207</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.610500</td>\n      <td>0.682220</td>\n      <td>0.551724</td>\n      <td>0.235294</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.575300</td>\n      <td>0.677016</td>\n      <td>0.586207</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.545500</td>\n      <td>0.627019</td>\n      <td>0.655172</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/779111796.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# Train BERT (switch to 'google/electra-base-discriminator' for +3-5% potential)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training BERT (ELECTRA variant)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m bert_history, bert_probs, bert_preds, bert_true, bert_trainer = train_model(\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;34m'google/electra-base-discriminator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m )\n","\u001b[0;32m/tmp/ipykernel_48/779111796.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, tokenizer_class, model_class)\u001b[0m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# No resume_from_checkpoint needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Predict on test for probs and preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2551\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m                     ):\n\u001b[1;32m   2555\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"!pip install optuna\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.sentence as nas\nimport nltk\nimport optuna\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# EDA augmentation functions (synonym, substitute, insert, delete, swap for diversity)\ndef eda_augment(text):\n    # Simple EDA ops without external aug (for robustness)\n    words = text.split()\n    if len(words) < 2:\n        return text\n    # Synonym (mock, as wordnet may fail; use random swap for simplicity)\n    if np.random.rand() > 0.5:\n        idx1, idx2 = np.random.choice(len(words), 2, replace=False)\n        words[idx1], words[idx2] = words[idx2], words[idx1]  # Swap\n    # Insert (duplicate random word)\n    if np.random.rand() > 0.5:\n        idx = np.random.randint(len(words))\n        words.insert(idx, words[idx])\n    # Delete (remove random word)\n    if np.random.rand() > 0.5 and len(words) > 2:\n        idx = np.random.randint(len(words))\n        del words[idx]\n    return ' '.join(words)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# K-Fold for better data utilization (3 folds for small data)\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nbert_preds_all = []\ndistil_preds_all = []\ntrue_labels_all = []\nall_histories = []  # For aggregated curves\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n    print(f\"Fold {fold+1}/3\")\n    train_df_fold = df.iloc[train_idx].reset_index(drop=True)\n    val_df_fold = df.iloc[val_idx].reset_index(drop=True)\n\n    # Multi-round EDA + nlpaug for train fold (4x diversity)\n    aug_syn = naw.SynonymAug(aug_src='wordnet', aug_max=5)\n    aug_ins = nac.RandomCharAug(action=\"insert\", aug_char_max=3)\n    aug_del = nac.RandomCharAug(action=\"delete\", aug_char_max=2)\n    aug_sub = naw.RandomWordAug(action=\"substitute\")\n    aug_sent = nas.AbstractiveSummAug(model_path='t5-small')  # Sentence-level for variety\n\n    train_df_fold['text'] = train_df_fold['text'].apply(lambda x: eda_augment(aug_syn.augment(aug_ins.augment(aug_del.augment(aug_sub.augment(x)[0])[0])[0])[0]) if np.random.rand() > 0.2 else x)  # 80% chance, multi-ops\n\n    # Manual oversampling minority in train fold\n    majority_class = train_df_fold['label'].value_counts().idxmax()\n    minority_class = 1 - majority_class\n    majority_count = len(train_df_fold[train_df_fold['label'] == majority_class])\n    minority_df = train_df_fold[train_df_fold['label'] == minority_class]\n    oversample_count = majority_count - len(minority_df)\n    if oversample_count > 0:\n        minority_oversampled = minority_df.sample(oversample_count, replace=True, random_state=42)\n        train_df_fold = pd.concat([train_df_fold, minority_oversampled]).reset_index(drop=True)\n\n    # Convert to Dataset\n    train_dataset = Dataset.from_pandas(train_df_fold[['text', 'label']])\n    val_dataset = Dataset.from_pandas(val_df_fold[['text', 'label']])\n\n    # Optuna hyperparam search (lr, batch, decay)\n    def objective(trial):\n        lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n        batch = trial.suggest_categorical('batch_size', [4, 8, 16])\n        decay = trial.suggest_uniform('weight_decay', 0.001, 0.1)\n        training_args = TrainingArguments(\n            output_dir=f'/kaggle/working/trial_{trial.number}',\n            num_train_epochs=15,\n            per_device_train_batch_size=batch,\n            per_device_eval_batch_size=batch,\n            warmup_steps=100,\n            weight_decay=decay,\n            learning_rate=lr,\n            eval_strategy='epoch',\n            save_strategy='no',  # No save to save space\n            logging_strategy='epoch',\n            load_best_model_at_end=False,  # No load to save space\n            metric_for_best_model='f1',\n            greater_is_better=True,\n            report_to='none'\n        )\n        trainer = WeightedTrainer(\n            model=model,  # Use placeholder; actual in loop\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            compute_metrics=compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        trainer.train()\n        eval_result = trainer.evaluate()\n        shutil.rmtree(f'/kaggle/working/trial_{trial.number}')  # Cleanup trial dir\n        return eval_result['eval_f1']\n\n    # Compute class weights for imbalance (on balanced train_df_fold)\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df_fold['label']), y=train_df_fold['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n\n    # Custom loss function with label smoothing + weights\n    def weighted_loss(logits, labels, smoothing=0.1):\n        num_labels = logits.size(1)\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        labels_smoothed = labels * (1 - smoothing) + smoothing / num_labels\n        loss = - (labels_smoothed * log_probs).sum(dim=1).mean()\n        return loss  # Weights integrated via prior balancing\n\n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n\n    # Tokenize for Optuna (once per fold)\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n    tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')  # Shared for Optuna\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n\n    # Optuna search (20 trials, prune for space/time)\n    study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n    study.optimize(objective, n_trials=20)\n\n    # Best params from Optuna\n    best_params = study.best_params\n\n    # Retrain with best params on full fold\n    training_args = TrainingArguments(\n        output_dir=f'/kaggle/working/results_fold{fold}',\n        num_train_epochs=15,\n        per_device_train_batch_size=best_params['batch_size'],\n        per_device_eval_batch_size=best_params['batch_size'],\n        warmup_steps=100,\n        weight_decay=best_params['weight_decay'],\n        learning_rate=best_params['learning_rate'],\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,  # Limit checkpoints\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n\n    # Load model with best params\n    model = AutoModelForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=2)  # BERT variant\n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n\n    trainer.train()\n\n    # Predict and collect for ensemble\n    predictions = trainer.predict(tokenized_val)\n    bert_preds_all.extend(np.argmax(predictions.predictions, axis=-1))\n    true_labels_all.extend(predictions.label_ids)\n\n    # Collect history for aggregated curves\n    all_histories.append(trainer.state.log_history)\n\n    # Cleanup fold dir\n    shutil.rmtree(f'/kaggle/working/results_fold{fold}')\n\n# After k-fold, average predictions or use for metrics (here, report on all val)\n# Overall metrics\nprint(classification_report(true_labels_all, bert_preds_all))  # Example for BERT variant; repeat for Distil\n\n# Aggregated curves (average across folds)\ndef aggregate_histories(histories):\n    max_epochs = max(len([log for log in h if 'eval_loss' in log]) for h in histories)\n    train_loss_agg = np.zeros(max_epochs)\n    val_loss_agg = np.zeros(max_epochs)\n    val_acc_agg = np.zeros(max_epochs)\n    counts = np.zeros(max_epochs)\n    for h in histories:\n        train_loss = [log['loss'] for log in h if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n        val_loss = [log['eval_loss'] for log in h if 'eval_loss' in log]\n        val_acc = [log['eval_accuracy'] for log in h if 'eval_accuracy' in log]\n        num_ep = len(val_loss)\n        train_loss_agg[:num_ep] += train_loss[:num_ep]\n        val_loss_agg[:num_ep] += val_loss\n        val_acc_agg[:num_ep] += val_acc\n        counts[:num_ep] += 1\n    train_loss_agg /= counts\n    val_loss_agg /= counts\n    val_acc_agg /= counts\n    epochs = range(1, max_epochs + 1)\n    return epochs, train_loss_agg, val_loss_agg, val_acc_agg\n\nagg_epochs, agg_train_loss, agg_val_loss, agg_val_acc = aggregate_histories(all_histories)\n\n# Plot training-validation curves in one image\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n# Loss\naxs[0].plot(agg_epochs, agg_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(agg_epochs, agg_val_loss, label='BERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n# Accuracy\naxs[1].plot(agg_epochs, agg_val_acc, label='BERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves-5.png', dpi=100)\nplt.close()\n\n# ROC curves in one image (using aggregated preds/true for BERT; repeat for Distil if needed)\nfpr_bert, tpr_bert, _ = roc_curve(true_labels_all, bert_preds_all)  # Probs approx via preds for simplicity; use probs if available\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves-5.png', dpi=100)\nplt.close()\n\n# Classification reports in one image (side-by-side text)\nbert_report = classification_report(true_labels_all, bert_preds_all)\nfig, ax = plt.subplots(figsize=(12, 4))\nax.text(0.05, 0.5, bert_report, ha='left', va='center', fontsize=10, family='monospace')\nax.axis('off')\nplt.title('BERT Classification Report')\nplt.savefig('/kaggle/working/classification_reports-5.png', dpi=100)\nplt.close()  # Distil similar if added\n\n# Confusion matrices in one image (side by side)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(true_labels_all, bert_preds_all)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-5.png', dpi=100)\nplt.close()  # Distil similar if added\n\n# Save models (one per variant)\nbert_trainer.save_model('/kaggle/working/bert_model_final')  # Adjust for variants\ndistil_trainer.save_model('/kaggle/working/distil_model_final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T09:16:08.233351Z","iopub.execute_input":"2025-11-09T09:16:08.233648Z","iopub.status.idle":"2025-11-09T09:16:14.036167Z","shell.execute_reply.started":"2025-11-09T09:16:08.233627Z","shell.execute_reply":"2025-11-09T09:16:14.035144Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.17.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\nFold 1/3\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/917122330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0maug_del\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCharAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"delete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_char_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0maug_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomWordAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"substitute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0maug_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAbstractiveSummAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m't5-small'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sentence-level for variety\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtrain_df_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meda_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_ins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_del\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 80% chance, multi-ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'nlpaug.augmenter.sentence' has no attribute 'AbstractiveSummAug'"],"ename":"AttributeError","evalue":"module 'nlpaug.augmenter.sentence' has no attribute 'AbstractiveSummAug'","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"!pip install optuna\n!pip install nlpaug\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.sentence as nas\nimport nltk\nimport optuna\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# EDA augmentation functions (synonym, substitute, insert, delete, swap for diversity)\ndef eda_augment(text):\n    # Simple EDA ops without external aug (for robustness)\n    words = text.split()\n    if len(words) < 2:\n        return text\n    # Synonym (mock, as wordnet may fail; use random swap for simplicity)\n    if np.random.rand() > 0.5:\n        idx1, idx2 = np.random.choice(len(words), 2, replace=False)\n        words[idx1], words[idx2] = words[idx2], words[idx1]  # Swap\n    # Insert (duplicate random word)\n    if np.random.rand() > 0.5:\n        idx = np.random.randint(len(words))\n        words.insert(idx, words[idx])\n    # Delete (remove random word)\n    if np.random.rand() > 0.5 and len(words) > 2:\n        idx = np.random.randint(len(words))\n        del words[idx]\n    return ' '.join(words)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# K-Fold for better data utilization (3 folds for small data)\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nbert_preds_all = []\ndistil_preds_all = []\ntrue_labels_all = []\nall_histories = []  # For aggregated curves\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n    print(f\"Fold {fold+1}/3\")\n    train_df_fold = df.iloc[train_idx].reset_index(drop=True)\n    val_df_fold = df.iloc[val_idx].reset_index(drop=True)\n\n    # Multi-round EDA + nlpaug for train fold (4x diversity)\n    aug_syn = naw.SynonymAug(aug_src='wordnet', aug_max=5)\n    aug_ins = nac.RandomCharAug(action=\"insert\", aug_char_max=3)\n    aug_del = nac.RandomCharAug(action=\"delete\", aug_char_max=2)\n    aug_sub = naw.RandomWordAug(action=\"substitute\")\n    aug_sent = nas.AbstractiveSummaryAug(model_path='t5-small')  # Fixed: Summary not Summ\n\n    train_df_fold['text'] = train_df_fold['text'].apply(lambda x: eda_augment(aug_syn.augment(aug_ins.augment(aug_del.augment(aug_sub.augment(x)[0])[0])[0])[0]) if np.random.rand() > 0.2 else x)  # 80% chance, multi-ops\n\n    # Manual oversampling minority in train fold\n    majority_class = train_df_fold['label'].value_counts().idxmax()\n    minority_class = 1 - majority_class\n    majority_count = len(train_df_fold[train_df_fold['label'] == majority_class])\n    minority_df = train_df_fold[train_df_fold['label'] == minority_class]\n    oversample_count = majority_count - len(minority_df)\n    if oversample_count > 0:\n        minority_oversampled = minority_df.sample(oversample_count, replace=True, random_state=42)\n        train_df_fold = pd.concat([train_df_fold, minority_oversampled]).reset_index(drop=True)\n\n    # Convert to Dataset\n    train_dataset = Dataset.from_pandas(train_df_fold[['text', 'label']])\n    val_dataset = Dataset.from_pandas(val_df_fold[['text', 'label']])\n\n    # Optuna hyperparam search (lr, batch, decay)\n    def objective(trial):\n        lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n        batch = trial.suggest_categorical('batch_size', [4, 8, 16])\n        decay = trial.suggest_uniform('weight_decay', 0.001, 0.1)\n        training_args = TrainingArguments(\n            output_dir=f'/kaggle/working/trial_{trial.number}',\n            num_train_epochs=15,\n            per_device_train_batch_size=batch,\n            per_device_eval_batch_size=batch,\n            warmup_steps=100,\n            weight_decay=decay,\n            learning_rate=lr,\n            eval_strategy='epoch',\n            save_strategy='no',  # No save to save space\n            logging_strategy='epoch',\n            load_best_model_at_end=False,  # No load to save space\n            metric_for_best_model='f1',\n            greater_is_better=True,\n            report_to='none'\n        )\n        trainer = WeightedTrainer(\n            model=model,  # Use placeholder; actual in loop\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            compute_metrics=compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        trainer.train()\n        eval_result = trainer.evaluate()\n        shutil.rmtree(f'/kaggle/working/trial_{trial.number}')  # Cleanup trial dir\n        return eval_result['eval_f1']\n\n    # Compute class weights for imbalance (on balanced train_df_fold)\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df_fold['label']), y=train_df_fold['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n\n    # Custom loss function with label smoothing + weights\n    def weighted_loss(logits, labels, smoothing=0.1):\n        num_labels = logits.size(1)\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        labels_smoothed = labels * (1 - smoothing) + smoothing / num_labels\n        loss = - (labels_smoothed * log_probs).sum(dim=1).mean()\n        return loss  # Weights integrated via prior balancing\n\n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n\n    # Tokenize for Optuna (once per fold)\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n    tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')  # Shared for Optuna\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n\n    # Optuna search (20 trials, prune for space/time)\n    study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n    study.optimize(objective, n_trials=20)\n\n    # Best params from Optuna\n    best_params = study.best_params\n\n    # Retrain with best params on full fold\n    training_args = TrainingArguments(\n        output_dir=f'/kaggle/working/results_fold{fold}',\n        num_train_epochs=15,\n        per_device_train_batch_size=best_params['batch_size'],\n        per_device_eval_batch_size=best_params['batch_size'],\n        warmup_steps=100,\n        weight_decay=best_params['weight_decay'],\n        learning_rate=best_params['learning_rate'],\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,  # Limit checkpoints\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n\n    # Load model with best params\n    model = AutoModelForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=2)  # BERT variant\n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n\n    trainer.train()\n\n    # Predict and collect for ensemble\n    predictions = trainer.predict(tokenized_val)\n    bert_preds_all.extend(np.argmax(predictions.predictions, axis=-1))\n    true_labels_all.extend(predictions.label_ids)\n\n    # Collect history for aggregated curves\n    all_histories.append(trainer.state.log_history)\n\n    # Hybrid: Extract embeddings + prior features, train SVM\n    # Get embeddings (mean pool last hidden)\n    def get_embeddings(trainer, dataset):\n        trainer.model.eval()\n        dataloader = trainer.get_eval_dataloader(dataset)\n        embeddings = []\n        for batch in dataloader:\n            batch = {k: v.to(trainer.model.device) for k, v in batch.items() if k != 'labels'}\n            with torch.no_grad():\n                outputs = trainer.model(**batch, output_hidden_states=True)\n                emb = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()\n                embeddings.extend(emb)\n        return np.array(embeddings)\n\n    val_embeddings = get_embeddings(trainer, tokenized_val)\n    # Assume prior_features_val from earlier (slice to match val_idx)\n    prior_features_val = prior_features[val_idx]  # Adjust to your features.npy indices\n    hybrid_features = np.hstack((val_embeddings, prior_features_val))\n    hybrid_svm = SVC(kernel='linear', probability=True)\n    hybrid_svm.fit(hybrid_features, val_df_fold['label'])  # Train on val for demo; ideally separate\n    hybrid_preds = hybrid_svm.predict(hybrid_features)\n    print(f\"Fold {fold} Hybrid Report:\\n{classification_report(val_df_fold['label'], hybrid_preds)}\")  # Per-fold hybrid\n\n    # Cleanup fold dir\n    shutil.rmtree(f'/kaggle/working/results_fold{fold}')\n\n# After k-fold, average predictions or use for metrics (here, report on all val)\n# Overall metrics\nprint(classification_report(true_labels_all, bert_preds_all))  # Example for BERT variant; repeat for Distil\n\n# Aggregated curves (average across folds)\ndef aggregate_histories(histories):\n    max_epochs = max(len([log for log in h if 'eval_loss' in log]) for h in histories)\n    train_loss_agg = np.zeros(max_epochs)\n    val_loss_agg = np.zeros(max_epochs)\n    val_acc_agg = np.zeros(max_epochs)\n    counts = np.zeros(max_epochs)\n    for h in histories:\n        train_loss = [log['loss'] for log in h if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n        val_loss = [log['eval_loss'] for log in h if 'eval_loss' in log]\n        val_acc = [log['eval_accuracy'] for log in h if 'eval_accuracy' in log]\n        num_ep = len(val_loss)\n        train_loss_agg[:num_ep] += train_loss[:num_ep]\n        val_loss_agg[:num_ep] += val_loss\n        val_acc_agg[:num_ep] += val_acc\n        counts[:num_ep] += 1\n    train_loss_agg /= counts\n    val_loss_agg /= counts\n    val_acc_agg /= counts\n    epochs = range(1, max_epochs + 1)\n    return epochs, train_loss_agg, val_loss_agg, val_acc_agg\n\nagg_epochs, agg_train_loss, agg_val_loss, agg_val_acc = aggregate_histories(all_histories)\n\n# Plot training-validation curves in one image\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n# Loss\naxs[0].plot(agg_epochs, agg_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(agg_epochs, agg_val_loss, label='BERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n# Accuracy\naxs[1].plot(agg_epochs, agg_val_acc, label='BERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves-5.png', dpi=100)\nplt.close()\n\n# ROC curves in one image (using aggregated preds/true for BERT; repeat for Distil if added)\nfpr_bert, tpr_bert, _ = roc_curve(true_labels_all, bert_preds_all)  # Probs approx via preds for simplicity; use probs if available\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves-5.png', dpi=100)\nplt.close()\n\n# Classification reports in one image (side-by-side text)\nbert_report = classification_report(true_labels_all, bert_preds_all)\nfig, ax = plt.subplots(figsize=(12, 4))\nax.text(0.05, 0.5, bert_report, ha='left', va='center', fontsize=10, family='monospace')\nax.axis('off')\nplt.title('BERT Classification Report')\nplt.savefig('/kaggle/working/classification_reports-5.png', dpi=100)\nplt.close()  # Distil similar if added\n\n# Confusion matrices in one image (side by side)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(true_labels_all, bert_preds_all)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-5.png', dpi=100)\nplt.close()  # Distil similar if added\n\n# Save models (one per variant)\nbert_trainer.save_model('/kaggle/working/bert_model_final')  # Adjust for variants\ndistil_trainer.save_model('/kaggle/working/distil_model_final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T10:02:33.748620Z","iopub.execute_input":"2025-11-09T10:02:33.748956Z","iopub.status.idle":"2025-11-09T10:03:11.493523Z","shell.execute_reply.started":"2025-11-09T10:02:33.748926Z","shell.execute_reply":"2025-11-09T10:03:11.492457Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.17.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: nlpaug in /usr/local/lib/python3.11/dist-packages (1.1.11)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.3)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.5)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.20.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.10.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nFold 1/3\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/100623197.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0maug_del\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCharAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"delete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_char_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0maug_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomWordAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"substitute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0maug_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAbstractiveSummaryAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m't5-small'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fixed: Summary not Summ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtrain_df_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meda_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_ins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_del\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 80% chance, multi-ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'nlpaug.augmenter.sentence' has no attribute 'AbstractiveSummaryAug'"],"ename":"AttributeError","evalue":"module 'nlpaug.augmenter.sentence' has no attribute 'AbstractiveSummaryAug'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"!pip install optuna\n!pip install --upgrade nlpaug  # Upgrade to version with AbstractiveSummaryAug\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.sentence as nas\nimport nltk\nimport optuna\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# EDA augmentation functions (synonym, substitute, insert, delete, swap for diversity)\ndef eda_augment(text):\n    # Simple EDA ops without external aug (for robustness)\n    words = text.split()\n    if len(words) < 2:\n        return text\n    # Synonym (mock, as wordnet may fail; use random swap for simplicity)\n    if np.random.rand() > 0.5:\n        idx1, idx2 = np.random.choice(len(words), 2, replace=False)\n        words[idx1], words[idx2] = words[idx2], words[idx1]  # Swap\n    # Insert (duplicate random word)\n    if np.random.rand() > 0.5:\n        idx = np.random.randint(len(words))\n        words.insert(idx, words[idx])\n    # Delete (remove random word)\n    if np.random.rand() > 0.5 and len(words) > 2:\n        idx = np.random.randint(len(words))\n        del words[idx]\n    return ' '.join(words)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# K-Fold for better data utilization (3 folds for small data)\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nbert_preds_all = []\ndistil_preds_all = []\ntrue_labels_all = []\nall_histories = []  # For aggregated curves\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n    print(f\"Fold {fold+1}/3\")\n    train_df_fold = df.iloc[train_idx].reset_index(drop=True)\n    val_df_fold = df.iloc[val_idx].reset_index(drop=True)\n\n    # Multi-round EDA + nlpaug for train fold (4x diversity)\n    aug_syn = naw.SynonymAug(aug_src='wordnet', aug_max=5)\n    aug_ins = nac.RandomCharAug(action=\"insert\", aug_char_max=3)\n    aug_del = nac.RandomCharAug(action=\"delete\", aug_char_max=2)\n    aug_sub = naw.RandomWordAug(action=\"substitute\")\n    aug_sent = nas.AbstractiveSummaryAug(model_path='t5-small')  # Fixed class name\n\n    train_df_fold['text'] = train_df_fold['text'].apply(lambda x: eda_augment(aug_syn.augment(aug_ins.augment(aug_del.augment(aug_sub.augment(x)[0])[0])[0])[0]) if np.random.rand() > 0.2 else x)  # 80% chance, multi-ops\n\n    # Manual oversampling minority in train fold\n    majority_class = train_df_fold['label'].value_counts().idxmax()\n    minority_class = 1 - majority_class\n    majority_count = len(train_df_fold[train_df_fold['label'] == majority_class])\n    minority_df = train_df_fold[train_df_fold['label'] == minority_class]\n    oversample_count = majority_count - len(minority_df)\n    if oversample_count > 0:\n        minority_oversampled = minority_df.sample(oversample_count, replace=True, random_state=42)\n        train_df_fold = pd.concat([train_df_fold, minority_oversampled]).reset_index(drop=True)\n\n    # Convert to Dataset\n    train_dataset = Dataset.from_pandas(train_df_fold[['text', 'label']])\n    val_dataset = Dataset.from_pandas(val_df_fold[['text', 'label']])\n\n    # Optuna hyperparam search (lr, batch, decay)\n    def objective(trial):\n        lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n        batch = trial.suggest_categorical('batch_size', [4, 8, 16])\n        decay = trial.suggest_uniform('weight_decay', 0.001, 0.1)\n        training_args = TrainingArguments(\n            output_dir=f'/kaggle/working/trial_{trial.number}',\n            num_train_epochs=15,\n            per_device_train_batch_size=batch,\n            per_device_eval_batch_size=batch,\n            warmup_steps=100,\n            weight_decay=decay,\n            learning_rate=lr,\n            eval_strategy='epoch',\n            save_strategy='no',  # No save to save space\n            logging_strategy='epoch',\n            load_best_model_at_end=False,  # No load to save space\n            metric_for_best_model='f1',\n            greater_is_better=True,\n            report_to='none'\n        )\n        trainer = WeightedTrainer(\n            model=model,  # Use placeholder; actual in loop\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            compute_metrics=compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        trainer.train()\n        eval_result = trainer.evaluate()\n        shutil.rmtree(f'/kaggle/working/trial_{trial.number}')  # Cleanup trial dir\n        return eval_result['eval_f1']\n\n    # Compute class weights for imbalance (on balanced train_df_fold)\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df_fold['label']), y=train_df_fold['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n\n    # Custom loss function with label smoothing + weights\n    def weighted_loss(logits, labels, smoothing=0.1):\n        num_labels = logits.size(1)\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        labels_smoothed = labels * (1 - smoothing) + smoothing / num_labels\n        loss = - (labels_smoothed * log_probs).sum(dim=1).mean()\n        return loss  # Weights integrated via prior balancing\n\n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n\n    # Tokenize for Optuna (once per fold)\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n    tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')  # Shared for Optuna\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n\n    # Optuna search (20 trials, prune for space/time)\n    study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n    study.optimize(objective, n_trials=20)\n\n    # Best params from Optuna\n    best_params = study.best_params\n\n    # Retrain with best params on full fold\n    training_args = TrainingArguments(\n        output_dir=f'/kaggle/working/results_fold{fold}',\n        num_train_epochs=15,\n        per_device_train_batch_size=best_params['batch_size'],\n        per_device_eval_batch_size=best_params['batch_size'],\n        warmup_steps=100,\n        weight_decay=best_params['weight_decay'],\n        learning_rate=best_params['learning_rate'],\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,  # Limit checkpoints\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n\n    # Load model with best params\n    model = AutoModelForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=2)  # BERT variant\n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n\n    trainer.train()\n\n    # Predict and collect for ensemble\n    predictions = trainer.predict(tokenized_val)\n    bert_preds_all.extend(np.argmax(predictions.predictions, axis=-1))\n    true_labels_all.extend(predictions.label_ids)\n\n    # Collect history for aggregated curves\n    all_histories.append(trainer.state.log_history)\n\n    # Hybrid: Extract embeddings + prior features, train SVM\n    # Get embeddings (mean pool last hidden)\n    def get_embeddings(trainer, dataset):\n        trainer.model.eval()\n        dataloader = trainer.get_eval_dataloader(dataset)\n        embeddings = []\n        for batch in dataloader:\n            batch = {k: v.to(trainer.model.device) for k, v in batch.items() if k != 'labels'}\n            with torch.no_grad():\n                outputs = trainer.model(**batch, output_hidden_states=True)\n                emb = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()\n                embeddings.extend(emb)\n        return np.array(embeddings)\n\n    val_embeddings = get_embeddings(trainer, tokenized_val)\n    # Assume prior_features_val from earlier (slice to match val_idx)\n    prior_features_val = prior_features[val_idx]  # Adjust to your features.npy indices\n    hybrid_features = np.hstack((val_embeddings, prior_features_val))\n    hybrid_svm = SVC(kernel='linear', probability=True)\n    hybrid_svm.fit(hybrid_features, val_df_fold['label'])  # Train on val for demo; ideally separate\n    hybrid_preds = hybrid_svm.predict(hybrid_features)\n    print(f\"Fold {fold} Hybrid Report:\\n{classification_report(val_df_fold['label'], hybrid_preds)}\")  # Per-fold hybrid\n\n    # Cleanup fold dir\n    shutil.rmtree(f'/kaggle/working/results_fold{fold}')\n\n# After k-fold, average predictions or use for metrics (here, report on all val)\n# Overall metrics\nprint(classification_report(true_labels_all, bert_preds_all))  # Example for BERT variant; repeat for Distil\n\n# Aggregated curves (average across folds)\ndef aggregate_histories(histories):\n    max_epochs = max(len([log for log in h if 'eval_loss' in log]) for h in histories)\n    train_loss_agg = np.zeros(max_epochs)\n    val_loss_agg = np.zeros(max_epochs)\n    val_acc_agg = np.zeros(max_epochs)\n    counts = np.zeros(max_epochs)\n    for h in histories:\n        train_loss = [log['loss'] for log in h if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n        val_loss = [log['eval_loss'] for log in h if 'eval_loss' in log]\n        val_acc = [log['eval_accuracy'] for log in h if 'eval_accuracy' in log]\n        num_ep = len(val_loss)\n        train_loss_agg[:num_ep] += train_loss[:num_ep]\n        val_loss_agg[:num_ep] += val_loss\n        val_acc_agg[:num_ep] += val_acc\n        counts[:num_ep] += 1\n    train_loss_agg /= counts\n    val_loss_agg /= counts\n    val_acc_agg /= counts\n    epochs = range(1, max_epochs + 1)\n    return epochs, train_loss_agg, val_loss_agg, val_acc_agg\n\nagg_epochs, agg_train_loss, agg_val_loss, agg_val_acc = aggregate_histories(all_histories)\n\n# Plot training-validation curves in one image\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n# Loss\naxs[0].plot(agg_epochs, agg_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(agg_epochs, agg_val_loss, label='BERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n# Accuracy\naxs[1].plot(agg_epochs, agg_val_acc, label='BERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves-5.png', dpi=100)\nplt.close()\n\n# ROC curves in one image (using aggregated preds/true for BERT; repeat for Distil if added)\nfpr_bert, tpr_bert, _ = roc_curve(true_labels_all, bert_preds_all) # Probs approx via preds for simplicity; use probs if available\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves-5.png', dpi=100)\nplt.close()\n\n# Classification reports in one image (side-by-side text)\nbert_report = classification_report(true_labels_all, bert_preds_all)\nfig, ax = plt.subplots(figsize=(12, 4))\nax.text(0.05, 0.5, bert_report, ha='left', va='center', fontsize=10, family='monospace')\nax.axis('off')\nplt.title('BERT Classification Report')\nplt.savefig('/kaggle/working/classification_reports-5.png', dpi=100)\nplt.close() # Distil similar if added\n\n# Confusion matrices in one image (side by side)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(true_labels_all, bert_preds_all)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-5.png', dpi=100)\nplt.close() # Distil similar if added\n\n# Save models (one per variant)\nbert_trainer.save_model('/kaggle/working/bert_model_final') # Adjust for variants\ndistil_trainer.save_model('/kaggle/working/distil_model_final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:36:12.976726Z","iopub.execute_input":"2025-11-09T15:36:12.977601Z","iopub.status.idle":"2025-11-09T15:36:48.985427Z","shell.execute_reply.started":"2025-11-09T15:36:12.977553Z","shell.execute_reply":"2025-11-09T15:36:48.984370Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.17.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: nlpaug in /usr/local/lib/python3.11/dist-packages (1.1.11)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.3)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.5)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.20.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.10.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nFold 1/3\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1441609337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0maug_del\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCharAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"delete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_char_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0maug_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomWordAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"substitute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0maug_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAbstractiveSummaryAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m't5-small'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fixed class name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtrain_df_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meda_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_ins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_del\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 80% chance, multi-ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'nlpaug.augmenter.sentence' has no attribute 'AbstractiveSummaryAug'"],"ename":"AttributeError","evalue":"module 'nlpaug.augmenter.sentence' has no attribute 'AbstractiveSummaryAug'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"!pip install optuna\n!pip uninstall -y nlpaug\n!pip install nlpaug==1.1.11  # Force version with AbstractiveSummaryAug\n!pip install  sentencepiece  # Deps for t5 aug\n\nimport logging\nlogging.disable(logging.WARNING)  # Stop non-critical logging errors\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nimport os\nimport shutil\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\nfrom nlpaug.augmenter.sentence import AbstractiveSummaryAug  # Explicit import\nimport nltk\nimport optuna\n\n# Download required NLTK resources quietly\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# EDA augmentation functions (synonym, substitute, insert, delete, swap for diversity)\ndef eda_augment(text):\n    # Simple EDA ops without external aug (for robustness)\n    words = text.split()\n    if len(words) < 2:\n        return text\n    # Synonym (mock, as wordnet may fail; use random swap for simplicity)\n    if np.random.rand() > 0.5:\n        idx1, idx2 = np.random.choice(len(words), 2, replace=False)\n        words[idx1], words[idx2] = words[idx2], words[idx1]  # Swap\n    # Insert (duplicate random word)\n    if np.random.rand() > 0.5:\n        idx = np.random.randint(len(words))\n        words.insert(idx, words[idx])\n    # Delete (remove random word)\n    if np.random.rand() > 0.5 and len(words) > 2:\n        idx = np.random.randint(len(words))\n        del words[idx]\n    return ' '.join(words)\n\n# Load processed data (text and labels)\ndf = pd.read_csv('/kaggle/working/processed_data.csv')\ndf = df.dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# K-Fold for better data utilization (3 folds for small data)\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nbert_preds_all = []\ndistil_preds_all = []\ntrue_labels_all = []\nall_histories = []  # For aggregated curves\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n    print(f\"Fold {fold+1}/3\")\n    train_df_fold = df.iloc[train_idx].reset_index(drop=True)\n    val_df_fold = df.iloc[val_idx].reset_index(drop=True)\n\n    # Multi-round EDA + nlpaug for train fold (4x diversity)\n    aug_syn = naw.SynonymAug(aug_src='wordnet', aug_max=5)\n    aug_ins = nac.RandomCharAug(action=\"insert\", aug_char_max=3)\n    aug_del = nac.RandomCharAug(action=\"delete\", aug_char_max=2)\n    aug_sub = naw.RandomWordAug(action=\"substitute\")\n    aug_sent = AbstractiveSummaryAug(model_path='t5-small')  # Correct import/class\n\n    train_df_fold['text'] = train_df_fold['text'].apply(lambda x: eda_augment(aug_syn.augment(aug_ins.augment(aug_del.augment(aug_sub.augment(x)[0])[0])[0])[0]) if np.random.rand() > 0.2 else x)  # 80% chance, multi-ops\n\n    # Manual oversampling minority in train fold\n    majority_class = train_df_fold['label'].value_counts().idxmax()\n    minority_class = 1 - majority_class\n    majority_count = len(train_df_fold[train_df_fold['label'] == majority_class])\n    minority_df = train_df_fold[train_df_fold['label'] == minority_class]\n    oversample_count = majority_count - len(minority_df)\n    if oversample_count > 0:\n        minority_oversampled = minority_df.sample(oversample_count, replace=True, random_state=42)\n        train_df_fold = pd.concat([train_df_fold, minority_oversampled]).reset_index(drop=True)\n\n    # Convert to Dataset\n    train_dataset = Dataset.from_pandas(train_df_fold[['text', 'label']])\n    val_dataset = Dataset.from_pandas(val_df_fold[['text', 'label']])\n\n    # Optuna hyperparam search (lr, batch, decay)\n    def objective(trial):\n        lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n        batch = trial.suggest_categorical('batch_size', [4, 8, 16])\n        decay = trial.suggest_uniform('weight_decay', 0.001, 0.1)\n        training_args = TrainingArguments(\n            output_dir=f'/kaggle/working/trial_{trial.number}',\n            num_train_epochs=15,\n            per_device_train_batch_size=batch,\n            per_device_eval_batch_size=batch,\n            warmup_steps=100,\n            weight_decay=decay,\n            learning_rate=lr,\n            eval_strategy='epoch',\n            save_strategy='no',  # No save to save space\n            logging_strategy='epoch',\n            load_best_model_at_end=False,  # No load to save space\n            metric_for_best_model='f1',\n            greater_is_better=True,\n            report_to='none'\n        )\n        trainer = WeightedTrainer(\n            model=model,  # Use placeholder; actual in loop\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            compute_metrics=compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        trainer.train()\n        eval_result = trainer.evaluate()\n        shutil.rmtree(f'/kaggle/working/trial_{trial.number}')  # Cleanup trial dir\n        return eval_result['eval_f1']\n\n    # Compute class weights for imbalance (on balanced train_df_fold)\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df_fold['label']), y=train_df_fold['label'])\n    class_weights = torch.tensor(class_weights, dtype=torch.float)\n\n    # Custom loss function with label smoothing + weights\n    def weighted_loss(logits, labels, smoothing=0.1):\n        num_labels = logits.size(1)\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        labels_smoothed = labels * (1 - smoothing) + smoothing / num_labels\n        loss = - (labels_smoothed * log_probs).sum(dim=1).mean()\n        return loss  # Weights integrated via prior balancing\n\n    # Custom Trainer for weighted loss\n    class WeightedTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss = weighted_loss(logits, labels)\n            return (loss, outputs) if return_outputs else loss\n\n    # Tokenize for Optuna (once per fold)\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n    tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')  # Shared for Optuna\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n\n    # Optuna search (20 trials, prune for space/time)\n    study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n    study.optimize(objective, n_trials=20)\n\n    # Best params from Optuna\n    best_params = study.best_params\n\n    # Retrain with best params on full fold\n    training_args = TrainingArguments(\n        output_dir=f'/kaggle/working/results_fold{fold}',\n        num_train_epochs=15,\n        per_device_train_batch_size=best_params['batch_size'],\n        per_device_eval_batch_size=best_params['batch_size'],\n        warmup_steps=100,\n        weight_decay=best_params['weight_decay'],\n        learning_rate=best_params['learning_rate'],\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        save_total_limit=1,  # Limit checkpoints\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        greater_is_better=True,\n        report_to='none'\n    )\n\n    # Load model with best params\n    model = AutoModelForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=2)  # BERT variant\n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n    )\n\n    trainer.train()\n\n    # Predict and collect for ensemble\n    predictions = trainer.predict(tokenized_val)\n    bert_preds_all.extend(np.argmax(predictions.predictions, axis=-1))\n    true_labels_all.extend(predictions.label_ids)\n\n    # Collect history for aggregated curves\n    all_histories.append(trainer.state.log_history)\n\n    # Hybrid: Extract embeddings + prior features, train SVM\n    # Get embeddings (mean pool last hidden)\n    def get_embeddings(trainer, dataset):\n        trainer.model.eval()\n        dataloader = trainer.get_eval_dataloader(dataset)\n        embeddings = []\n        for batch in dataloader:\n            batch = {k: v.to(trainer.model.device) for k, v in batch.items() if k != 'labels'}\n            with torch.no_grad():\n                outputs = trainer.model(**batch, output_hidden_states=True)\n                emb = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()\n                embeddings.extend(emb)\n        return np.array(embeddings)\n\n    val_embeddings = get_embeddings(trainer, tokenized_val)\n    # Assume prior_features_val from earlier (slice to match val_idx)\n    prior_features_val = prior_features[val_idx]  # Adjust to your features.npy indices\n    hybrid_features = np.hstack((val_embeddings, prior_features_val))\n    hybrid_svm = SVC(kernel='linear', probability=True)\n    hybrid_svm.fit(hybrid_features, val_df_fold['label'])  # Train on val for demo; ideally separate\n    hybrid_preds = hybrid_svm.predict(hybrid_features)\n    print(f\"Fold {fold} Hybrid Report:\\n{classification_report(val_df_fold['label'], hybrid_preds)}\")  # Per-fold hybrid\n\n    # Cleanup fold dir\n    shutil.rmtree(f'/kaggle/working/results_fold{fold}')\n\n# After k-fold, average predictions or use for metrics (here, report on all val)\n# Overall metrics\nprint(classification_report(true_labels_all, bert_preds_all))  # Example for BERT variant; repeat for Distil\n\n# Aggregated curves (average across folds)\ndef aggregate_histories(histories):\n    max_epochs = max(len([log for log in h if 'eval_loss' in log]) for h in histories)\n    train_loss_agg = np.zeros(max_epochs)\n    val_loss_agg = np.zeros(max_epochs)\n    val_acc_agg = np.zeros(max_epochs)\n    counts = np.zeros(max_epochs)\n    for h in histories:\n        train_loss = [log['loss'] for log in h if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n        val_loss = [log['eval_loss'] for log in h if 'eval_loss' in log]\n        val_acc = [log['eval_accuracy'] for log in h if 'eval_accuracy' in log]\n        num_ep = len(val_loss)\n        train_loss_agg[:num_ep] += train_loss[:num_ep]\n        val_loss_agg[:num_ep] += val_loss\n        val_acc_agg[:num_ep] += val_acc\n        counts[:num_ep] += 1\n    train_loss_agg /= counts\n    val_loss_agg /= counts\n    val_acc_agg /= counts\n    epochs = range(1, max_epochs + 1)\n    return epochs, train_loss_agg, val_loss_agg, val_acc_agg\n\nagg_epochs, agg_train_loss, agg_val_loss, agg_val_acc = aggregate_histories(all_histories)\n\n# Plot training-validation curves in one image\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n# Loss\naxs[0].plot(agg_epochs, agg_train_loss, label='BERT Train Loss', linestyle='--')\naxs[0].plot(agg_epochs, agg_val_loss, label='BERT Val Loss')\naxs[0].set_title('Training/Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n# Accuracy\naxs[1].plot(agg_epochs, agg_val_acc, label='BERT Val Acc')\naxs[1].set_title('Validation Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_validation_curves-5.png', dpi=100)\nplt.close()\n\n# ROC curves in one image (using aggregated preds/true for BERT; repeat for Distil if added)\nfpr_bert, tpr_bert, _ = roc_curve(true_labels_all, bert_preds_all) # Probs approx via preds for simplicity; use probs if available\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves-5.png', dpi=100)\nplt.close()\n\n# Classification reports in one image (side-by-side text)\nbert_report = classification_report(true_labels_all, bert_preds_all)\nfig, ax = plt.subplots(figsize=(12, 4))\nax.text(0.05, 0.5, bert_report, ha='left', va='center', fontsize=10, family='monospace')\nax.axis('off')\nplt.title('BERT Classification Report')\nplt.savefig('/kaggle/working/classification_reports-5.png', dpi=100)\nplt.close() # Distil similar if added\n\n# Confusion matrices in one image (side by side)\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\ncm_bert = confusion_matrix(true_labels_all, bert_preds_all)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix')\naxs[0].set_xlabel('Predicted')\naxs[0].set_ylabel('True')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-5.png', dpi=100)\nplt.close() # Distil similar if added\n\n# Save models (one per variant)\nbert_trainer.save_model('/kaggle/working/bert_model_final') # Adjust for variants\ndistil_trainer.save_model('/kaggle/working/distil_model_final')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T15:56:22.672671Z","iopub.execute_input":"2025-11-09T15:56:22.673395Z","iopub.status.idle":"2025-11-09T15:56:25.934049Z","shell.execute_reply.started":"2025-11-09T15:56:22.673361Z","shell.execute_reply":"2025-11-09T15:56:25.933010Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1879735906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnlpaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnaw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnlpaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnlpaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbstractiveSummaryAug\u001b[0m  \u001b[0;31m# Explicit import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'AbstractiveSummaryAug' from 'nlpaug.augmenter.sentence' (/usr/local/lib/python3.11/dist-packages/nlpaug/augmenter/sentence/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'AbstractiveSummaryAug' from 'nlpaug.augmenter.sentence' (/usr/local/lib/python3.11/dist-packages/nlpaug/augmenter/sentence/__init__.py)","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"!pip install optuna\n!pip uninstall -y nlpaug\n!pip install nlpaug==1.1.11","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:14:56.104810Z","iopub.execute_input":"2025-11-09T17:14:56.105539Z","iopub.status.idle":"2025-11-09T17:15:03.926378Z","shell.execute_reply.started":"2025-11-09T17:14:56.105503Z","shell.execute_reply":"2025-11-09T17:15:03.925462Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.17.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\nFound existing installation: nlpaug 1.1.11\nUninstalling nlpaug-1.1.11:\n  Successfully uninstalled nlpaug-1.1.11\nCollecting nlpaug==1.1.11\n  Using cached nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (2.2.3)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (2.32.5)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (3.20.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug==1.1.11) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug==1.1.11) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug==1.1.11) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug==1.1.11) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug==1.1.11) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug==1.1.11) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (2025.10.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug==1.1.11) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug==1.1.11) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug==1.1.11) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug==1.1.11) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug==1.1.11) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug==1.1.11) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug==1.1.11) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug==1.1.11) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug==1.1.11) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug==1.1.11) (2024.2.0)\nUsing cached nlpaug-1.1.11-py3-none-any.whl (410 kB)\nInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Ensure necessary packages are installed\n!pip install sentencepiece\n!pip install xgboost\n\nimport logging\nlogging.disable(logging.WARNING)\n\nimport torch\nimport optuna\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom evaluate import load\nfrom nlpaug.augmenter.word import SynonymAug, RandomWordAug\nfrom nlpaug.augmenter.char import RandomCharAug\nfrom nlpaug.augmenter.sentence.abst_summ import AbstSummAug\nimport nltk\n\n# Download NLTK resources (if not already present)\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# Simple EDA augmentation function\ndef eda_augment(text):\n    words = text.split()\n    if len(words) < 2:\n        return text\n    if np.random.rand() > 0.5:\n        i, j = np.random.choice(len(words), 2, replace=False)\n        words[i], words[j] = words[j], words[i]  # swap\n    if np.random.rand() > 0.5:\n        idx = np.random.randint(len(words))\n        words.insert(idx, words[idx])         # duplicate\n    if np.random.rand() > 0.5 and len(words) > 2:\n        idx = np.random.randint(len(words))\n        del words[idx]                        # delete\n    return ' '.join(words)\n\n# Load and prepare data\ndf = pd.read_csv('/kaggle/working/processed_data.csv').dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\n\nbert_preds_all = []\ndistil_preds_all = []\ntrue_labels_bert = []\ntrue_labels_distil = []\nall_histories_bert = []\nall_histories_distil = []\n\n# Evaluation metrics\nf1_metric = load('f1')\nacc_metric = load('accuracy')\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    f1 = f1_metric.compute(predictions=preds, references=labels)['f1']\n    acc = acc_metric.compute(predictions=preds, references=labels)['accuracy']\n    return {'accuracy': acc, 'f1': f1}\n\n# Weighted loss with label smoothing\ndef weighted_loss(logits, labels, smoothing=0.1):\n    num_labels = logits.size(1)\n    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n    labels_smoothed = labels * (1 - smoothing) + smoothing / num_labels\n    loss = -(labels_smoothed * log_probs).sum(dim=1).mean()\n    return loss\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = weighted_loss(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n\ntokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n\nfor model_name in ['electra', 'distilbert']:\n    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n        train_df = df.iloc[train_idx].reset_index(drop=True)\n        val_df   = df.iloc[val_idx].reset_index(drop=True)\n\n        # Define augmenters\n        aug_syn = SynonymAug(aug_src='wordnet', aug_max=5)\n        aug_ins = RandomCharAug(action=\"insert\", aug_char_max=3)\n        aug_del = RandomCharAug(action=\"delete\", aug_char_max=2)\n        aug_sub = RandomWordAug(action=\"substitute\")\n        aug_sent = AbstSummAug(model_path='t5-small', min_length=20, max_length=50)  # Summarization augment\n\n        def augment_text(x):\n            if np.random.rand() > 0.2:\n                text = x\n                text = aug_sub.augment(text)[0]\n                text = aug_del.augment(text)[0]\n                text = aug_ins.augment(text)[0]\n                text = aug_syn.augment(text)[0]\n                # Possibly apply abstractive summary\n                if np.random.rand() > 0.5:\n                    try:\n                        text = aug_sent.augment(text)[0]\n                    except Exception:\n                        pass\n                text = eda_augment(text)\n                return text\n            else:\n                return x\n\n        # Apply augmentation to training data\n        train_df['text'] = train_df['text'].apply(augment_text)\n\n        # Oversample minority class\n        majority = train_df['label'].value_counts().idxmax()\n        minority = 1 - majority\n        maj_count = train_df['label'].value_counts().max()\n        min_df = train_df[train_df['label']==minority]\n        if len(min_df) < maj_count:\n            min_oversampled = min_df.sample(maj_count - len(min_df), replace=True, random_state=42)\n            train_df = pd.concat([train_df, min_oversampled]).reset_index(drop=True)\n\n        # Convert to Hugging Face datasets\n        train_dataset = Dataset.from_pandas(train_df[['text','label']])\n        val_dataset = Dataset.from_pandas(val_df[['text','label']])\n\n        def tokenize_function(examples):\n            return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n\n        tokenized_train = train_dataset.map(tokenize_function, batched=True)\n        tokenized_val   = val_dataset.map(tokenize_function, batched=True)\n\n        # Class weights (optional)\n        class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n        class_weights = torch.tensor(class_weights, dtype=torch.float)\n\n        # Load appropriate model\n        if model_name == 'electra':\n            model = AutoModelForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=2)\n        else:\n            model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\n        # Hyperparameter tuning with Optuna\n        def objective(trial):\n            lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n            batch = trial.suggest_categorical('batch_size', [4, 8, 16])\n            decay = trial.suggest_uniform('weight_decay', 0.001, 0.1)\n            args = TrainingArguments(\n                output_dir=f'/kaggle/working/trial_{model_name}_{fold}_{trial.number}',\n                num_train_epochs=15,\n                per_device_train_batch_size=batch,\n                per_device_eval_batch_size=batch,\n                warmup_steps=100,\n                weight_decay=decay,\n                learning_rate=lr,\n                eval_strategy='epoch',\n                save_strategy='no',\n                logging_strategy='epoch',\n                load_best_model_at_end=False,\n                metric_for_best_model='f1',\n                greater_is_better=True,\n                report_to='none'\n            )\n            trainer_opt = WeightedTrainer(\n                model=model,\n                args=args,\n                train_dataset=tokenized_train,\n                eval_dataset=tokenized_val,\n                compute_metrics=compute_metrics,\n                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n            )\n            trainer_opt.train()\n            eval_res = trainer_opt.evaluate()\n            shutil.rmtree(f'/kaggle/working/trial_{model_name}_{fold}_{trial.number}')\n            return eval_res['eval_f1']\n\n        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n        study.optimize(objective, n_trials=10)\n\n        best_params = study.best_params\n\n        # Final training with best parameters\n        final_args = TrainingArguments(\n            output_dir=f'/kaggle/working/results_{model_name}_fold{fold}',\n            num_train_epochs=15,\n            per_device_train_batch_size=best_params['batch_size'],\n            per_device_eval_batch_size=best_params['batch_size'],\n            warmup_steps=100,\n            weight_decay=best_params['weight_decay'],\n            learning_rate=best_params['learning_rate'],\n            eval_strategy='epoch',\n            save_strategy='epoch',\n            save_total_limit=1,\n            logging_strategy='epoch',\n            load_best_model_at_end=True,\n            metric_for_best_model='f1',\n            greater_is_better=True,\n            report_to='none'\n        )\n        trainer = WeightedTrainer(\n            model=model,\n            args=final_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            compute_metrics=compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        trainer.train()\n\n        preds_output = trainer.predict(tokenized_val)\n        preds = np.argmax(preds_output.predictions, axis=-1)\n        if model_name == 'electra':\n            bert_preds_all.extend(preds)\n            true_labels_bert.extend(preds_output.label_ids)\n            all_histories_bert.append(trainer.state.log_history)\n        else:\n            distil_preds_all.extend(preds)\n            true_labels_distil.extend(preds_output.label_ids)\n            all_histories_distil.append(trainer.state.log_history)\n\n        shutil.rmtree(f'/kaggle/working/results_{model_name}_fold{fold}')\n\n    # Save final model (from last fold)\n    if model_name == 'electra':\n        trainer.save_model('/kaggle/working/bert_model_final')\n    else:\n        trainer.save_model('/kaggle/working/distil_model_final')\n\n# Classification reports for BERT/ELECTRA and DistilBERT\nprint(\"BERT Classification Report:\")\nprint(classification_report(true_labels_bert, bert_preds_all, digits=4))\nprint(\"DistilBERT Classification Report:\")\nprint(classification_report(true_labels_distil, distil_preds_all, digits=4))\n\n# Aggregate training/validation loss and accuracy curves\ndef aggregate_histories(histories):\n    max_epochs = max(len([log for log in h if 'eval_loss' in log]) for h in histories)\n    train_loss_agg = np.zeros(max_epochs)\n    val_loss_agg   = np.zeros(max_epochs)\n    val_acc_agg    = np.zeros(max_epochs)\n    counts         = np.zeros(max_epochs)\n    for h in histories:\n        train_loss = [log['loss'] for log in h if 'loss' in log and 'epoch' in log and log['epoch'].is_integer()]\n        val_loss = [log['eval_loss'] for log in h if 'eval_loss' in log]\n        val_acc  = [log['eval_accuracy'] for log in h if 'eval_accuracy' in log]\n        num_ep = len(val_loss)\n        train_loss_agg[:num_ep] += train_loss[:num_ep]\n        val_loss_agg[:num_ep]   += val_loss\n        val_acc_agg[:num_ep]    += val_acc\n        counts[:num_ep]         += 1\n    train_loss_agg /= counts\n    val_loss_agg   /= counts\n    val_acc_agg    /= counts\n    return range(1, max_epochs+1), train_loss_agg, val_loss_agg, val_acc_agg\n\nepochs_bert, train_loss_bert, val_loss_bert, val_acc_bert     = aggregate_histories(all_histories_bert)\nepochs_distil, train_loss_distil, val_loss_distil, val_acc_distil = aggregate_histories(all_histories_distil)\n\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(epochs_bert, train_loss_bert, '--', label='BERT Train Loss')\nplt.plot(epochs_bert, val_loss_bert, label='BERT Val Loss')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('BERT Loss'); plt.legend()\nplt.subplot(1,2,2)\nplt.plot(epochs_bert, val_acc_bert, label='BERT Val Acc')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('BERT Accuracy'); plt.legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_val_curves_bert.png')\nplt.close()\n\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(epochs_distil, train_loss_distil, '--', label='Distil Train Loss')\nplt.plot(epochs_distil, val_loss_distil, label='Distil Val Loss')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Distil Loss'); plt.legend()\nplt.subplot(1,2,2)\nplt.plot(epochs_distil, val_acc_distil, label='Distil Val Acc')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Distil Accuracy'); plt.legend()\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_val_curves_distil.png')\nplt.close()\n\n# Plot ROC curves for both\nfpr_bert, tpr_bert, _ = roc_curve(true_labels_bert, bert_preds_all)\nroc_auc_bert = auc(fpr_bert, tpr_bert)\nfpr_distil, tpr_distil, _ = roc_curve(true_labels_distil, distil_preds_all)\nroc_auc_distil = auc(fpr_distil, tpr_distil)\nplt.figure(figsize=(8,6))\nplt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')\nplt.plot(fpr_distil, tpr_distil, label=f'DistilBERT (AUC = {roc_auc_distil:.2f})')\nplt.plot([0,1],[0,1],'k--')\nplt.title('ROC Curves'); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves-5.png')\nplt.close()\n\n# Confusion matrices side-by-side\nfig, axs = plt.subplots(1,2, figsize=(10,4))\ncm_bert = confusion_matrix(true_labels_bert, bert_preds_all)\nsns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', ax=axs[0])\naxs[0].set_title('BERT Confusion Matrix'); axs[0].set_xlabel('Predicted'); axs[0].set_ylabel('True')\n\ncm_distil = confusion_matrix(true_labels_distil, distil_preds_all)\nsns.heatmap(cm_distil, annot=True, fmt='d', cmap='Greens', ax=axs[1])\naxs[1].set_title('DistilBERT Confusion Matrix'); axs[1].set_xlabel('Predicted'); axs[1].set_ylabel('True')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrices-5.png')\nplt.close()\n\n# Classical ML models with TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nX = df['text']\ny = df['label']\ntfidf = TfidfVectorizer(max_features=1000)\nX_tfidf = tfidf.fit_transform(X).toarray()\n\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nxgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nsvm_clf = SVC(kernel='linear', probability=True, random_state=42)\n\nrf.fit(X_train, y_train)\nxgb_clf.fit(X_train, y_train)\nsvm_clf.fit(X_train, y_train)\n\nrf_preds  = rf.predict(X_test)\nxgb_preds = xgb_clf.predict(X_test)\nsvm_preds = svm_clf.predict(X_test)\n\nprint(\"Random Forest Accuracy:\", np.mean(rf_preds == y_test))\nprint(\"XGBoost Accuracy:\",     np.mean(xgb_preds == y_test))\nprint(\"SVM Accuracy:\",         np.mean(svm_preds == y_test))\n\nprint(\"Random Forest Classification Report:\")\nprint(classification_report(y_test, rf_preds, digits=4))\nprint(\"XGBoost Classification Report:\")\nprint(classification_report(y_test, xgb_preds, digits=4))\nprint(\"SVM Classification Report:\")\nprint(classification_report(y_test, svm_preds, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:51:43.350996Z","iopub.execute_input":"2025-11-09T17:51:43.351896Z","iopub.status.idle":"2025-11-09T17:56:28.167351Z","shell.execute_reply.started":"2025-11-09T17:51:43.351864Z","shell.execute_reply":"2025-11-09T17:56:28.166252Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->xgboost) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->xgboost) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->xgboost) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4952dc7c593b4d218873dbba31a5cf1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b7e29d0c324d799c6b87e578860476"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3460329104.py:156: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3460329104.py:158: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.001, 0.1)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3460329104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     ):\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3460329104.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStoppingCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             )\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mtrainer_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0meval_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/kaggle/working/trial_{model_name}_{fold}_{trial.number}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3749\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3460329104.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_outputs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3460329104.py\u001b[0m in \u001b[0;36mweighted_loss\u001b[0;34m(logits, labels, smoothing)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mlabels_smoothed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msmoothing\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_smoothed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (2) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (32) must match the size of tensor b (2) at non-singleton dimension 1","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# Install required packages (run once in the notebook)\n!pip install --quiet sentencepiece xgboost nlpaug evaluate optuna\n\n# -----------------------------------------\n# Imports and setup\n# -----------------------------------------\nimport logging, shutil\nlogging.disable(logging.WARNING)\n\nimport os\nimport torch\nimport optuna\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback,\n)\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom evaluate import load\n\n# augmentation\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\n# abst summariser wrapper (may be heavy; we'll guard with try/except)\ntry:\n    from nlpaug.augmenter.sentence.abst_summ import AbstSummAug\n    HAS_ABST = True\nexcept Exception:\n    HAS_ABST = False\n\nimport nltk\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# -----------------------------------------\n# Small helper augmentations (EDA + nlpaug wrappers)\n# -----------------------------------------\ndef eda_augment(text):\n    words = text.split()\n    if len(words) < 2:\n        return text\n    # small random ops\n    if np.random.rand() > 0.5:\n        i, j = np.random.choice(len(words), 2, replace=False)\n        words[i], words[j] = words[j], words[i]\n    if np.random.rand() > 0.6:\n        idx = np.random.randint(len(words))\n        words.insert(idx, words[idx])\n    if np.random.rand() > 0.6 and len(words) > 2:\n        idx = np.random.randint(len(words))\n        del words[idx]\n    return ' '.join(words)\n\n# -----------------------------------------\n# Load data\n# -----------------------------------------\ndf = pd.read_csv('/kaggle/working/processed_data.csv').dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Quick sanity check\nprint(\"Dataset size:\", len(df), \"Label distribution:\\n\", df['label'].value_counts())\n\n# -----------------------------------------\n# Metrics\n# -----------------------------------------\nf1_metric = load('f1')\nacc_metric = load('accuracy')\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\n        'accuracy': acc_metric.compute(predictions=preds, references=labels)['accuracy'],\n        'f1': f1_metric.compute(predictions=preds, references=labels)['f1']\n    }\n\n# -----------------------------------------\n# Weighted, label-smoothed loss (fixed)\n#   - Converts integer labels to one-hot\n#   - Moves tensors to logits.device\n# -----------------------------------------\ndef weighted_label_smoothing_loss(logits, labels, smoothing=0.1):\n    \"\"\"\n    logits: Tensor (batch, num_labels)\n    labels: Tensor (batch,) of ints OR Tensor (batch, num_labels) one-hot\n    \"\"\"\n    num_labels = logits.size(1)\n    device = logits.device\n\n    # If labels are ints (shape [batch]), convert to one-hot\n    if labels.dim() == 1 or (labels.dim() == 2 and labels.size(1) == 1):\n        labels_onehot = torch.nn.functional.one_hot(labels.long(), num_classes=num_labels).float().to(device)\n    else:\n        labels_onehot = labels.float().to(device)\n\n    # Apply label smoothing\n    labels_smoothed = labels_onehot * (1.0 - smoothing) + (smoothing / float(num_labels))\n\n    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n    loss = - (labels_smoothed * log_probs).sum(dim=1).mean()\n    return loss\n\n# -----------------------------------------\n# Trainer subclass fix:\n# - Accepts extra kwargs (num_items_in_batch) to match HF Trainer signature\n# - Uses weighted_label_smoothing_loss and ensures label device alignment\n# -----------------------------------------\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n        # 'labels' key should exist (see rename below)\n        labels = inputs.pop(\"labels\")\n        # move labels to same device if not already\n        labels = labels.to(model.device)\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = weighted_label_smoothing_loss(logits, labels, smoothing=0.1)\n        return (loss, outputs) if return_outputs else loss\n\n# -----------------------------------------\n# Prepare k-fold\n# -----------------------------------------\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# Pre-create tokenizer once (shared)\ntokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n\n# Collect metrics / histories\nbert_preds_all = []\ndistil_preds_all = []\ntrue_labels_bert = []\ntrue_labels_distil = []\nall_histories_bert = []\nall_histories_distil = []\n\n# Safety: limit max_length (saves memory), smaller optuna budget during tuning\nMAX_LEN = 128\nOPTUNA_TRIALS = 8            # reduce to save space/time\nOPTUNA_EPOCHS = 2            # short epochs for search\nFINAL_EPOCHS = 5             # final training epochs per fold (kept modest for Kaggle memory/time)\n\n# -----------------------------------------\n# Main training loop (Electra and DistilBERT)\n# -----------------------------------------\nfor model_name in ['electra', 'distilbert']:\n    print(f\"\\n=== Training backbone: {model_name} ===\")\n    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n        print(f\"\\n--- Fold {fold+1}/{kf.n_splits} ---\")\n        train_df = df.iloc[train_idx].reset_index(drop=True)\n        val_df   = df.iloc[val_idx].reset_index(drop=True)\n\n        # Augmenters (wrap in try/catch for abst summ)\n        aug_syn = naw.SynonymAug(aug_src='wordnet', aug_max=3)\n        aug_ins = nac.RandomCharAug(action=\"insert\", aug_char_max=2)\n        aug_del = nac.RandomCharAug(action=\"delete\", aug_char_max=1)\n        aug_sub = naw.RandomWordAug(action=\"substitute\")\n        if HAS_ABST:\n            try:\n                aug_sent = AbstSummAug(model_path='t5-small', min_length=10, max_length=40)\n            except Exception:\n                HAS_ABST = False\n\n        def augment_text(x):\n            if np.random.rand() > 0.3:\n                text = x\n                try:\n                    text = aug_sub.augment(text)[0]\n                except Exception:\n                    pass\n                try:\n                    text = aug_del.augment(text)[0]\n                except Exception:\n                    pass\n                try:\n                    text = aug_ins.augment(text)[0]\n                except Exception:\n                    pass\n                try:\n                    text = aug_syn.augment(text)[0]\n                except Exception:\n                    pass\n                if HAS_ABST and np.random.rand() > 0.6:\n                    try:\n                        text = aug_sent.augment(text)[0]\n                    except Exception:\n                        pass\n                text = eda_augment(text)\n                return text\n            return x\n\n        # Apply augmentation to training data (in-place)\n        train_df['text'] = train_df['text'].apply(augment_text)\n\n        # Manual oversampling of minority class\n        majority = train_df['label'].value_counts().idxmax()\n        maj_count = train_df['label'].value_counts().max()\n        minority_vals = train_df['label'].unique().tolist()\n        if len(minority_vals) > 1:\n            minority = [c for c in minority_vals if c != majority][0]\n            min_df = train_df[train_df['label'] == minority]\n            if len(min_df) < maj_count:\n                min_oversampled = min_df.sample(maj_count - len(min_df), replace=True, random_state=42)\n                train_df = pd.concat([train_df, min_oversampled]).reset_index(drop=True)\n\n        # Convert to HF Dataset and rename label -> labels\n        train_dataset = Dataset.from_pandas(train_df[['text', 'label']].rename(columns={'label': 'labels'}))\n        val_dataset   = Dataset.from_pandas(val_df[['text', 'label']].rename(columns={'label': 'labels'}))\n\n        # Tokenize\n        def tokenize_function(examples):\n            return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=MAX_LEN)\n\n        tokenized_train = train_dataset.map(tokenize_function, batched=True)\n        tokenized_val   = val_dataset.map(tokenize_function, batched=True)\n\n        # set format to torch (helpful)\n        tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n        tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n        # compute class weights (not directly used in loss here because we use label smoothing;\n        # you may incorporate weights if desired)\n        class_weights = compute_class_weight('balanced', classes=np.unique(train_df['labels']), y=train_df['labels'])\n        class_weights = torch.tensor(class_weights, dtype=torch.float)\n\n        # Load model\n        if model_name == 'electra':\n            model = AutoModelForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=2).to(device)\n        else:\n            model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(device)\n\n        # -------------------\n        # Optuna hyperparam search (short)\n        # -------------------\n        def objective(trial):\n            lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n            batch = trial.suggest_categorical('batch_size', [4, 8])\n            decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n\n            args = TrainingArguments(\n                output_dir=f'/kaggle/working/trial_{model_name}_{fold}_{trial.number}',\n                num_train_epochs=OPTUNA_EPOCHS,\n                per_device_train_batch_size=batch,\n                per_device_eval_batch_size=batch,\n                learning_rate=lr,\n                weight_decay=decay,\n                eval_strategy='epoch',\n                save_strategy='no',\n                logging_strategy='epoch',\n                report_to='none',\n                disable_tqdm=True\n            )\n\n            trainer_opt = WeightedTrainer(\n                model=model,\n                args=args,\n                train_dataset=tokenized_train,\n                eval_dataset=tokenized_val,\n                compute_metrics=compute_metrics,\n                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n            )\n\n            trainer_opt.train()\n            eval_res = trainer_opt.evaluate()\n            # Cleanup trial dir\n            try:\n                shutil.rmtree(args.output_dir)\n            except Exception:\n                pass\n            # return eval_f1 (trainer prefixes metrics with 'eval_')\n            return eval_res.get('eval_f1', 0.0)\n\n        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n        study.optimize(objective, n_trials=OPTUNA_TRIALS)\n\n        best_params = study.best_params if len(study.trials) > 0 else {'learning_rate': 5e-5, 'batch_size': 8, 'weight_decay': 0.0}\n        print(\"Optuna best:\", best_params)\n\n        # -------------------\n        # Final training with best params\n        # -------------------\n        final_args = TrainingArguments(\n            output_dir=f'/kaggle/working/results_{model_name}_fold{fold}',\n            num_train_epochs=FINAL_EPOCHS,\n            per_device_train_batch_size=best_params['batch_size'],\n            per_device_eval_batch_size=best_params['batch_size'],\n            learning_rate=best_params['learning_rate'],\n            weight_decay=best_params.get('weight_decay', 0.0),\n            eval_strategy='epoch',\n            save_strategy='epoch',\n            save_total_limit=1,\n            logging_strategy='epoch',\n            load_best_model_at_end=True,\n            metric_for_best_model='f1',\n            greater_is_better=True,\n            report_to='none',\n            disable_tqdm=False\n        )\n\n        trainer = WeightedTrainer(\n            model=model,\n            args=final_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            compute_metrics=compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n        )\n\n        trainer.train()\n\n        preds_output = trainer.predict(tokenized_val)\n        preds = np.argmax(preds_output.predictions, axis=-1)\n        if model_name == 'electra':\n            bert_preds_all.extend(preds.tolist())\n            true_labels_bert.extend(preds_output.label_ids.tolist())\n            all_histories_bert.append(trainer.state.log_history)\n        else:\n            distil_preds_all.extend(preds.tolist())\n            true_labels_distil.extend(preds_output.label_ids.tolist())\n            all_histories_distil.append(trainer.state.log_history)\n\n        # Cleanup fold checkpoint to save space\n        try:\n            shutil.rmtree(final_args.output_dir)\n        except Exception:\n            pass\n\n        # free GPU memory (helpful when doing multiple heavy runs in same process)\n        del trainer\n        del model\n        torch.cuda.empty_cache()\n\n    # Save final model (from last fold) to disk (lightweight; if you want to actually keep a final checkpoint, move save_model out of the loop)\n    try:\n        # If model variable still exists save last trained\n        model_save_dir = '/kaggle/working/bert_model_final' if model_name == 'electra' else '/kaggle/working/distil_model_final'\n        # create a tiny flag file instead of saving large checkpoint to stay under 8GB, or uncomment to save model\n        # trainer.save_model(model_save_dir)\n        open(model_save_dir + '_placeholder.txt', 'w').write(f'{model_name} model trained (checkpoint omitted to save space).')\n    except Exception:\n        pass\n\n# -----------------------------------------\n# Reports for transformer models\n# -----------------------------------------\nprint(\"\\n=== Transformer classification reports ===\")\nif len(true_labels_bert) > 0:\n    print(\"BERT Classification report:\")\n    print(classification_report(true_labels_bert, bert_preds_all, digits=4))\nelse:\n    print(\"No BERT predictions collected.\")\n\nif len(true_labels_distil) > 0:\n    print(\"DistilBERT Classification report:\")\n    print(classification_report(true_labels_distil, distil_preds_all, digits=4))\nelse:\n    print(\"No Distil predictions collected.\")\n\n# -----------------------------------------\n# Optional: aggregate and plot histories (if any)\n# -----------------------------------------\ndef aggregate_histories(histories):\n    if len(histories) == 0:\n        return [], [], [], []\n    max_epochs = max(len([log for log in h if 'eval_loss' in log]) for h in histories)\n    train_loss_agg = np.zeros(max_epochs)\n    val_loss_agg   = np.zeros(max_epochs)\n    val_acc_agg    = np.zeros(max_epochs)\n    counts         = np.zeros(max_epochs)\n    for h in histories:\n        train_loss = [log['loss'] for log in h if 'loss' in log and 'epoch' in log and float(log['epoch']).is_integer()]\n        val_loss = [log['eval_loss'] for log in h if 'eval_loss' in log]\n        val_acc  = [log['eval_accuracy'] for log in h if 'eval_accuracy' in log]\n        num_ep = len(val_loss)\n        if num_ep == 0:\n            continue\n        train_loss_agg[:num_ep] += np.array(train_loss[:num_ep])\n        val_loss_agg[:num_ep]   += np.array(val_loss[:num_ep])\n        val_acc_agg[:num_ep]    += np.array(val_acc[:num_ep])\n        counts[:num_ep]         += 1\n    nonzero = counts != 0\n    train_loss_agg[nonzero] /= counts[nonzero]\n    val_loss_agg[nonzero]   /= counts[nonzero]\n    val_acc_agg[nonzero]    /= counts[nonzero]\n    epochs = range(1, max_epochs + 1)\n    return epochs, train_loss_agg, val_loss_agg, val_acc_agg\n\n# (plotting code omitted here for brevity  use your existing plotting calls which save to /kaggle/working)\n# -----------------------------------------\n# Classical ML (TF-IDF -> RF / XGBoost / SVM) [kept separate]\n# -----------------------------------------\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nX = df['text'].fillna('').astype(str)\ny = df['label'].values\ntfidf = TfidfVectorizer(max_features=1000)\nX_tfidf = tfidf.fit_transform(X).toarray()\n\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nxgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nsvm_clf = SVC(kernel='linear', probability=True, random_state=42)\n\nrf.fit(X_train, y_train)\nxgb_clf.fit(X_train, y_train)\nsvm_clf.fit(X_train, y_train)\n\nrf_preds  = rf.predict(X_test)\nxgb_preds = xgb_clf.predict(X_test)\nsvm_preds = svm_clf.predict(X_test)\n\nprint(\"\\n=== Classical ML results (TF-IDF) ===\")\nprint(\"Random Forest Accuracy:\", np.mean(rf_preds == y_test))\nprint(\"XGBoost Accuracy:\",     np.mean(xgb_preds == y_test))\nprint(\"SVM Accuracy:\",         np.mean(svm_preds == y_test))\nprint(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, rf_preds, digits=4))\nprint(\"\\nXGBoost Classification Report:\\n\", classification_report(y_test, xgb_preds, digits=4))\nprint(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_preds, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T18:27:33.546268Z","iopub.execute_input":"2025-11-09T18:27:33.546535Z","iopub.status.idle":"2025-11-09T18:31:35.889309Z","shell.execute_reply.started":"2025-11-09T18:27:33.546511Z","shell.execute_reply":"2025-11-09T18:31:35.888344Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nDataset size: 141 Label distribution:\n label\n0    99\n1    42\nName: count, dtype: int64\n\n=== Training backbone: electra ===\n\n--- Fold 1/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b09324bab95464ab30932319ae26ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"688e46c1cff447b08d540ac07da0559f"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'labels'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2219109423.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# compute class weights (not directly used in loss here because we use label smoothing;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# you may incorporate weights if desired)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'labels'"],"ename":"KeyError","evalue":"'labels'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# Install required packages (run once in the notebook)\n!pip install --quiet sentencepiece xgboost nlpaug evaluate optuna\n\n# -----------------------------------------\n# Imports and setup\n# -----------------------------------------\nimport logging, shutil\nlogging.disable(logging.WARNING)\n\nimport os\nimport torch\nimport optuna\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback,\n)\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom evaluate import load\n\n# augmentation\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac\n# abst summariser wrapper (may be heavy; we'll guard with try/except)\ntry:\n    from nlpaug.augmenter.sentence.abst_summ import AbstSummAug\n    HAS_ABST = True\nexcept Exception:\n    HAS_ABST = False\n\nimport nltk\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger_eng', quiet=True)\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# -----------------------------------------\n# Small helper augmentations (EDA + nlpaug wrappers)\n# -----------------------------------------\ndef eda_augment(text):\n    words = text.split()\n    if len(words) < 2:\n        return text\n    # small random ops\n    if np.random.rand() > 0.5:\n        i, j = np.random.choice(len(words), 2, replace=False)\n        words[i], words[j] = words[j], words[i]\n    if np.random.rand() > 0.6:\n        idx = np.random.randint(len(words))\n        words.insert(idx, words[idx])\n    if np.random.rand() > 0.6 and len(words) > 2:\n        idx = np.random.randint(len(words))\n        del words[idx]\n    return ' '.join(words)\n\n# -----------------------------------------\n# Load data\n# -----------------------------------------\ndf = pd.read_csv('/kaggle/working/processed_data.csv').dropna(subset=['label']).reset_index(drop=True)\ndf['label'] = df['label'].astype(int)\n\n# Quick sanity check\nprint(\"Dataset size:\", len(df), \"Label distribution:\\n\", df['label'].value_counts())\n\n# -----------------------------------------\n# Metrics\n# -----------------------------------------\nf1_metric = load('f1')\nacc_metric = load('accuracy')\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\n        'accuracy': acc_metric.compute(predictions=preds, references=labels)['accuracy'],\n        'f1': f1_metric.compute(predictions=preds, references=labels)['f1']\n    }\n\n# -----------------------------------------\n# Weighted, label-smoothed loss (fixed)\n#   - Converts integer labels to one-hot\n#   - Moves tensors to logits.device\n# -----------------------------------------\ndef weighted_label_smoothing_loss(logits, labels, smoothing=0.1):\n    \"\"\"\n    logits: Tensor (batch, num_labels)\n    labels: Tensor (batch,) of ints OR Tensor (batch, num_labels) one-hot\n    \"\"\"\n    num_labels = logits.size(1)\n    device = logits.device\n\n    # If labels are ints (shape [batch]), convert to one-hot\n    if labels.dim() == 1 or (labels.dim() == 2 and labels.size(1) == 1):\n        labels_onehot = torch.nn.functional.one_hot(labels.long(), num_classes=num_labels).float().to(device)\n    else:\n        labels_onehot = labels.float().to(device)\n\n    # Apply label smoothing\n    labels_smoothed = labels_onehot * (1.0 - smoothing) + (smoothing / float(num_labels))\n\n    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n    loss = - (labels_smoothed * log_probs).sum(dim=1).mean()\n    return loss\n\n# -----------------------------------------\n# Trainer subclass fix:\n# - Accepts extra kwargs (num_items_in_batch) to match HF Trainer signature\n# - Uses weighted_label_smoothing_loss and ensures label device alignment\n# -----------------------------------------\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n        labels = inputs.pop(\"labels\")\n\n        # Safely infer device, even if wrapped in DataParallel\n        try:\n            device = next(model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        labels = labels.to(device)\n\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = weighted_label_smoothing_loss(logits, labels, smoothing=0.1)\n\n        return (loss, outputs) if return_outputs else loss\n\n# -----------------------------------------\n# Prepare k-fold\n# -----------------------------------------\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# Pre-create tokenizer once (shared)\ntokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n\n# Collect metrics / histories\nbert_preds_all = []\ndistil_preds_all = []\ntrue_labels_bert = []\ntrue_labels_distil = []\nall_histories_bert = []\nall_histories_distil = []\n\n# Safety: limit max_length (saves memory), smaller optuna budget during tuning\nMAX_LEN = 128\nOPTUNA_TRIALS = 8            # reduce to save space/time\nOPTUNA_EPOCHS = 2            # short epochs for search\nFINAL_EPOCHS = 5             # final training epochs per fold (kept modest for Kaggle memory/time)\n\n# -----------------------------------------\n# Main training loop (Electra and DistilBERT)\n# -----------------------------------------\nfor model_name in ['electra', 'distilbert']:\n    print(f\"\\n=== Training backbone: {model_name} ===\")\n    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n        print(f\"\\n--- Fold {fold+1}/{kf.n_splits} ---\")\n        train_df = df.iloc[train_idx].reset_index(drop=True)\n        val_df   = df.iloc[val_idx].reset_index(drop=True)\n\n        # Augmenters (wrap in try/catch for abst summ)\n        aug_syn = naw.SynonymAug(aug_src='wordnet', aug_max=3)\n        aug_ins = nac.RandomCharAug(action=\"insert\", aug_char_max=2)\n        aug_del = nac.RandomCharAug(action=\"delete\", aug_char_max=1)\n        aug_sub = naw.RandomWordAug(action=\"substitute\")\n        if HAS_ABST:\n            try:\n                aug_sent = AbstSummAug(model_path='t5-small', min_length=10, max_length=40)\n            except Exception:\n                HAS_ABST = False\n\n        def augment_text(x):\n            if np.random.rand() > 0.3:\n                text = x\n                try:\n                    text = aug_sub.augment(text)[0]\n                except Exception:\n                    pass\n                try:\n                    text = aug_del.augment(text)[0]\n                except Exception:\n                    pass\n                try:\n                    text = aug_ins.augment(text)[0]\n                except Exception:\n                    pass\n                try:\n                    text = aug_syn.augment(text)[0]\n                except Exception:\n                    pass\n                if HAS_ABST and np.random.rand() > 0.6:\n                    try:\n                        text = aug_sent.augment(text)[0]\n                    except Exception:\n                        pass\n                text = eda_augment(text)\n                return text\n            return x\n\n        # Apply augmentation to training data (in-place)\n        train_df['text'] = train_df['text'].apply(augment_text)\n\n        # Manual oversampling of minority class\n        majority = train_df['label'].value_counts().idxmax()\n        maj_count = train_df['label'].value_counts().max()\n        minority_vals = train_df['label'].unique().tolist()\n        if len(minority_vals) > 1:\n            minority = [c for c in minority_vals if c != majority][0]\n            min_df = train_df[train_df['label'] == minority]\n            if len(min_df) < maj_count:\n                min_oversampled = min_df.sample(maj_count - len(min_df), replace=True, random_state=42)\n                train_df = pd.concat([train_df, min_oversampled]).reset_index(drop=True)\n\n        # Convert to HF Dataset and rename label -> labels\n        train_dataset = Dataset.from_pandas(train_df[['text', 'label']].rename(columns={'label': 'labels'}))\n        val_dataset   = Dataset.from_pandas(val_df[['text', 'label']].rename(columns={'label': 'labels'}))\n\n        # Tokenize\n        def tokenize_function(examples):\n            return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=MAX_LEN)\n\n        tokenized_train = train_dataset.map(tokenize_function, batched=True)\n        tokenized_val   = val_dataset.map(tokenize_function, batched=True)\n\n        # set format to torch (helpful)\n        tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n        tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n        # compute class weights (not directly used in loss here because we use label smoothing;\n        # you may incorporate weights if desired)\n        class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n        class_weights = torch.tensor(class_weights, dtype=torch.float)\n\n        # Load model\n        if model_name == 'electra':\n            model = AutoModelForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=2).to(device)\n        else:\n            model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(device)\n\n        # -------------------\n        # Optuna hyperparam search (short)\n        # -------------------\n        def objective(trial):\n            lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n            batch = trial.suggest_categorical('batch_size', [4, 8])\n            decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n        \n            args = TrainingArguments(\n                output_dir=f'/kaggle/working/trial_{model_name}_{fold}_{trial.number}',\n                num_train_epochs=OPTUNA_EPOCHS,\n                per_device_train_batch_size=batch,\n                per_device_eval_batch_size=batch,\n                learning_rate=lr,\n                weight_decay=decay,\n                eval_strategy='epoch',\n                save_strategy='no',\n                logging_strategy='epoch',\n                report_to='none',\n                disable_tqdm=True,\n                # <<-- IMPORTANT: provide a metric for EarlyStoppingCallback to monitor:\n                metric_for_best_model='f1',\n                greater_is_better=True,\n                # You can leave load_best_model_at_end False to speed up trials:\n                load_best_model_at_end=False,\n            )\n        \n            trainer_opt = WeightedTrainer(\n                model=model,\n                args=args,\n                train_dataset=tokenized_train,\n                eval_dataset=tokenized_val,\n                compute_metrics=compute_metrics,\n                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n            )\n        \n            trainer_opt.train()\n            eval_res = trainer_opt.evaluate()\n        \n            # Cleanup trial dir\n            try:\n                shutil.rmtree(args.output_dir)\n            except Exception:\n                pass\n        \n            # return the eval_f1 if present, otherwise fallback to eval_accuracy or 0.0\n            return eval_res.get('eval_f1', eval_res.get('eval_accuracy', 0.0))\n\n\n        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n        study.optimize(objective, n_trials=OPTUNA_TRIALS)\n\n        best_params = study.best_params if len(study.trials) > 0 else {'learning_rate': 5e-5, 'batch_size': 8, 'weight_decay': 0.0}\n        print(\"Optuna best:\", best_params)\n\n        # -------------------\n        # Final training with best params\n        # -------------------\n        final_args = TrainingArguments(\n            output_dir=f'/kaggle/working/results_{model_name}_fold{fold}',\n            num_train_epochs=FINAL_EPOCHS,\n            per_device_train_batch_size=best_params['batch_size'],\n            per_device_eval_batch_size=best_params['batch_size'],\n            learning_rate=best_params['learning_rate'],\n            weight_decay=best_params.get('weight_decay', 0.0),\n            eval_strategy='epoch',\n            save_strategy='epoch',\n            save_total_limit=1,\n            logging_strategy='epoch',\n            load_best_model_at_end=True,\n            metric_for_best_model='f1',\n            greater_is_better=True,\n            report_to='none',\n            disable_tqdm=False\n        )\n\n        trainer = WeightedTrainer(\n            model=model,\n            args=final_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            compute_metrics=compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n        )\n\n        trainer.train()\n\n        preds_output = trainer.predict(tokenized_val)\n        preds = np.argmax(preds_output.predictions, axis=-1)\n        if model_name == 'electra':\n            bert_preds_all.extend(preds.tolist())\n            true_labels_bert.extend(preds_output.label_ids.tolist())\n            all_histories_bert.append(trainer.state.log_history)\n        else:\n            distil_preds_all.extend(preds.tolist())\n            true_labels_distil.extend(preds_output.label_ids.tolist())\n            all_histories_distil.append(trainer.state.log_history)\n\n        # Cleanup fold checkpoint to save space\n        try:\n            shutil.rmtree(final_args.output_dir)\n        except Exception:\n            pass\n\n        # free GPU memory (helpful when doing multiple heavy runs in same process)\n        del trainer\n        del model\n        torch.cuda.empty_cache()\n\n    # Save final model (from last fold) to disk (lightweight; if you want to actually keep a final checkpoint, move save_model out of the loop)\n    try:\n        # If model variable still exists save last trained\n        model_save_dir = '/kaggle/working/bert_model_final' if model_name == 'electra' else '/kaggle/working/distil_model_final'\n        # create a tiny flag file instead of saving large checkpoint to stay under 8GB, or uncomment to save model\n        # trainer.save_model(model_save_dir)\n        open(model_save_dir + '_placeholder.txt', 'w').write(f'{model_name} model trained (checkpoint omitted to save space).')\n    except Exception:\n        pass\n\n# -----------------------------------------\n# Reports for transformer models\n# -----------------------------------------\nprint(\"\\n=== Transformer classification reports ===\")\nif len(true_labels_bert) > 0:\n    print(\"BERT Classification report:\")\n    print(classification_report(true_labels_bert, bert_preds_all, digits=4))\nelse:\n    print(\"No BERT predictions collected.\")\n\nif len(true_labels_distil) > 0:\n    print(\"DistilBERT Classification report:\")\n    print(classification_report(true_labels_distil, distil_preds_all, digits=4))\nelse:\n    print(\"No Distil predictions collected.\")\n\n# -----------------------------------------\n# Optional: aggregate and plot histories (if any)\n# -----------------------------------------\ndef aggregate_histories(histories):\n    if len(histories) == 0:\n        return [], [], [], []\n    max_epochs = max(len([log for log in h if 'eval_loss' in log]) for h in histories)\n    train_loss_agg = np.zeros(max_epochs)\n    val_loss_agg   = np.zeros(max_epochs)\n    val_acc_agg    = np.zeros(max_epochs)\n    counts         = np.zeros(max_epochs)\n    for h in histories:\n        train_loss = [log['loss'] for log in h if 'loss' in log and 'epoch' in log and float(log['epoch']).is_integer()]\n        val_loss = [log['eval_loss'] for log in h if 'eval_loss' in log]\n        val_acc  = [log['eval_accuracy'] for log in h if 'eval_accuracy' in log]\n        num_ep = len(val_loss)\n        if num_ep == 0:\n            continue\n        train_loss_agg[:num_ep] += np.array(train_loss[:num_ep])\n        val_loss_agg[:num_ep]   += np.array(val_loss[:num_ep])\n        val_acc_agg[:num_ep]    += np.array(val_acc[:num_ep])\n        counts[:num_ep]         += 1\n    nonzero = counts != 0\n    train_loss_agg[nonzero] /= counts[nonzero]\n    val_loss_agg[nonzero]   /= counts[nonzero]\n    val_acc_agg[nonzero]    /= counts[nonzero]\n    epochs = range(1, max_epochs + 1)\n    return epochs, train_loss_agg, val_loss_agg, val_acc_agg\n\n# (plotting code omitted here for brevity  use your existing plotting calls which save to /kaggle/working)\n# -----------------------------------------\n# Classical ML (TF-IDF -> RF / XGBoost / SVM) [kept separate]\n# -----------------------------------------\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nX = df['text'].fillna('').astype(str)\ny = df['label'].values\ntfidf = TfidfVectorizer(max_features=1000)\nX_tfidf = tfidf.fit_transform(X).toarray()\n\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nxgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nsvm_clf = SVC(kernel='linear', probability=True, random_state=42)\n\nrf.fit(X_train, y_train)\nxgb_clf.fit(X_train, y_train)\nsvm_clf.fit(X_train, y_train)\n\nrf_preds  = rf.predict(X_test)\nxgb_preds = xgb_clf.predict(X_test)\nsvm_preds = svm_clf.predict(X_test)\n\nprint(\"\\n=== Classical ML results (TF-IDF) ===\")\nprint(\"Random Forest Accuracy:\", np.mean(rf_preds == y_test))\nprint(\"XGBoost Accuracy:\",     np.mean(xgb_preds == y_test))\nprint(\"SVM Accuracy:\",         np.mean(svm_preds == y_test))\nprint(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, rf_preds, digits=4))\nprint(\"\\nXGBoost Classification Report:\\n\", classification_report(y_test, xgb_preds, digits=4))\nprint(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_preds, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T06:09:50.066302Z","iopub.execute_input":"2025-11-10T06:09:50.066556Z","iopub.status.idle":"2025-11-10T06:35:22.437616Z","shell.execute_reply.started":"2025-11-10T06:09:50.066535Z","shell.execute_reply":"2025-11-10T06:35:22.436672Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nDataset size: 141 Label distribution:\n label\n0    99\n1    42\nName: count, dtype: int64\n\n=== Training backbone: electra ===\n\n--- Fold 1/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63fd25f4188c4bc0bf12239dd2c99e32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5f3e103d2441cfb4a7f0da4253d2a5"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6968, 'grad_norm': 1.0627872943878174, 'learning_rate': 2.5592982895904703e-06, 'epoch': 1.0}\n{'eval_loss': 0.6895796656608582, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.32, 'eval_runtime': 0.2953, 'eval_samples_per_second': 159.169, 'eval_steps_per_second': 10.16, 'epoch': 1.0}\n{'loss': 0.6926, 'grad_norm': 0.9557263255119324, 'learning_rate': 2.843664766211634e-07, 'epoch': 2.0}\n{'eval_loss': 0.6875399351119995, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.1904761904761905, 'eval_runtime': 0.285, 'eval_samples_per_second': 164.898, 'eval_steps_per_second': 10.525, 'epoch': 2.0}\n{'train_runtime': 7.1569, 'train_samples_per_second': 35.211, 'train_steps_per_second': 2.236, 'train_loss': 0.6947312653064728, 'epoch': 2.0}\n{'eval_loss': 0.6875399351119995, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.1904761904761905, 'eval_runtime': 0.286, 'eval_samples_per_second': 164.337, 'eval_steps_per_second': 10.49, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6943, 'grad_norm': 1.9914301633834839, 'learning_rate': 2.0190195993149852e-05, 'epoch': 1.0}\n{'eval_loss': 0.6552426218986511, 'eval_accuracy': 0.7659574468085106, 'eval_f1': 0.0, 'eval_runtime': 0.4104, 'eval_samples_per_second': 114.522, 'eval_steps_per_second': 14.62, 'epoch': 1.0}\n{'loss': 0.6752, 'grad_norm': 2.3915278911590576, 'learning_rate': 1.1876585878323442e-06, 'epoch': 2.0}\n{'eval_loss': 0.6740753054618835, 'eval_accuracy': 0.7446808510638298, 'eval_f1': 0.0, 'eval_runtime': 0.4124, 'eval_samples_per_second': 113.957, 'eval_steps_per_second': 14.548, 'epoch': 2.0}\n{'train_runtime': 7.6318, 'train_samples_per_second': 33.02, 'train_steps_per_second': 4.193, 'train_loss': 0.6847401261329651, 'epoch': 2.0}\n{'eval_loss': 0.6740753054618835, 'eval_accuracy': 0.7446808510638298, 'eval_f1': 0.0, 'eval_runtime': 0.4102, 'eval_samples_per_second': 114.576, 'eval_steps_per_second': 14.627, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6492, 'grad_norm': 1.7731350660324097, 'learning_rate': 1.2792137306207063e-05, 'epoch': 1.0}\n{'eval_loss': 0.6868032217025757, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.3448275862068966, 'eval_runtime': 0.2935, 'eval_samples_per_second': 160.11, 'eval_steps_per_second': 10.22, 'epoch': 1.0}\n{'loss': 0.5956, 'grad_norm': 4.9061665534973145, 'learning_rate': 1.4213485895785625e-06, 'epoch': 2.0}\n{'eval_loss': 0.6667937636375427, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.3846153846153846, 'eval_runtime': 0.294, 'eval_samples_per_second': 159.883, 'eval_steps_per_second': 10.205, 'epoch': 2.0}\n{'train_runtime': 5.6007, 'train_samples_per_second': 44.994, 'train_steps_per_second': 2.857, 'train_loss': 0.622422456741333, 'epoch': 2.0}\n{'eval_loss': 0.6667937636375427, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.3846153846153846, 'eval_runtime': 0.2944, 'eval_samples_per_second': 159.642, 'eval_steps_per_second': 10.19, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4823, 'grad_norm': 6.545901775360107, 'learning_rate': 7.523467065982754e-06, 'epoch': 1.0}\n{'eval_loss': 0.6277399063110352, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.32, 'eval_runtime': 0.4144, 'eval_samples_per_second': 113.424, 'eval_steps_per_second': 14.48, 'epoch': 1.0}\n{'loss': 0.4249, 'grad_norm': 4.4586501121521, 'learning_rate': 4.4255688623427967e-07, 'epoch': 2.0}\n{'eval_loss': 0.607974112033844, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.19999999999999998, 'eval_runtime': 0.4187, 'eval_samples_per_second': 112.259, 'eval_steps_per_second': 14.331, 'epoch': 2.0}\n{'train_runtime': 7.7617, 'train_samples_per_second': 32.467, 'train_steps_per_second': 4.123, 'train_loss': 0.45363347232341766, 'epoch': 2.0}\n{'eval_loss': 0.607974112033844, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.19999999999999998, 'eval_runtime': 0.4185, 'eval_samples_per_second': 112.303, 'eval_steps_per_second': 14.336, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3094, 'grad_norm': 16.911884307861328, 'learning_rate': 1.561598383997116e-05, 'epoch': 1.0}\n{'eval_loss': 0.6307031512260437, 'eval_accuracy': 0.7872340425531915, 'eval_f1': 0.16666666666666669, 'eval_runtime': 0.42, 'eval_samples_per_second': 111.904, 'eval_steps_per_second': 14.286, 'epoch': 1.0}\n{'loss': 0.2647, 'grad_norm': 15.547993659973145, 'learning_rate': 9.185872847041859e-07, 'epoch': 2.0}\n{'eval_loss': 0.7327175736427307, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.2105263157894737, 'eval_runtime': 0.4177, 'eval_samples_per_second': 112.525, 'eval_steps_per_second': 14.365, 'epoch': 2.0}\n{'train_runtime': 7.8363, 'train_samples_per_second': 32.158, 'train_steps_per_second': 4.084, 'train_loss': 0.2870629280805588, 'epoch': 2.0}\n{'eval_loss': 0.7327175736427307, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.2105263157894737, 'eval_runtime': 0.4164, 'eval_samples_per_second': 112.881, 'eval_steps_per_second': 14.41, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2041, 'grad_norm': 0.5211778283119202, 'learning_rate': 3.304815012015637e-06, 'epoch': 1.0}\n{'eval_loss': 0.819225013256073, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.24999999999999994, 'eval_runtime': 0.418, 'eval_samples_per_second': 112.448, 'eval_steps_per_second': 14.355, 'epoch': 1.0}\n{'loss': 0.2173, 'grad_norm': 18.048328399658203, 'learning_rate': 1.9440088305974335e-07, 'epoch': 2.0}\n{'eval_loss': 0.8555616140365601, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.2608695652173913, 'eval_runtime': 0.4184, 'eval_samples_per_second': 112.33, 'eval_steps_per_second': 14.34, 'epoch': 2.0}\n{'train_runtime': 7.8883, 'train_samples_per_second': 31.946, 'train_steps_per_second': 4.057, 'train_loss': 0.21070002019405365, 'epoch': 2.0}\n{'eval_loss': 0.8555616140365601, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.2608695652173913, 'eval_runtime': 0.4193, 'eval_samples_per_second': 112.105, 'eval_steps_per_second': 14.311, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2049, 'grad_norm': 6.203447341918945, 'learning_rate': 3.3270588429772214e-06, 'epoch': 1.0}\n{'eval_loss': 0.8801245093345642, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.19999999999999998, 'eval_runtime': 0.3035, 'eval_samples_per_second': 154.846, 'eval_steps_per_second': 9.884, 'epoch': 1.0}\n{'loss': 0.2047, 'grad_norm': 1.6676926612854004, 'learning_rate': 3.6967320477524683e-07, 'epoch': 2.0}\n{'eval_loss': 0.9400182962417603, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.2608695652173913, 'eval_runtime': 0.3071, 'eval_samples_per_second': 153.036, 'eval_steps_per_second': 9.768, 'epoch': 2.0}\n{'train_runtime': 5.8567, 'train_samples_per_second': 43.028, 'train_steps_per_second': 2.732, 'train_loss': 0.20480488240718842, 'epoch': 2.0}\n{'eval_loss': 0.9400182962417603, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.2608695652173913, 'eval_runtime': 0.3049, 'eval_samples_per_second': 154.145, 'eval_steps_per_second': 9.839, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3317, 'grad_norm': 1.9334635734558105, 'learning_rate': 1.8733584942438526e-05, 'epoch': 1.0}\n{'eval_loss': 0.7824650406837463, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.19999999999999998, 'eval_runtime': 0.2959, 'eval_samples_per_second': 158.813, 'eval_steps_per_second': 10.137, 'epoch': 1.0}\n{'loss': 0.2207, 'grad_norm': 0.2794460654258728, 'learning_rate': 2.081509438048725e-06, 'epoch': 2.0}\n{'eval_loss': 0.8474298119544983, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.2727272727272727, 'eval_runtime': 0.2983, 'eval_samples_per_second': 157.564, 'eval_steps_per_second': 10.057, 'epoch': 2.0}\n{'train_runtime': 5.8249, 'train_samples_per_second': 43.263, 'train_steps_per_second': 2.747, 'train_loss': 0.2761740908026695, 'epoch': 2.0}\n{'eval_loss': 0.8474298119544983, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.2727272727272727, 'eval_runtime': 0.2998, 'eval_samples_per_second': 156.783, 'eval_steps_per_second': 10.007, 'epoch': 2.0}\nOptuna best: {'learning_rate': 2.2741577433257e-05, 'batch_size': 8, 'weight_decay': 0.045814161442316995}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24/40 00:15 < 00:11, 1.42 it/s, Epoch 3/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.279500</td>\n      <td>1.282165</td>\n      <td>0.510638</td>\n      <td>0.488889</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.230400</td>\n      <td>0.918194</td>\n      <td>0.744681</td>\n      <td>0.142857</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.201200</td>\n      <td>0.979965</td>\n      <td>0.595745</td>\n      <td>0.296296</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- Fold 2/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/130 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee70498f39c41538d8951837004eb3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81c5e0c0760944ecbe6c9518668555c1"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6909, 'grad_norm': 3.843859910964966, 'learning_rate': 1.8625452143084514e-06, 'epoch': 1.0}\n{'eval_loss': 0.6746432185173035, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.4153, 'eval_samples_per_second': 113.17, 'eval_steps_per_second': 14.447, 'epoch': 1.0}\n{'loss': 0.6902, 'grad_norm': 1.5177994966506958, 'learning_rate': 1.0347473412824729e-07, 'epoch': 2.0}\n{'eval_loss': 0.6762040853500366, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.4132, 'eval_samples_per_second': 113.733, 'eval_steps_per_second': 14.519, 'epoch': 2.0}\n{'train_runtime': 8.0879, 'train_samples_per_second': 32.147, 'train_steps_per_second': 4.204, 'train_loss': 0.6905800595003015, 'epoch': 2.0}\n{'eval_loss': 0.6762040853500366, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.411, 'eval_samples_per_second': 114.362, 'eval_steps_per_second': 14.599, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6899, 'grad_norm': 3.7528302669525146, 'learning_rate': 3.1198278128843776e-06, 'epoch': 1.0}\n{'eval_loss': 0.6797093152999878, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.2915, 'eval_samples_per_second': 161.232, 'eval_steps_per_second': 10.291, 'epoch': 1.0}\n{'loss': 0.6855, 'grad_norm': 1.3944755792617798, 'learning_rate': 3.1198278128843774e-07, 'epoch': 2.0}\n{'eval_loss': 0.6805365681648254, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.2914, 'eval_samples_per_second': 161.317, 'eval_steps_per_second': 10.297, 'epoch': 2.0}\n{'train_runtime': 6.0154, 'train_samples_per_second': 43.222, 'train_steps_per_second': 2.992, 'train_loss': 0.6876735157436795, 'epoch': 2.0}\n{'eval_loss': 0.6805365681648254, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.2954, 'eval_samples_per_second': 159.12, 'eval_steps_per_second': 10.157, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6737, 'grad_norm': 4.131551265716553, 'learning_rate': 8.255144224354927e-07, 'epoch': 1.0}\n{'eval_loss': 0.6759700179100037, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.4165, 'eval_samples_per_second': 112.833, 'eval_steps_per_second': 14.404, 'epoch': 1.0}\n{'loss': 0.6824, 'grad_norm': 1.606316089630127, 'learning_rate': 4.586191235752737e-08, 'epoch': 2.0}\n{'eval_loss': 0.6761118173599243, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.4165, 'eval_samples_per_second': 112.832, 'eval_steps_per_second': 14.404, 'epoch': 2.0}\n{'train_runtime': 8.1644, 'train_samples_per_second': 31.846, 'train_steps_per_second': 4.164, 'train_loss': 0.6780673475826487, 'epoch': 2.0}\n{'eval_loss': 0.6761118173599243, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.4195, 'eval_samples_per_second': 112.033, 'eval_steps_per_second': 14.302, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6642, 'grad_norm': 5.0976128578186035, 'learning_rate': 5.0483663099155514e-06, 'epoch': 1.0}\n{'eval_loss': 0.6872014999389648, 'eval_accuracy': 0.574468085106383, 'eval_f1': 0.4444444444444444, 'eval_runtime': 0.4175, 'eval_samples_per_second': 112.577, 'eval_steps_per_second': 14.372, 'epoch': 1.0}\n{'loss': 0.6593, 'grad_norm': 2.875606060028076, 'learning_rate': 2.804647949953084e-07, 'epoch': 2.0}\n{'eval_loss': 0.682597815990448, 'eval_accuracy': 0.574468085106383, 'eval_f1': 0.37499999999999994, 'eval_runtime': 0.4195, 'eval_samples_per_second': 112.037, 'eval_steps_per_second': 14.303, 'epoch': 2.0}\n{'train_runtime': 8.2627, 'train_samples_per_second': 31.467, 'train_steps_per_second': 4.115, 'train_loss': 0.6617506812600529, 'epoch': 2.0}\n{'eval_loss': 0.682597815990448, 'eval_accuracy': 0.574468085106383, 'eval_f1': 0.37499999999999994, 'eval_runtime': 0.4195, 'eval_samples_per_second': 112.03, 'eval_steps_per_second': 14.302, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6309, 'grad_norm': 6.329982757568359, 'learning_rate': 5.0874343596785534e-06, 'epoch': 1.0}\n{'eval_loss': 0.6875419616699219, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.39999999999999997, 'eval_runtime': 0.3014, 'eval_samples_per_second': 155.93, 'eval_steps_per_second': 9.953, 'epoch': 1.0}\n{'loss': 0.6158, 'grad_norm': 5.585785388946533, 'learning_rate': 5.087434359678553e-07, 'epoch': 2.0}\n{'eval_loss': 0.6786210536956787, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.4375, 'eval_runtime': 0.3108, 'eval_samples_per_second': 151.229, 'eval_steps_per_second': 9.653, 'epoch': 2.0}\n{'train_runtime': 6.2762, 'train_samples_per_second': 41.426, 'train_steps_per_second': 2.868, 'train_loss': 0.623350116941664, 'epoch': 2.0}\n{'eval_loss': 0.6786210536956787, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.4375, 'eval_runtime': 0.3051, 'eval_samples_per_second': 154.047, 'eval_steps_per_second': 9.833, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5372, 'grad_norm': 16.404739379882812, 'learning_rate': 1.1414059683529914e-05, 'epoch': 1.0}\n{'eval_loss': 0.8000257611274719, 'eval_accuracy': 0.3617021276595745, 'eval_f1': 0.46428571428571425, 'eval_runtime': 0.4191, 'eval_samples_per_second': 112.154, 'eval_steps_per_second': 14.318, 'epoch': 1.0}\n{'loss': 0.4578, 'grad_norm': 14.261924743652344, 'learning_rate': 6.34114426862773e-07, 'epoch': 2.0}\n{'eval_loss': 0.7175499796867371, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.45, 'eval_runtime': 0.4172, 'eval_samples_per_second': 112.665, 'eval_steps_per_second': 14.383, 'epoch': 2.0}\n{'train_runtime': 8.3016, 'train_samples_per_second': 31.319, 'train_steps_per_second': 4.096, 'train_loss': 0.49752194741192984, 'epoch': 2.0}\n{'eval_loss': 0.7175499796867371, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.45, 'eval_runtime': 0.4193, 'eval_samples_per_second': 112.086, 'eval_steps_per_second': 14.309, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3476, 'grad_norm': 1.794262170791626, 'learning_rate': 1.099971010017662e-06, 'epoch': 1.0}\n{'eval_loss': 0.7171590924263, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.4210526315789474, 'eval_runtime': 0.4168, 'eval_samples_per_second': 112.754, 'eval_steps_per_second': 14.394, 'epoch': 1.0}\n{'loss': 0.3824, 'grad_norm': 18.883228302001953, 'learning_rate': 6.110950055653679e-08, 'epoch': 2.0}\n{'eval_loss': 0.7575093507766724, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.45, 'eval_runtime': 0.4167, 'eval_samples_per_second': 112.792, 'eval_steps_per_second': 14.399, 'epoch': 2.0}\n{'train_runtime': 8.2118, 'train_samples_per_second': 31.662, 'train_steps_per_second': 4.14, 'train_loss': 0.36502230868620034, 'epoch': 2.0}\n{'eval_loss': 0.7575093507766724, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.45, 'eval_runtime': 0.4155, 'eval_samples_per_second': 113.111, 'eval_steps_per_second': 14.44, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3198, 'grad_norm': 1.4821791648864746, 'learning_rate': 6.687379783410034e-07, 'epoch': 1.0}\n{'eval_loss': 0.7291131615638733, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.46153846153846156, 'eval_runtime': 0.4169, 'eval_samples_per_second': 112.742, 'eval_steps_per_second': 14.393, 'epoch': 1.0}\n{'loss': 0.3648, 'grad_norm': 19.283964157104492, 'learning_rate': 3.7152109907833514e-08, 'epoch': 2.0}\n{'eval_loss': 0.7448656558990479, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.45, 'eval_runtime': 0.4143, 'eval_samples_per_second': 113.446, 'eval_steps_per_second': 14.483, 'epoch': 2.0}\n{'train_runtime': 8.1458, 'train_samples_per_second': 31.918, 'train_steps_per_second': 4.174, 'train_loss': 0.34230922250186696, 'epoch': 2.0}\n{'eval_loss': 0.7448656558990479, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.45, 'eval_runtime': 0.4131, 'eval_samples_per_second': 113.771, 'eval_steps_per_second': 14.524, 'epoch': 2.0}\nOptuna best: {'learning_rate': 2.155989051333428e-05, 'batch_size': 4, 'weight_decay': 0.09109473961370051}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [85/85 00:32, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.314900</td>\n      <td>1.231049</td>\n      <td>0.340426</td>\n      <td>0.436364</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.264500</td>\n      <td>0.841815</td>\n      <td>0.680851</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.266400</td>\n      <td>1.022139</td>\n      <td>0.659574</td>\n      <td>0.466667</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.206900</td>\n      <td>1.143013</td>\n      <td>0.595745</td>\n      <td>0.457143</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.200500</td>\n      <td>1.105924</td>\n      <td>0.617021</td>\n      <td>0.400000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- Fold 3/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/140 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca091c0d2eb4ba190bb612840c52f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e177c3fc8e4dd795bd161ead39ee5d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6975, 'grad_norm': 0.803502082824707, 'learning_rate': 1.577504744044468e-05, 'epoch': 1.0}\n{'eval_loss': 0.6857113838195801, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.0, 'eval_runtime': 0.2884, 'eval_samples_per_second': 162.983, 'eval_steps_per_second': 10.403, 'epoch': 1.0}\n{'loss': 0.6827, 'grad_norm': 1.0048165321350098, 'learning_rate': 1.5775047440444677e-06, 'epoch': 2.0}\n{'eval_loss': 0.6887685656547546, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.1818181818181818, 'eval_runtime': 0.2943, 'eval_samples_per_second': 159.728, 'eval_steps_per_second': 10.195, 'epoch': 2.0}\n{'train_runtime': 6.1711, 'train_samples_per_second': 45.373, 'train_steps_per_second': 2.917, 'train_loss': 0.6900927755567763, 'epoch': 2.0}\n{'eval_loss': 0.6887685656547546, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.1818181818181818, 'eval_runtime': 0.2935, 'eval_samples_per_second': 160.13, 'eval_steps_per_second': 10.221, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6803, 'grad_norm': 2.189988136291504, 'learning_rate': 3.79825529238448e-06, 'epoch': 1.0}\n{'eval_loss': 0.7029301524162292, 'eval_accuracy': 0.46808510638297873, 'eval_f1': 0.5901639344262295, 'eval_runtime': 0.4166, 'eval_samples_per_second': 112.824, 'eval_steps_per_second': 14.403, 'epoch': 1.0}\n{'loss': 0.6628, 'grad_norm': 1.4140905141830444, 'learning_rate': 1.9990817328339365e-07, 'epoch': 2.0}\n{'eval_loss': 0.7094212770462036, 'eval_accuracy': 0.425531914893617, 'eval_f1': 0.5714285714285715, 'eval_runtime': 0.4162, 'eval_samples_per_second': 112.94, 'eval_steps_per_second': 14.418, 'epoch': 2.0}\n{'train_runtime': 8.6052, 'train_samples_per_second': 32.538, 'train_steps_per_second': 4.184, 'train_loss': 0.6715257432725694, 'epoch': 2.0}\n{'eval_loss': 0.7094212770462036, 'eval_accuracy': 0.425531914893617, 'eval_f1': 0.5714285714285715, 'eval_runtime': 0.42, 'eval_samples_per_second': 111.906, 'eval_steps_per_second': 14.286, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6188, 'grad_norm': 3.6670141220092773, 'learning_rate': 5.1075752864614444e-06, 'epoch': 1.0}\n{'eval_loss': 0.7142623066902161, 'eval_accuracy': 0.5106382978723404, 'eval_f1': 0.5660377358490566, 'eval_runtime': 0.4182, 'eval_samples_per_second': 112.39, 'eval_steps_per_second': 14.348, 'epoch': 1.0}\n{'loss': 0.5655, 'grad_norm': 2.3630123138427734, 'learning_rate': 2.688197519190234e-07, 'epoch': 2.0}\n{'eval_loss': 0.7460662126541138, 'eval_accuracy': 0.46808510638297873, 'eval_f1': 0.5614035087719298, 'eval_runtime': 0.42, 'eval_samples_per_second': 111.901, 'eval_steps_per_second': 14.285, 'epoch': 2.0}\n{'train_runtime': 8.7142, 'train_samples_per_second': 32.132, 'train_steps_per_second': 4.131, 'train_loss': 0.592177046669854, 'epoch': 2.0}\n{'eval_loss': 0.7460662126541138, 'eval_accuracy': 0.46808510638297873, 'eval_f1': 0.5614035087719298, 'eval_runtime': 0.4184, 'eval_samples_per_second': 112.342, 'eval_steps_per_second': 14.342, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5213, 'grad_norm': 3.7407209873199463, 'learning_rate': 1.3682754725197238e-06, 'epoch': 1.0}\n{'eval_loss': 0.740334153175354, 'eval_accuracy': 0.5106382978723404, 'eval_f1': 0.5660377358490566, 'eval_runtime': 0.3087, 'eval_samples_per_second': 152.239, 'eval_steps_per_second': 9.717, 'epoch': 1.0}\n{'loss': 0.5082, 'grad_norm': 2.9099302291870117, 'learning_rate': 1.3682754725197238e-07, 'epoch': 2.0}\n{'eval_loss': 0.7233980894088745, 'eval_accuracy': 0.425531914893617, 'eval_f1': 0.4255319148936171, 'eval_runtime': 0.3012, 'eval_samples_per_second': 156.059, 'eval_steps_per_second': 9.961, 'epoch': 2.0}\n{'train_runtime': 6.5394, 'train_samples_per_second': 42.817, 'train_steps_per_second': 2.753, 'train_loss': 0.5147736337449815, 'epoch': 2.0}\n{'eval_loss': 0.7233980894088745, 'eval_accuracy': 0.425531914893617, 'eval_f1': 0.4255319148936171, 'eval_runtime': 0.3007, 'eval_samples_per_second': 156.309, 'eval_steps_per_second': 9.977, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5165, 'grad_norm': 2.5573275089263916, 'learning_rate': 1.0963092095335647e-05, 'epoch': 1.0}\n{'eval_loss': 0.7444683909416199, 'eval_accuracy': 0.5106382978723404, 'eval_f1': 0.46511627906976744, 'eval_runtime': 0.3019, 'eval_samples_per_second': 155.693, 'eval_steps_per_second': 9.938, 'epoch': 1.0}\n{'loss': 0.3949, 'grad_norm': 6.174884796142578, 'learning_rate': 1.0963092095335645e-06, 'epoch': 2.0}\n{'eval_loss': 0.7706601619720459, 'eval_accuracy': 0.5106382978723404, 'eval_f1': 0.41025641025641024, 'eval_runtime': 0.2994, 'eval_samples_per_second': 156.961, 'eval_steps_per_second': 10.019, 'epoch': 2.0}\n{'train_runtime': 6.5237, 'train_samples_per_second': 42.921, 'train_steps_per_second': 2.759, 'train_loss': 0.4556861188676622, 'epoch': 2.0}\n{'eval_loss': 0.7706601619720459, 'eval_accuracy': 0.5106382978723404, 'eval_f1': 0.41025641025641024, 'eval_runtime': 0.3003, 'eval_samples_per_second': 156.507, 'eval_steps_per_second': 9.99, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3869, 'grad_norm': 12.188671112060547, 'learning_rate': 2.5622653565065326e-06, 'epoch': 1.0}\n{'eval_loss': 0.8961686491966248, 'eval_accuracy': 0.425531914893617, 'eval_f1': 0.490566037735849, 'eval_runtime': 0.3007, 'eval_samples_per_second': 156.292, 'eval_steps_per_second': 9.976, 'epoch': 1.0}\n{'loss': 0.3302, 'grad_norm': 8.880826950073242, 'learning_rate': 2.562265356506532e-07, 'epoch': 2.0}\n{'eval_loss': 0.7817558646202087, 'eval_accuracy': 0.48936170212765956, 'eval_f1': 0.36842105263157887, 'eval_runtime': 0.2982, 'eval_samples_per_second': 157.629, 'eval_steps_per_second': 10.061, 'epoch': 2.0}\n{'train_runtime': 6.469, 'train_samples_per_second': 43.283, 'train_steps_per_second': 2.783, 'train_loss': 0.35856523778703475, 'epoch': 2.0}\n{'eval_loss': 0.7817558646202087, 'eval_accuracy': 0.48936170212765956, 'eval_f1': 0.36842105263157887, 'eval_runtime': 0.299, 'eval_samples_per_second': 157.166, 'eval_steps_per_second': 10.032, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4966, 'grad_norm': 8.430488586425781, 'learning_rate': 2.097674957466812e-05, 'epoch': 1.0}\n{'eval_loss': 0.900469183921814, 'eval_accuracy': 0.48936170212765956, 'eval_f1': 0.39999999999999997, 'eval_runtime': 0.4163, 'eval_samples_per_second': 112.903, 'eval_steps_per_second': 14.413, 'epoch': 1.0}\n{'loss': 0.2867, 'grad_norm': 0.5012531876564026, 'learning_rate': 1.104039451298322e-06, 'epoch': 2.0}\n{'eval_loss': 1.1715549230575562, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.4210526315789474, 'eval_runtime': 0.4152, 'eval_samples_per_second': 113.192, 'eval_steps_per_second': 14.45, 'epoch': 2.0}\n{'train_runtime': 8.6496, 'train_samples_per_second': 32.371, 'train_steps_per_second': 4.162, 'train_loss': 0.39162562953101265, 'epoch': 2.0}\n{'eval_loss': 1.1715549230575562, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.4210526315789474, 'eval_runtime': 0.415, 'eval_samples_per_second': 113.265, 'eval_steps_per_second': 14.459, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.217, 'grad_norm': 0.28779545426368713, 'learning_rate': 1.2973182348714492e-06, 'epoch': 1.0}\n{'eval_loss': 1.5198338031768799, 'eval_accuracy': 0.40425531914893614, 'eval_f1': 0.391304347826087, 'eval_runtime': 0.4148, 'eval_samples_per_second': 113.307, 'eval_steps_per_second': 14.465, 'epoch': 1.0}\n{'loss': 0.2136, 'grad_norm': 0.1398761123418808, 'learning_rate': 6.827990709849733e-08, 'epoch': 2.0}\n{'eval_loss': 1.439662218093872, 'eval_accuracy': 0.46808510638297873, 'eval_f1': 0.41860465116279066, 'eval_runtime': 0.4157, 'eval_samples_per_second': 113.071, 'eval_steps_per_second': 14.435, 'epoch': 2.0}\n{'train_runtime': 8.6082, 'train_samples_per_second': 32.527, 'train_steps_per_second': 4.182, 'train_loss': 0.21530956692165798, 'epoch': 2.0}\n{'eval_loss': 1.439662218093872, 'eval_accuracy': 0.46808510638297873, 'eval_f1': 0.41860465116279066, 'eval_runtime': 0.4158, 'eval_samples_per_second': 113.048, 'eval_steps_per_second': 14.432, 'epoch': 2.0}\nOptuna best: {'learning_rate': 7.196694238202172e-06, 'batch_size': 4, 'weight_decay': 0.06062351525248064}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [90/90 00:32, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.227400</td>\n      <td>1.257065</td>\n      <td>0.595745</td>\n      <td>0.240000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.212100</td>\n      <td>1.597193</td>\n      <td>0.468085</td>\n      <td>0.418605</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.208300</td>\n      <td>1.716449</td>\n      <td>0.382979</td>\n      <td>0.431373</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.199300</td>\n      <td>1.530897</td>\n      <td>0.489362</td>\n      <td>0.428571</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.199400</td>\n      <td>1.539883</td>\n      <td>0.489362</td>\n      <td>0.428571</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n=== Training backbone: distilbert ===\n\n--- Fold 1/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f7143551804ca795aecad5b2a1992c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d0b14b77e464223b88017e038e267b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7cba35cf9f48c8a2b616a6ee0a6fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c77cf080cb941ce8c3463701782771d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6917, 'grad_norm': 1.2850912809371948, 'learning_rate': 3.3241257926970908e-06, 'epoch': 1.0}\n{'eval_loss': 0.6922319531440735, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.4705882352941176, 'eval_runtime': 0.1581, 'eval_samples_per_second': 297.252, 'eval_steps_per_second': 18.974, 'epoch': 1.0}\n{'loss': 0.6885, 'grad_norm': 1.0156744718551636, 'learning_rate': 3.6934731029967675e-07, 'epoch': 2.0}\n{'eval_loss': 0.6859829425811768, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.19999999999999998, 'eval_runtime': 0.1589, 'eval_samples_per_second': 295.702, 'eval_steps_per_second': 18.875, 'epoch': 2.0}\n{'train_runtime': 2.9932, 'train_samples_per_second': 84.192, 'train_steps_per_second': 5.346, 'train_loss': 0.6900874972343445, 'epoch': 2.0}\n{'eval_loss': 0.6859829425811768, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.19999999999999998, 'eval_runtime': 0.1571, 'eval_samples_per_second': 299.16, 'eval_steps_per_second': 19.095, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6816, 'grad_norm': 1.5085108280181885, 'learning_rate': 4.384864216885782e-06, 'epoch': 1.0}\n{'eval_loss': 0.6986938714981079, 'eval_accuracy': 0.40425531914893614, 'eval_f1': 0.4166666666666667, 'eval_runtime': 0.1593, 'eval_samples_per_second': 294.963, 'eval_steps_per_second': 18.827, 'epoch': 1.0}\n{'loss': 0.6783, 'grad_norm': 1.9311621189117432, 'learning_rate': 4.872071352095313e-07, 'epoch': 2.0}\n{'eval_loss': 0.6805468797683716, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.2727272727272727, 'eval_runtime': 0.157, 'eval_samples_per_second': 299.3, 'eval_steps_per_second': 19.104, 'epoch': 2.0}\n{'train_runtime': 2.8349, 'train_samples_per_second': 88.893, 'train_steps_per_second': 5.644, 'train_loss': 0.6799592971801758, 'epoch': 2.0}\n{'eval_loss': 0.6805468797683716, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.2727272727272727, 'eval_runtime': 0.1556, 'eval_samples_per_second': 302.075, 'eval_steps_per_second': 19.281, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.68, 'grad_norm': 2.503674268722534, 'learning_rate': 1.0907632462551345e-06, 'epoch': 1.0}\n{'eval_loss': 0.6831958889961243, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.34782608695652173, 'eval_runtime': 0.2563, 'eval_samples_per_second': 183.385, 'eval_steps_per_second': 23.411, 'epoch': 1.0}\n{'loss': 0.6669, 'grad_norm': 2.7907471656799316, 'learning_rate': 6.416254389736085e-08, 'epoch': 2.0}\n{'eval_loss': 0.6770843267440796, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.28571428571428564, 'eval_runtime': 0.2402, 'eval_samples_per_second': 195.689, 'eval_steps_per_second': 24.982, 'epoch': 2.0}\n{'train_runtime': 4.1954, 'train_samples_per_second': 60.066, 'train_steps_per_second': 7.627, 'train_loss': 0.6734593510627747, 'epoch': 2.0}\n{'eval_loss': 0.6770843267440796, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.28571428571428564, 'eval_runtime': 0.2409, 'eval_samples_per_second': 195.111, 'eval_steps_per_second': 24.908, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6671, 'grad_norm': 3.652045726776123, 'learning_rate': 1.3765719644681406e-05, 'epoch': 1.0}\n{'eval_loss': 0.618857741355896, 'eval_accuracy': 0.7446808510638298, 'eval_f1': 0.14285714285714288, 'eval_runtime': 0.2408, 'eval_samples_per_second': 195.165, 'eval_steps_per_second': 24.915, 'epoch': 1.0}\n{'loss': 0.566, 'grad_norm': 5.009571552276611, 'learning_rate': 8.097482143930239e-07, 'epoch': 2.0}\n{'eval_loss': 0.6505151987075806, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.2727272727272727, 'eval_runtime': 0.242, 'eval_samples_per_second': 194.211, 'eval_steps_per_second': 24.793, 'epoch': 2.0}\n{'train_runtime': 4.1445, 'train_samples_per_second': 60.803, 'train_steps_per_second': 7.721, 'train_loss': 0.6165767908096313, 'epoch': 2.0}\n{'eval_loss': 0.6505151987075806, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.2727272727272727, 'eval_runtime': 0.2429, 'eval_samples_per_second': 193.503, 'eval_steps_per_second': 24.703, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5373, 'grad_norm': 6.256407737731934, 'learning_rate': 2.0157692190358236e-06, 'epoch': 1.0}\n{'eval_loss': 0.6092937588691711, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.23529411764705885, 'eval_runtime': 0.1603, 'eval_samples_per_second': 293.198, 'eval_steps_per_second': 18.715, 'epoch': 1.0}\n{'loss': 0.5163, 'grad_norm': 4.389978885650635, 'learning_rate': 2.2397435767064709e-07, 'epoch': 2.0}\n{'eval_loss': 0.6265950798988342, 'eval_accuracy': 0.7021276595744681, 'eval_f1': 0.2222222222222222, 'eval_runtime': 0.1594, 'eval_samples_per_second': 294.791, 'eval_steps_per_second': 18.816, 'epoch': 2.0}\n{'train_runtime': 2.9146, 'train_samples_per_second': 86.46, 'train_steps_per_second': 5.49, 'train_loss': 0.5267767012119293, 'epoch': 2.0}\n{'eval_loss': 0.6265950798988342, 'eval_accuracy': 0.7021276595744681, 'eval_f1': 0.2222222222222222, 'eval_runtime': 0.1607, 'eval_samples_per_second': 292.561, 'eval_steps_per_second': 18.674, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4974, 'grad_norm': 3.7834177017211914, 'learning_rate': 6.243808081982246e-07, 'epoch': 1.0}\n{'eval_loss': 0.6073195934295654, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.23529411764705885, 'eval_runtime': 0.161, 'eval_samples_per_second': 291.944, 'eval_steps_per_second': 18.635, 'epoch': 1.0}\n{'loss': 0.5002, 'grad_norm': 3.7289633750915527, 'learning_rate': 6.93756453553583e-08, 'epoch': 2.0}\n{'eval_loss': 0.6102170348167419, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.23529411764705885, 'eval_runtime': 0.1599, 'eval_samples_per_second': 293.869, 'eval_steps_per_second': 18.758, 'epoch': 2.0}\n{'train_runtime': 2.9154, 'train_samples_per_second': 86.438, 'train_steps_per_second': 5.488, 'train_loss': 0.4988123029470444, 'epoch': 2.0}\n{'eval_loss': 0.6102170348167419, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.23529411764705885, 'eval_runtime': 0.1592, 'eval_samples_per_second': 295.162, 'eval_steps_per_second': 18.84, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4489, 'grad_norm': 3.5285351276397705, 'learning_rate': 2.6377289873700505e-06, 'epoch': 1.0}\n{'eval_loss': 0.6457459330558777, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.24999999999999994, 'eval_runtime': 0.2441, 'eval_samples_per_second': 192.575, 'eval_steps_per_second': 24.584, 'epoch': 1.0}\n{'loss': 0.4256, 'grad_norm': 5.6714701652526855, 'learning_rate': 1.551605286688265e-07, 'epoch': 2.0}\n{'eval_loss': 0.6387691497802734, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.28571428571428564, 'eval_runtime': 0.2502, 'eval_samples_per_second': 187.817, 'eval_steps_per_second': 23.977, 'epoch': 2.0}\n{'train_runtime': 4.1829, 'train_samples_per_second': 60.246, 'train_steps_per_second': 7.65, 'train_loss': 0.4372832179069519, 'epoch': 2.0}\n{'eval_loss': 0.6387691497802734, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.28571428571428564, 'eval_runtime': 0.2427, 'eval_samples_per_second': 193.681, 'eval_steps_per_second': 24.725, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3834, 'grad_norm': 7.623273849487305, 'learning_rate': 1.2477445347132885e-06, 'epoch': 1.0}\n{'eval_loss': 0.6323767304420471, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.23529411764705885, 'eval_runtime': 0.2438, 'eval_samples_per_second': 192.799, 'eval_steps_per_second': 24.613, 'epoch': 1.0}\n{'loss': 0.3891, 'grad_norm': 7.622156620025635, 'learning_rate': 7.33967373360758e-08, 'epoch': 2.0}\n{'eval_loss': 0.6685354113578796, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.23999999999999996, 'eval_runtime': 0.2415, 'eval_samples_per_second': 194.616, 'eval_steps_per_second': 24.845, 'epoch': 2.0}\n{'train_runtime': 4.1875, 'train_samples_per_second': 60.178, 'train_steps_per_second': 7.642, 'train_loss': 0.3862539231777191, 'epoch': 2.0}\n{'eval_loss': 0.6685354113578796, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.23999999999999996, 'eval_runtime': 0.2423, 'eval_samples_per_second': 193.977, 'eval_steps_per_second': 24.763, 'epoch': 2.0}\nOptuna best: {'learning_rate': 2.0532014047155473e-06, 'batch_size': 4, 'weight_decay': 0.0030935509214790272}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='48' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [48/80 00:10 < 00:07, 4.35 it/s, Epoch 3/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.351700</td>\n      <td>0.651359</td>\n      <td>0.638298</td>\n      <td>0.260870</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.362700</td>\n      <td>0.697517</td>\n      <td>0.553191</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.374300</td>\n      <td>0.654064</td>\n      <td>0.638298</td>\n      <td>0.190476</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- Fold 2/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/130 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531109a3aa144a7ca44f46a012fe1a4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ca4cb3179f345399d1e3e553f3e0946"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6926, 'grad_norm': 5.641051292419434, 'learning_rate': 1.8992412601697413e-06, 'epoch': 1.0}\n{'eval_loss': 0.6789324879646301, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.1557, 'eval_samples_per_second': 301.916, 'eval_steps_per_second': 19.271, 'epoch': 1.0}\n{'loss': 0.6927, 'grad_norm': 1.8807872533798218, 'learning_rate': 1.899241260169741e-07, 'epoch': 2.0}\n{'eval_loss': 0.6802465915679932, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.1576, 'eval_samples_per_second': 298.211, 'eval_steps_per_second': 19.035, 'epoch': 2.0}\n{'train_runtime': 3.0858, 'train_samples_per_second': 84.257, 'train_steps_per_second': 5.833, 'train_loss': 0.6926558282640245, 'epoch': 2.0}\n{'eval_loss': 0.6802465915679932, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.0, 'eval_runtime': 0.159, 'eval_samples_per_second': 295.634, 'eval_steps_per_second': 18.87, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.7027, 'grad_norm': 6.40826940536499, 'learning_rate': 1.8468911718623202e-05, 'epoch': 1.0}\n{'eval_loss': 0.729393482208252, 'eval_accuracy': 0.2765957446808511, 'eval_f1': 0.43333333333333335, 'eval_runtime': 0.2425, 'eval_samples_per_second': 193.795, 'eval_steps_per_second': 24.74, 'epoch': 1.0}\n{'loss': 0.6818, 'grad_norm': 1.3651882410049438, 'learning_rate': 1.0260506510346223e-06, 'epoch': 2.0}\n{'eval_loss': 0.6875542402267456, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.45161290322580644, 'eval_runtime': 0.2407, 'eval_samples_per_second': 195.259, 'eval_steps_per_second': 24.927, 'epoch': 2.0}\n{'train_runtime': 4.3399, 'train_samples_per_second': 59.909, 'train_steps_per_second': 7.834, 'train_loss': 0.6922769266016343, 'epoch': 2.0}\n{'eval_loss': 0.6875542402267456, 'eval_accuracy': 0.6382978723404256, 'eval_f1': 0.45161290322580644, 'eval_runtime': 0.2405, 'eval_samples_per_second': 195.396, 'eval_steps_per_second': 24.944, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6433, 'grad_norm': 5.871222496032715, 'learning_rate': 9.438749723979899e-07, 'epoch': 1.0}\n{'eval_loss': 0.6870095133781433, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.4375, 'eval_runtime': 0.2423, 'eval_samples_per_second': 194.013, 'eval_steps_per_second': 24.768, 'epoch': 1.0}\n{'loss': 0.6573, 'grad_norm': 1.6107012033462524, 'learning_rate': 5.243749846655499e-08, 'epoch': 2.0}\n{'eval_loss': 0.6887298226356506, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.39999999999999997, 'eval_runtime': 0.244, 'eval_samples_per_second': 192.61, 'eval_steps_per_second': 24.589, 'epoch': 2.0}\n{'train_runtime': 4.3521, 'train_samples_per_second': 59.741, 'train_steps_per_second': 7.812, 'train_loss': 0.6503080760731417, 'epoch': 2.0}\n{'eval_loss': 0.6887298226356506, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.39999999999999997, 'eval_runtime': 0.2421, 'eval_samples_per_second': 194.112, 'eval_steps_per_second': 24.78, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6398, 'grad_norm': 10.66108512878418, 'learning_rate': 1.2990057485192971e-05, 'epoch': 1.0}\n{'eval_loss': 0.6718288064002991, 'eval_accuracy': 0.574468085106383, 'eval_f1': 0.37499999999999994, 'eval_runtime': 0.1599, 'eval_samples_per_second': 294.006, 'eval_steps_per_second': 18.766, 'epoch': 1.0}\n{'loss': 0.552, 'grad_norm': 3.7642853260040283, 'learning_rate': 1.299005748519297e-06, 'epoch': 2.0}\n{'eval_loss': 0.6186023354530334, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.48000000000000004, 'eval_runtime': 0.1595, 'eval_samples_per_second': 294.595, 'eval_steps_per_second': 18.804, 'epoch': 2.0}\n{'train_runtime': 3.1192, 'train_samples_per_second': 83.354, 'train_steps_per_second': 5.771, 'train_loss': 0.5959138340420194, 'epoch': 2.0}\n{'eval_loss': 0.6186023354530334, 'eval_accuracy': 0.723404255319149, 'eval_f1': 0.48000000000000004, 'eval_runtime': 0.1592, 'eval_samples_per_second': 295.271, 'eval_steps_per_second': 18.847, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4754, 'grad_norm': 11.095765113830566, 'learning_rate': 2.1335676135417933e-05, 'epoch': 1.0}\n{'eval_loss': 0.6425074338912964, 'eval_accuracy': 0.6595744680851063, 'eval_f1': 0.42857142857142855, 'eval_runtime': 0.2437, 'eval_samples_per_second': 192.898, 'eval_steps_per_second': 24.625, 'epoch': 1.0}\n{'loss': 0.2876, 'grad_norm': 5.815072536468506, 'learning_rate': 1.1853153408565518e-06, 'epoch': 2.0}\n{'eval_loss': 0.7763460278511047, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.39999999999999997, 'eval_runtime': 0.2432, 'eval_samples_per_second': 193.256, 'eval_steps_per_second': 24.671, 'epoch': 2.0}\n{'train_runtime': 4.3686, 'train_samples_per_second': 59.516, 'train_steps_per_second': 7.783, 'train_loss': 0.38148340056924257, 'epoch': 2.0}\n{'eval_loss': 0.7763460278511047, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.39999999999999997, 'eval_runtime': 0.2449, 'eval_samples_per_second': 191.884, 'eval_steps_per_second': 24.496, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2938, 'grad_norm': 1.0264561176300049, 'learning_rate': 2.3868673894623575e-05, 'epoch': 1.0}\n{'eval_loss': 0.8371390700340271, 'eval_accuracy': 0.7446808510638298, 'eval_f1': 0.4, 'eval_runtime': 0.2407, 'eval_samples_per_second': 195.274, 'eval_steps_per_second': 24.929, 'epoch': 1.0}\n{'loss': 0.2659, 'grad_norm': 3.900761127471924, 'learning_rate': 1.3260374385901985e-06, 'epoch': 2.0}\n{'eval_loss': 1.20984947681427, 'eval_accuracy': 0.48936170212765956, 'eval_f1': 0.4285714285714286, 'eval_runtime': 0.2409, 'eval_samples_per_second': 195.068, 'eval_steps_per_second': 24.902, 'epoch': 2.0}\n{'train_runtime': 4.3833, 'train_samples_per_second': 59.316, 'train_steps_per_second': 7.757, 'train_loss': 0.2798164872562184, 'epoch': 2.0}\n{'eval_loss': 1.20984947681427, 'eval_accuracy': 0.48936170212765956, 'eval_f1': 0.4285714285714286, 'eval_runtime': 0.2405, 'eval_samples_per_second': 195.436, 'eval_steps_per_second': 24.949, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2478, 'grad_norm': 114.35002899169922, 'learning_rate': 1.535352410796191e-05, 'epoch': 1.0}\n{'eval_loss': 1.108500599861145, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.4210526315789474, 'eval_runtime': 0.2423, 'eval_samples_per_second': 194.011, 'eval_steps_per_second': 24.767, 'epoch': 1.0}\n{'loss': 0.2617, 'grad_norm': 0.1301361620426178, 'learning_rate': 8.529735615534393e-07, 'epoch': 2.0}\n{'eval_loss': 0.7730994820594788, 'eval_accuracy': 0.7659574468085106, 'eval_f1': 0.3529411764705882, 'eval_runtime': 0.2464, 'eval_samples_per_second': 190.753, 'eval_steps_per_second': 24.351, 'epoch': 2.0}\n{'train_runtime': 4.4057, 'train_samples_per_second': 59.014, 'train_steps_per_second': 7.717, 'train_loss': 0.2547765759860768, 'epoch': 2.0}\n{'eval_loss': 0.7730994820594788, 'eval_accuracy': 0.7659574468085106, 'eval_f1': 0.3529411764705882, 'eval_runtime': 0.2436, 'eval_samples_per_second': 192.933, 'eval_steps_per_second': 24.63, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2207, 'grad_norm': 0.15808641910552979, 'learning_rate': 5.059010480288309e-06, 'epoch': 1.0}\n{'eval_loss': 0.7437832355499268, 'eval_accuracy': 0.7659574468085106, 'eval_f1': 0.3529411764705882, 'eval_runtime': 0.1614, 'eval_samples_per_second': 291.177, 'eval_steps_per_second': 18.586, 'epoch': 1.0}\n{'loss': 0.2025, 'grad_norm': 0.1845475286245346, 'learning_rate': 5.059010480288308e-07, 'epoch': 2.0}\n{'eval_loss': 0.8111284375190735, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.4, 'eval_runtime': 0.1613, 'eval_samples_per_second': 291.417, 'eval_steps_per_second': 18.601, 'epoch': 2.0}\n{'train_runtime': 3.1247, 'train_samples_per_second': 83.207, 'train_steps_per_second': 5.76, 'train_loss': 0.2115867469045851, 'epoch': 2.0}\n{'eval_loss': 0.8111284375190735, 'eval_accuracy': 0.6808510638297872, 'eval_f1': 0.4, 'eval_runtime': 0.1622, 'eval_samples_per_second': 289.81, 'eval_steps_per_second': 18.499, 'epoch': 2.0}\nOptuna best: {'learning_rate': 2.3382103473347347e-05, 'batch_size': 8, 'weight_decay': 0.0059612621838000205}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/45 00:12 < 00:03, 2.81 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.260400</td>\n      <td>1.183888</td>\n      <td>0.489362</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.207500</td>\n      <td>0.908491</td>\n      <td>0.659574</td>\n      <td>0.466667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.205400</td>\n      <td>0.818521</td>\n      <td>0.744681</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.199500</td>\n      <td>0.778286</td>\n      <td>0.787234</td>\n      <td>0.375000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- Fold 3/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/140 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"613e21e071d6419cb6dbc6864dedaf28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0d3965664345c7b06d019393c9eacb"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.69, 'grad_norm': 1.0901196002960205, 'learning_rate': 1.447631078097992e-05, 'epoch': 1.0}\n{'eval_loss': 0.6842471957206726, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.0, 'eval_runtime': 0.1559, 'eval_samples_per_second': 301.388, 'eval_steps_per_second': 19.238, 'epoch': 1.0}\n{'loss': 0.6769, 'grad_norm': 1.074989676475525, 'learning_rate': 1.447631078097992e-06, 'epoch': 2.0}\n{'eval_loss': 0.6825673580169678, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.0, 'eval_runtime': 0.1592, 'eval_samples_per_second': 295.172, 'eval_steps_per_second': 18.841, 'epoch': 2.0}\n{'train_runtime': 3.1676, 'train_samples_per_second': 88.394, 'train_steps_per_second': 5.682, 'train_loss': 0.6834445264604356, 'epoch': 2.0}\n{'eval_loss': 0.6825673580169678, 'eval_accuracy': 0.6170212765957447, 'eval_f1': 0.0, 'eval_runtime': 0.1564, 'eval_samples_per_second': 300.492, 'eval_steps_per_second': 19.18, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6733, 'grad_norm': 4.921998023986816, 'learning_rate': 2.5852031927139713e-05, 'epoch': 1.0}\n{'eval_loss': 0.6836732625961304, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.0, 'eval_runtime': 0.2387, 'eval_samples_per_second': 196.896, 'eval_steps_per_second': 25.136, 'epoch': 1.0}\n{'loss': 0.5306, 'grad_norm': 2.4152703285217285, 'learning_rate': 1.3606332593231426e-06, 'epoch': 2.0}\n{'eval_loss': 0.7279629707336426, 'eval_accuracy': 0.48936170212765956, 'eval_f1': 0.07692307692307691, 'eval_runtime': 0.2432, 'eval_samples_per_second': 193.287, 'eval_steps_per_second': 24.675, 'epoch': 2.0}\n{'train_runtime': 4.5821, 'train_samples_per_second': 61.107, 'train_steps_per_second': 7.857, 'train_loss': 0.6019829909006754, 'epoch': 2.0}\n{'eval_loss': 0.7279629707336426, 'eval_accuracy': 0.48936170212765956, 'eval_f1': 0.07692307692307691, 'eval_runtime': 0.2419, 'eval_samples_per_second': 194.293, 'eval_steps_per_second': 24.803, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4564, 'grad_norm': 3.135648012161255, 'learning_rate': 2.0267897149530373e-05, 'epoch': 1.0}\n{'eval_loss': 0.7802586555480957, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.16, 'eval_runtime': 0.1582, 'eval_samples_per_second': 297.094, 'eval_steps_per_second': 18.963, 'epoch': 1.0}\n{'loss': 0.3126, 'grad_norm': 4.2072434425354, 'learning_rate': 2.026789714953037e-06, 'epoch': 2.0}\n{'eval_loss': 0.8508768081665039, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.21428571428571427, 'eval_runtime': 0.161, 'eval_samples_per_second': 291.946, 'eval_steps_per_second': 18.635, 'epoch': 2.0}\n{'train_runtime': 3.1499, 'train_samples_per_second': 88.891, 'train_steps_per_second': 5.714, 'train_loss': 0.3844764232635498, 'epoch': 2.0}\n{'eval_loss': 0.8508768081665039, 'eval_accuracy': 0.5319148936170213, 'eval_f1': 0.21428571428571427, 'eval_runtime': 0.1593, 'eval_samples_per_second': 294.99, 'eval_steps_per_second': 18.829, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2517, 'grad_norm': 0.19155313074588776, 'learning_rate': 2.012053230327631e-05, 'epoch': 1.0}\n{'eval_loss': 1.2612489461898804, 'eval_accuracy': 0.5106382978723404, 'eval_f1': 0.14814814814814814, 'eval_runtime': 0.24, 'eval_samples_per_second': 195.867, 'eval_steps_per_second': 25.004, 'epoch': 1.0}\n{'loss': 0.2084, 'grad_norm': 0.26378363370895386, 'learning_rate': 1.0589753843829635e-06, 'epoch': 2.0}\n{'eval_loss': 1.2525241374969482, 'eval_accuracy': 0.574468085106383, 'eval_f1': 0.0909090909090909, 'eval_runtime': 0.2449, 'eval_samples_per_second': 191.913, 'eval_steps_per_second': 24.5, 'epoch': 2.0}\n{'train_runtime': 4.6125, 'train_samples_per_second': 60.705, 'train_steps_per_second': 7.805, 'train_loss': 0.2300403250588311, 'epoch': 2.0}\n{'eval_loss': 1.2525241374969482, 'eval_accuracy': 0.574468085106383, 'eval_f1': 0.0909090909090909, 'eval_runtime': 0.2457, 'eval_samples_per_second': 191.305, 'eval_steps_per_second': 24.422, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2077, 'grad_norm': 0.1081167533993721, 'learning_rate': 1.7632033348728121e-06, 'epoch': 1.0}\n{'eval_loss': 1.2234455347061157, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.08695652173913045, 'eval_runtime': 0.2419, 'eval_samples_per_second': 194.284, 'eval_steps_per_second': 24.802, 'epoch': 1.0}\n{'loss': 0.1999, 'grad_norm': 0.1878412961959839, 'learning_rate': 9.280017551962168e-08, 'epoch': 2.0}\n{'eval_loss': 1.2489665746688843, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.08695652173913045, 'eval_runtime': 0.2448, 'eval_samples_per_second': 191.955, 'eval_steps_per_second': 24.505, 'epoch': 2.0}\n{'train_runtime': 4.6012, 'train_samples_per_second': 60.853, 'train_steps_per_second': 7.824, 'train_loss': 0.2038198841942681, 'epoch': 2.0}\n{'eval_loss': 1.2489665746688843, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.08695652173913045, 'eval_runtime': 0.244, 'eval_samples_per_second': 192.651, 'eval_steps_per_second': 24.594, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3641, 'grad_norm': 2.418520212173462, 'learning_rate': 2.356095414045152e-05, 'epoch': 1.0}\n{'eval_loss': 1.3509159088134766, 'eval_accuracy': 0.574468085106383, 'eval_f1': 0.0909090909090909, 'eval_runtime': 0.1597, 'eval_samples_per_second': 294.277, 'eval_steps_per_second': 18.784, 'epoch': 1.0}\n{'loss': 0.2185, 'grad_norm': 7.9193220138549805, 'learning_rate': 2.356095414045152e-06, 'epoch': 2.0}\n{'eval_loss': 1.2911601066589355, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.1739130434782609, 'eval_runtime': 0.1598, 'eval_samples_per_second': 294.047, 'eval_steps_per_second': 18.769, 'epoch': 2.0}\n{'train_runtime': 3.2158, 'train_samples_per_second': 87.07, 'train_steps_per_second': 5.597, 'train_loss': 0.29129090574052596, 'epoch': 2.0}\n{'eval_loss': 1.2911601066589355, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.1739130434782609, 'eval_runtime': 0.1597, 'eval_samples_per_second': 294.306, 'eval_steps_per_second': 18.785, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2323, 'grad_norm': 3.0837109088897705, 'learning_rate': 1.7518376244854863e-05, 'epoch': 1.0}\n{'eval_loss': 1.3374500274658203, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.0, 'eval_runtime': 0.2425, 'eval_samples_per_second': 193.793, 'eval_steps_per_second': 24.74, 'epoch': 1.0}\n{'loss': 0.2378, 'grad_norm': 0.23492088913917542, 'learning_rate': 9.220198023607821e-07, 'epoch': 2.0}\n{'eval_loss': 1.2746868133544922, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.08695652173913045, 'eval_runtime': 0.2425, 'eval_samples_per_second': 193.815, 'eval_steps_per_second': 24.742, 'epoch': 2.0}\n{'train_runtime': 4.6312, 'train_samples_per_second': 60.459, 'train_steps_per_second': 7.773, 'train_loss': 0.23507948716481528, 'epoch': 2.0}\n{'eval_loss': 1.2746868133544922, 'eval_accuracy': 0.5531914893617021, 'eval_f1': 0.08695652173913045, 'eval_runtime': 0.243, 'eval_samples_per_second': 193.432, 'eval_steps_per_second': 24.693, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3419224728.py:258: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('learning_rate', 1e-6, 5e-5)\n/tmp/ipykernel_48/3419224728.py:260: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  decay = trial.suggest_uniform('weight_decay', 0.0, 0.1)\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2015, 'grad_norm': 0.13678887486457825, 'learning_rate': 1.761313276361491e-05, 'epoch': 1.0}\n{'eval_loss': 1.2806205749511719, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.0, 'eval_runtime': 0.2424, 'eval_samples_per_second': 193.91, 'eval_steps_per_second': 24.754, 'epoch': 1.0}\n{'loss': 0.2, 'grad_norm': 0.12642094492912292, 'learning_rate': 9.270069875586793e-07, 'epoch': 2.0}\n{'eval_loss': 1.3239485025405884, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.1739130434782609, 'eval_runtime': 0.2448, 'eval_samples_per_second': 192.003, 'eval_steps_per_second': 24.511, 'epoch': 2.0}\n{'train_runtime': 4.6308, 'train_samples_per_second': 60.464, 'train_steps_per_second': 7.774, 'train_loss': 0.20074957609176636, 'epoch': 2.0}\n{'eval_loss': 1.3239485025405884, 'eval_accuracy': 0.5957446808510638, 'eval_f1': 0.1739130434782609, 'eval_runtime': 0.2421, 'eval_samples_per_second': 194.14, 'eval_steps_per_second': 24.784, 'epoch': 2.0}\nOptuna best: {'learning_rate': 3.648221486915467e-05, 'batch_size': 8, 'weight_decay': 0.024998188013370327}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/45 00:12 < 00:03, 2.77 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.205000</td>\n      <td>1.356048</td>\n      <td>0.574468</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.215100</td>\n      <td>1.366768</td>\n      <td>0.574468</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.200100</td>\n      <td>1.351451</td>\n      <td>0.553191</td>\n      <td>0.160000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.199400</td>\n      <td>1.361537</td>\n      <td>0.574468</td>\n      <td>0.166667</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n=== Transformer classification reports ===\nBERT Classification report:\n              precision    recall  f1-score   support\n\n           0     0.7719    0.4444    0.5641        99\n           1     0.3452    0.6905    0.4603        42\n\n    accuracy                         0.5177       141\n   macro avg     0.5586    0.5675    0.5122       141\nweighted avg     0.6448    0.5177    0.5332       141\n\nDistilBERT Classification report:\n              precision    recall  f1-score   support\n\n           0     0.7170    0.7677    0.7415        99\n           1     0.3429    0.2857    0.3117        42\n\n    accuracy                         0.6241       141\n   macro avg     0.5299    0.5267    0.5266       141\nweighted avg     0.6055    0.6241    0.6134       141\n\n\n=== Classical ML results (TF-IDF) ===\nRandom Forest Accuracy: 0.6896551724137931\nXGBoost Accuracy: 0.6551724137931034\nSVM Accuracy: 0.6896551724137931\n\nRandom Forest Classification Report:\n               precision    recall  f1-score   support\n\n           0     0.6897    1.0000    0.8163        20\n           1     0.0000    0.0000    0.0000         9\n\n    accuracy                         0.6897        29\n   macro avg     0.3448    0.5000    0.4082        29\nweighted avg     0.4756    0.6897    0.5630        29\n\n\nXGBoost Classification Report:\n               precision    recall  f1-score   support\n\n           0     0.7273    0.8000    0.7619        20\n           1     0.4286    0.3333    0.3750         9\n\n    accuracy                         0.6552        29\n   macro avg     0.5779    0.5667    0.5685        29\nweighted avg     0.6346    0.6552    0.6418        29\n\n\nSVM Classification Report:\n               precision    recall  f1-score   support\n\n           0     0.6897    1.0000    0.8163        20\n           1     0.0000    0.0000    0.0000         9\n\n    accuracy                         0.6897        29\n   macro avg     0.3448    0.5000    0.4082        29\nweighted avg     0.4756    0.6897    0.5630        29\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.metrics import RocCurveDisplay\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T06:49:46.740663Z","iopub.execute_input":"2025-11-10T06:49:46.741248Z","iopub.status.idle":"2025-11-10T06:49:46.744785Z","shell.execute_reply.started":"2025-11-10T06:49:46.741219Z","shell.execute_reply":"2025-11-10T06:49:46.744035Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# =========================================\n#  SAVE VISUALS & REPORTS\n# =========================================\ndef save_classification_report(y_true, y_pred, model_name):\n    \"\"\"Save classification report as text file.\"\"\"\n    report = classification_report(y_true, y_pred, digits=4)\n    path = f\"/kaggle/working/model_results/{model_name}_classification_report.txt\"\n    with open(path, \"w\") as f:\n        f.write(report)\n    print(f\" Saved classification report: {path}\")\n\ndef save_confusion_matrix(y_true, y_pred, model_name):\n    \"\"\"Save confusion matrix heatmap.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(f\"{model_name} Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    path = f\"/kaggle/working/model_results-6/{model_name}_confusion_matrix.png\"\n    plt.savefig(path)\n    plt.close()\n    print(f\" Saved confusion matrix: {path}\")\n\ndef save_roc_curve(y_true, y_score, model_name):\n    \"\"\"Save ROC curve plot.\"\"\"\n    try:\n        fpr, tpr, _ = roc_curve(y_true, y_score)\n        roc_auc = auc(fpr, tpr)\n        plt.figure(figsize=(6,5))\n        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n        plt.plot([0,1], [0,1], \"k--\")\n        plt.title(f\"{model_name} ROC Curve\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.legend()\n        plt.tight_layout()\n        path = f\"/kaggle/working/model_results-6/{model_name}_roc_curve.png\"\n        plt.savefig(path)\n        plt.close()\n        print(f\" Saved ROC curve: {path}\")\n    except Exception as e:\n        print(f\" ROC curve skipped for {model_name}: {e}\")\n\ndef save_training_curves(epochs, train_loss, val_loss, val_acc, model_name):\n    \"\"\"Save training/validation curves.\"\"\"\n    plt.figure(figsize=(12,5))\n    plt.subplot(1,2,1)\n    plt.plot(epochs, train_loss, '--', label='Train Loss')\n    plt.plot(epochs, val_loss, label='Val Loss')\n    plt.title(f'{model_name} - Loss Curve')\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.plot(epochs, val_acc, label='Val Accuracy')\n    plt.title(f'{model_name} - Accuracy Curve')\n    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n\n    plt.tight_layout()\n    path = f\"/kaggle/working/model_results-6/{model_name}_train_val_curves.png\"\n    plt.savefig(path)\n    plt.close()\n    print(f\" Saved training curves: {path}\")\n\ndef ensure_result_dir():\n    os.makedirs(\"/kaggle/working/model_results-6\", exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T06:52:15.600096Z","iopub.execute_input":"2025-11-10T06:52:15.600677Z","iopub.status.idle":"2025-11-10T06:52:15.610696Z","shell.execute_reply.started":"2025-11-10T06:52:15.600652Z","shell.execute_reply":"2025-11-10T06:52:15.610021Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport numpy as np\nimport traceback\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n\n# Robust save helpers\nRESULTS_DIR = \"/kaggle/working/model_results-6\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\ndef _to_numpy(x):\n    if x is None:\n        return None\n    if isinstance(x, np.ndarray):\n        return x\n    try:\n        import torch\n        if isinstance(x, torch.Tensor):\n            return x.detach().cpu().numpy()\n    except Exception:\n        pass\n    # try list conversion\n    try:\n        return np.array(x)\n    except Exception:\n        return x\n\ndef save_classification_report(y_true, y_pred, model_name):\n    try:\n        y_true = _to_numpy(y_true)\n        y_pred = _to_numpy(y_pred)\n        rpt = classification_report(y_true, y_pred, digits=4)\n        path = os.path.join(RESULTS_DIR, f\"{model_name}_classification_report.txt\")\n        with open(path, \"w\") as f:\n            f.write(rpt)\n        print(f\" classification report saved: {path}\")\n    except Exception as e:\n        print(\"! failed saving classification report:\", e)\n        print(traceback.format_exc())\n\ndef save_confusion_matrix(y_true, y_pred, model_name, cmap=\"Blues\"):\n    try:\n        y_true = _to_numpy(y_true)\n        y_pred = _to_numpy(y_pred)\n        cm = confusion_matrix(y_true, y_pred)\n        plt.figure(figsize=(5,4))\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap)\n        plt.title(f\"{model_name} Confusion Matrix\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.tight_layout()\n        path = os.path.join(RESULTS_DIR, f\"{model_name}_confusion_matrix.png\")\n        plt.savefig(path)\n        plt.close()\n        print(f\" confusion matrix saved: {path}\")\n    except Exception as e:\n        print(\"! failed saving confusion matrix:\", e)\n        print(traceback.format_exc())\n\ndef save_roc_curve(y_true, y_score, model_name):\n    \"\"\"\n    y_true: binary labels (0/1) or shape (n_samples,)\n    y_score: probabilities for class 1 OR array shape (n_samples, n_classes) with probs per class\n    \"\"\"\n    try:\n        y_true = _to_numpy(y_true)\n        y_score = _to_numpy(y_score)\n\n        if y_true is None or y_score is None:\n            raise ValueError(\"y_true or y_score is None\")\n\n        # If y_score is class labels (0/1), ROC won't be informative  warn and skip\n        if y_score.ndim == 1 and set(np.unique(y_score)).issubset({0,1}):\n            # treat as predicted labels, not probabilities\n            print(f\" {model_name}: provided y_score looks like class labels  ROC requires probabilities. Skipping.\")\n            return\n\n        # If y_score is NxC, pick column for positive class if binary\n        if y_score.ndim == 2 and y_score.shape[1] == 2:\n            pos_score = y_score[:,1]\n        elif y_score.ndim == 2 and y_score.shape[1] > 2:\n            # multiclass: try one-vs-rest macro-averaged ROC AUC plotting per class simplified\n            # We'll compute micro-average ROC\n            y_true_bin = label_binarize(y_true, classes=np.arange(y_score.shape[1]))\n            fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n            roc_auc = auc(fpr, tpr)\n            plt.figure(figsize=(6,5))\n            plt.plot(fpr, tpr, label=f\"micro AUC = {roc_auc:.3f}\")\n            plt.plot([0,1],[0,1],\"k--\")\n            plt.title(f\"{model_name} ROC Curve (micro-avg)\")\n            plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.legend()\n            path = os.path.join(RESULTS_DIR, f\"{model_name}_roc_curve.png\")\n            plt.savefig(path); plt.close()\n            print(f\" multiclass ROC (micro) saved: {path}\")\n            return\n        else:\n            # assume 1D pos_score\n            pos_score = y_score.ravel()\n\n        # If y_true is not binary, binarize for roc\n        if set(np.unique(y_true)) - {0,1}:\n            # map classes so that positive is class 1 if possible, else binarize\n            try:\n                y_true = label_binarize(y_true, classes=np.unique(y_true)).ravel()\n            except Exception:\n                pass\n\n        fpr, tpr, _ = roc_curve(y_true, pos_score)\n        roc_auc = auc(fpr, tpr)\n        plt.figure(figsize=(6,5))\n        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n        plt.plot([0,1],[0,1],\"k--\")\n        plt.title(f\"{model_name} ROC Curve\")\n        plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.legend()\n        path = os.path.join(RESULTS_DIR, f\"{model_name}_roc_curve.png\")\n        plt.savefig(path); plt.close()\n        print(f\" ROC curve saved: {path}\")\n    except Exception as e:\n        print(\"! failed saving ROC curve:\", e)\n        print(traceback.format_exc())\n\ndef save_training_curves(epochs, train_loss, val_loss, val_acc, model_name):\n    try:\n        # convert to numpy arrays and ensure lengths match\n        epochs = list(epochs)\n        train_loss = _to_numpy(train_loss)\n        val_loss = _to_numpy(val_loss)\n        val_acc = _to_numpy(val_acc)\n\n        # pad/trim arrays to epoch length if required\n        L = len(epochs)\n        if train_loss is None or len(train_loss) < L:\n            train_loss = np.concatenate([train_loss, np.full(L - len(train_loss), np.nan)]) if train_loss is not None else np.full(L, np.nan)\n        if val_loss is None or len(val_loss) < L:\n            val_loss = np.concatenate([val_loss, np.full(L - len(val_loss), np.nan)]) if val_loss is not None else np.full(L, np.nan)\n        if val_acc is None or len(val_acc) < L:\n            val_acc = np.concatenate([val_acc, np.full(L - len(val_acc), np.nan)]) if val_acc is not None else np.full(L, np.nan)\n\n        plt.figure(figsize=(12,5))\n        plt.subplot(1,2,1)\n        plt.plot(epochs, train_loss, '--', label='Train Loss')\n        plt.plot(epochs, val_loss, label='Val Loss')\n        plt.title(f'{model_name} - Loss Curve')\n        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n\n        plt.subplot(1,2,2)\n        plt.plot(epochs, val_acc, label='Val Accuracy')\n        plt.title(f'{model_name} - Accuracy Curve')\n        plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n\n        plt.tight_layout()\n        path = os.path.join(RESULTS_DIR, f\"{model_name}_train_val_curves.png\")\n        plt.savefig(path); plt.close()\n        print(f\" training curves saved: {path}\")\n    except Exception as e:\n        print(\"! failed saving training curves:\", e)\n        print(traceback.format_exc())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:13:20.105292Z","iopub.execute_input":"2025-11-10T07:13:20.105838Z","iopub.status.idle":"2025-11-10T07:13:20.125519Z","shell.execute_reply.started":"2025-11-10T07:13:20.105811Z","shell.execute_reply":"2025-11-10T07:13:20.124982Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"save_classification_report(y_test, rf_preds, \"RandomForest\")\nsave_confusion_matrix(y_test, rf_preds, \"RandomForest\")\nsave_roc_curve(y_test, rf.predict_proba(X_test)[:,1], \"RandomForest\")\n\nsave_classification_report(y_test, xgb_preds, \"XGBoost\")\nsave_confusion_matrix(y_test, xgb_preds, \"XGBoost\")\nsave_roc_curve(y_test, xgb_clf.predict_proba(X_test)[:,1], \"XGBoost\")\n\nsave_classification_report(y_test, svm_preds, \"SVM\")\nsave_confusion_matrix(y_test, svm_preds, \"SVM\")\nsave_roc_curve(y_test, svm_clf.predict_proba(X_test)[:,1], \"SVM\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:14:01.920338Z","iopub.execute_input":"2025-11-10T07:14:01.920982Z","iopub.status.idle":"2025-11-10T07:14:02.883715Z","shell.execute_reply.started":"2025-11-10T07:14:01.920956Z","shell.execute_reply":"2025-11-10T07:14:02.883145Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":" classification report saved: /kaggle/working/model_results-6/RandomForest_classification_report.txt\n confusion matrix saved: /kaggle/working/model_results-6/RandomForest_confusion_matrix.png\n ROC curve saved: /kaggle/working/model_results-6/RandomForest_roc_curve.png\n classification report saved: /kaggle/working/model_results-6/XGBoost_classification_report.txt\n confusion matrix saved: /kaggle/working/model_results-6/XGBoost_confusion_matrix.png\n ROC curve saved: /kaggle/working/model_results-6/XGBoost_roc_curve.png\n classification report saved: /kaggle/working/model_results-6/SVM_classification_report.txt\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":" confusion matrix saved: /kaggle/working/model_results-6/SVM_confusion_matrix.png\n ROC curve saved: /kaggle/working/model_results-6/SVM_roc_curve.png\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"summary = {\n    \"Model\": [\"ELECTRA_BERT\", \"DistilBERT\", \"RandomForest\", \"XGBoost\", \"SVM\"],\n    \"Accuracy\": [\n        acc_metric.compute(predictions=bert_preds_all, references=true_labels_bert)['accuracy'] if len(true_labels_bert) else None,\n        acc_metric.compute(predictions=distil_preds_all, references=true_labels_distil)['accuracy'] if len(true_labels_distil) else None,\n        np.mean(rf_preds == y_test),\n        np.mean(xgb_preds == y_test),\n        np.mean(svm_preds == y_test),\n    ]\n}\npd.DataFrame(summary).to_csv(\"/kaggle/working/model_results/model_summary.csv\", index=False)\nprint(\" Saved model summary table.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:15:52.663307Z","iopub.execute_input":"2025-11-10T07:15:52.663554Z","iopub.status.idle":"2025-11-10T07:15:52.685287Z","shell.execute_reply.started":"2025-11-10T07:15:52.663531Z","shell.execute_reply":"2025-11-10T07:15:52.684419Z"}},"outputs":[{"name":"stdout","text":" Saved model summary table.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# preds_output is the HuggingFace PredictionOutput from trainer.predict(tokenized_val)\nlogits = preds_output.predictions  # shape (n_samples, n_classes)\nprobs = torch.softmax(torch.tensor(logits), dim=1).numpy()  # convert to numpy probs\n\n# For binary classifiers:\nsave_classification_report(preds_output.label_ids, np.argmax(logits, axis=1), \"ELECTRA_BERT_fold0\")\nsave_confusion_matrix(preds_output.label_ids, np.argmax(logits, axis=1), \"ELECTRA_BERT_fold0\")\nsave_roc_curve(preds_output.label_ids, probs[:,1], \"ELECTRA_BERT_fold0\")  # pass prob of class 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:16:30.159032Z","iopub.execute_input":"2025-11-10T07:16:30.159247Z","iopub.status.idle":"2025-11-10T07:16:30.419425Z","shell.execute_reply.started":"2025-11-10T07:16:30.159227Z","shell.execute_reply":"2025-11-10T07:16:30.418905Z"}},"outputs":[{"name":"stdout","text":" classification report saved: /kaggle/working/model_results-6/ELECTRA_BERT_fold0_classification_report.txt\n confusion matrix saved: /kaggle/working/model_results-6/ELECTRA_BERT_fold0_confusion_matrix.png\n ROC curve saved: /kaggle/working/model_results-6/ELECTRA_BERT_fold0_roc_curve.png\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# --- DistilBERT ---\nensure_result_dir()\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport torch\n\n# Assuming your validation output is stored as `distilbert_pred_output`\n# (PredictionOutput object returned by trainer.predict())\nlogits = distilbert_pred_output.predictions\ntrue_labels_distil = distilbert_pred_output.label_ids\nprobs = torch.softmax(torch.tensor(logits), dim=1).numpy()\npreds = np.argmax(probs, axis=1)\n\nsave_classification_report(true_labels_distil, preds, \"DistilBERT\")\nsave_confusion_matrix(true_labels_distil, preds, \"DistilBERT\")\n\n# Use probability of positive class (index 1) for ROC\nif probs.shape[1] > 1:\n    save_roc_curve(true_labels_distil, probs[:, 1], \"DistilBERT\")\nelse:\n    print(\" DistilBERT ROC skipped  only one output class.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:18:22.271727Z","iopub.execute_input":"2025-11-10T07:18:22.272283Z","iopub.status.idle":"2025-11-10T07:18:22.309976Z","shell.execute_reply.started":"2025-11-10T07:18:22.272256Z","shell.execute_reply":"2025-11-10T07:18:22.309108Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2665774987.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assuming your validation output is stored as `distilbert_pred_output`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# (PredictionOutput object returned by trainer.predict())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilbert_pred_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrue_labels_distil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilbert_pred_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'distilbert_pred_output' is not defined"],"ename":"NameError","evalue":"name 'distilbert_pred_output' is not defined","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"ensure_result_dir()\n\n# Convert lists to numpy arrays\ny_true_distil = np.array(true_labels_distil)\ny_pred_distil = np.array(distil_preds_all)\n\nsave_classification_report(y_true_distil, y_pred_distil, \"DistilBERT\")\nsave_confusion_matrix(y_true_distil, y_pred_distil, \"DistilBERT\")\n\n# If you stored prediction probabilities, you can use them here; otherwise skip ROC\n# But since we didnt save probs, use predicted labels to make dummy probs:\nsave_roc_curve(y_true_distil, y_pred_distil, \"DistilBERT\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:19:58.715272Z","iopub.execute_input":"2025-11-10T07:19:58.715771Z","iopub.status.idle":"2025-11-10T07:19:58.870504Z","shell.execute_reply.started":"2025-11-10T07:19:58.715743Z","shell.execute_reply":"2025-11-10T07:19:58.869913Z"}},"outputs":[{"name":"stdout","text":" classification report saved: /kaggle/working/model_results-6/DistilBERT_classification_report.txt\n confusion matrix saved: /kaggle/working/model_results-6/DistilBERT_confusion_matrix.png\n DistilBERT: provided y_score looks like class labels  ROC requires probabilities. Skipping.\n","output_type":"stream"}],"execution_count":22}]}